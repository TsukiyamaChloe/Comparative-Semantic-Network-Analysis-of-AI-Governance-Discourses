BLUEPRINT FOR ANAI BILL OFRIGHTSMAKING AUTOMATEDSYSTEMS WORK FORTHE AMERICAN PEOPLEOCTOBER 2022published by the White House Office of Science and Technology Policy in October 2022. This framework wasreleased one year after OSTP announced the launch of a process to develop “a bill of rights for an AI-poweredworld.” Its release follows a year of public engagement to inform this initiative. The framework is availableonline at: whitehouse. gov/ostp/ai-bill-of-rights About the Office of Science and Technology Policy The Office of Science and Technology Policy (OSTP) was established by the National Science and Technology Policy, Organization, and Priorities Act of 1976 to provide the President and others within the Executive Officeof the President with advice on the scientific, engineering, and technological aspects of the economy, nationalsecurity, health, foreign relations, the environment, and the technological recovery and use of resources, amongother topics. OSTP leads interagency science and technology policy coordination efforts, assists the Office of Management and Budget (OMB) with an annual review and analysis of Federal research and development inbudgets, and serves as a source of scientific and technological analysis and judgment for the President withrespect to major policies, plans, and programs of the Federal Government. Legal Disclaimer The Blueprint for an AI Bill of Rights: Making Automated Systems Work for the American People is a white paperpublished by the White House Office of Science and Technology Policy. It is intended to support thedevelopment of policies and practices that protect civil rights and promote democratic values in the building, deployment, and governance of automated systems. The Blueprint for an AI Bill of Rights is non-binding and does not constitute U. S. government policy. Itdoes not supersede, modify, or direct an interpretation of any existing statute, regulation, policy, orinternational instrument. It does not constitute binding guidance for the public or Federal agencies andtherefore does not require compliance with the principles described herein. It also is not determinative of whatthe U. S. government’s position will be in any international negotiation. Adoption of these principles may notmeet the requirements of existing statutes, regulations, policies, or international instruments, or therequirements of the Federal agencies that enforce them. These principles are not intended to, and do not, prohibit or limit any lawful activity of a government agency, including law enforcement, national security, orintelligence activities. The appropriate application of the principles set forth in this white paper depends significantly on thecontext in which automated systems are being utilized. In some circumstances, application of these principlesin whole or in part may not be appropriate given the intended use of automated systems to achieve governmentagency missions. Future sector-specific guidance will likely be necessary and important for guiding the use ofautomated systems in certain settings such as AI systems used as part of school building security or automatedhealth diagnostic systems. The Blueprint for an AI Bill of Rights recognizes that law enforcement activities require a balancing ofequities, for example, between the protection of sensitive law enforcement information and the principle ofnotice; as such, notice may not be appropriate, or may need to be adjusted to protect sources, methods, andother law enforcement equities. Even in contexts where these principles may not apply in whole or in part, federal departments and agencies remain subject to judicial, privacy, and civil liberties oversight as well asexisting policies and safeguards that govern automated systems, including, for example, Executive Order 13960, Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government (December 2020). This white paper recognizes that national security (which includes certain law enforcement andhomeland security activities) and defense activities are of increased sensitivity and interest to our nation’sadversaries and are often subject to special requirements, such as those governing classified information andother protected data. Such activities require alternative, compatible safeguards through existing policies thatgovern automated systems and AI, such as the Department of Defense (DOD) AI Ethical Principles and Responsible AI Implementation Pathway and the Intelligence Community (IC) AI Ethics Principles and Framework. The implementation of these policies to national security and defense activities can be informed bythe Blueprint for an AI Bill of Rights where feasible. The Blueprint for an AI Bill of Rights is not intended to, and does not, create any legal right, benefit, ordefense, substantive or procedural, enforceable at law or in equity by any party against the United States, itsdepartments, agencies, or entities, its officers, employees, or agents, or any other person, nor does it constitute awaiver of sovereign immunity. Copyright Information This document is a work of the United States Government and is in the public domain (see 17 U. S. C. §105). FOREWORDAmong the great challenges posed to democracy today is the use of technology, data, and automated systems inways that threaten the rights of the American public. Too often, these tools are used to limit our opportunities andprevent our access to critical resources or services. These problems are well documented. In America and aroundthe world, systems supposed to help with patient care have proven unsafe, ineffective, or biased. Algorithms usedin hiring and credit decisions have been found to reflect and reproduce existing unwanted inequities or embednew harmful bias and discrimination. Unchecked social media data collection has been used to threaten people’sopportunities, undermine their privacy, or pervasively track their activity—often without their knowledge orconsent. These outcomes are deeply harmful—but they are not inevitable. Automated systems have brought about extraor-dinary benefits, from technology that helps farmers grow food more efficiently and computers that predict stormpaths, to algorithms that can identify diseases in patients. These tools now drive important decisions acrosssectors, while data is helping to revolutionize global industries. Fueled by the power of American innovation, these tools hold the potential to redefine every part of our society and make life better for everyone. This important progress must not come at the price of civil rights or democratic values, foundational Americanprinciples that President Biden has affirmed as a cornerstone of his Administration. On his first day in office, the President ordered the full Federal government to work to root out inequity, embed fairness in decision-making processes, and affirmatively advance civil rights, equal opportunity, and racial justice in America.1 The President has spoken forcefully about the urgent challenges posed to democracy today and has regularly calledon people of conscience to act to preserve civil rights—including the right to privacy, which he has called “thebasis for so many more rights that we have come to take for granted that are ingrained in the fabric of thiscountry.”2To advance President Biden’s vision, the White House Office of Science and Technology Policy has identifiedfive principles that should guide the design, use, and deployment of automated systems to protect the Americanpublic in the age of artificial intelligence. The Blueprint for an AI Bill of Rights is a guide for a society thatprotects all people from these threats—and uses technologies in ways that reinforce our highest values. Responding to the experiences of the American public, and informed by insights from researchers, technologists, advocates, journalists, and policymakers, this framework is accompanied by a technicalcompanion—a handbook for anyone seeking to incorporate these protections into policy and practice, includingdetailed steps toward actualizing these principles in the technological design process. These principles helpprovide guidance whenever automated systems can meaningfully impact the public’s rights, opportunities, or access to critical needs. ABOUT THIS FRAMEWORKThe Blueprint for an AI Bill of Rights is a set of five principles and associated practices to help guide thedesign, use, and deployment of automated systems to protect the rights of the American public in the age ofartificial intel-ligence. Developed through extensive consultation with the American public, these principles area blueprint for building and deploying automated systems that are aligned with democratic values and protectcivil rights, civil liberties, and privacy. The Blueprint for an AI Bill of Rights includes this Foreword, the fiveprinciples, notes on Applying the The Blueprint for an AI Bill of Rights, and a Technical Companion that givesconcrete steps that can be taken by many kinds of organizations—from governments at all levels to companies ofall sizes—to uphold these values. Experts from across the private sector, governments, and internationalconsortia have published principles and frameworks to guide the responsible use of automated systems; thisframework provides a national values statement and toolkit that is sector-agnostic to inform building theseprotections into policy, practice, or the technological design process. Where existing law or policy—such assector-specific privacy laws and oversight requirements—do not already provide guidance, the Blueprint for an AI Bill of Rights should be used to inform policy decisions. L A PISTENING TO THE MERICAN UBLICThe White House Office of Science and Technology Policy has led a year-long process to seek and distill inputfrom people across the country—from impacted communities and industry stakeholders to technology develop-ers and other experts across fields and sectors, as well as policymakers throughout the Federal government—onthe issue of algorithmic and data-driven harms and potential remedies. Through panel discussions, public listen-ing sessions, meetings, a formal request for information, and input to a publicly accessible and widely-publicizedemail address, people throughout the United States, public servants across Federal agencies, and members of theinternational community spoke up about both the promises and potential harms of these technologies, andplayed a central role in shaping the Blueprint for an AI Bill of Rights. The core messages gleaned from thesediscussions include that AI has transformative potential to improve Americans’ lives, and that preventing theharms of these technologies is both necessary and achievable. The Appendix includes a full list of public engage-ments. BLUEPRINT FOR AN AI BILL OF RIGHTSS E SAFE AND FFECTIVE YSTEMSYou should be protected from unsafe or ineffective systems. Automated systems should bedeveloped with consultation from diverse communities, stakeholders, and domain experts to identifyconcerns, risks, and potential impacts of the system. Systems should undergo pre-deployment testing, riskidentification and mitigation, and ongoing monitoring that demonstrate they are safe and effective based ontheir intended use, mitigation of unsafe outcomes including those beyond the intended use, and adherence todomain-specific standards. Outcomes of these protective measures should include the possibility of notdeploying the system or removing a system from use. Automated systems should not be designed with an intentor reasonably foreseeable possibility of endangering your safety or the safety of your community. They shouldbe designed to proactively protect you from harms stemming from unintended, yet foreseeable, uses orimpacts of automated systems. You should be protected from inappropriate or irrelevant data use in thedesign, development, and deployment of automated systems, and from the compounded harm of its reuse. Independent evaluation and reporting that confirms that the system is safe and effective, including reporting ofsteps taken to mitigate potential harms, should be performed and the results made public whenever possible. A D PLGORITHMIC ISCRIMINATION ROTECTIONSYou should not face discrimination by algorithms and systems should be used and designed inan equitable way. Algorithmic discrimination occurs when au tomated systems contribute to unjustifieddifferent treatment or impacts disfavoring people based on their race, color, ethnicity, sex (includingpregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexualorientation), religion, age, national origin, disability, veteran status, genetic information, or any otherclassification protected by law. Depending on the specific circumstances, such algorithmic discriminationmay violate legal protections. Designers, developers, and deployers of automated systems should takeproactive and continuous measures to protect individuals and communities from algorithmicdiscrimination and to use and design systems in an equitable way. This protection should include proactiveequity assessments as part of the system design, use of representative data and protection against proxiesfor demographic features, ensuring accessibility for people with disabilities in design and development, pre-deployment and ongoing disparity testing and mitigation, and clear organizational oversight. Independentevaluation and plain language reporting in the form of an algorithmic impact assessment, includingdisparity testing results and mitigation information, should be performed and made public wheneverpossible to confirm these protections. D PATA RIVACYYou should be protected from abusive data practices via built-in protections and youshould have agency over how data about you is used. You should be protected from violations ofprivacy through design choices that ensure such protections are included by default, including ensuring thatdata collection conforms to reasonable expectations and that only data strictly necessary for the specificcontext is collected. Designers, developers, and deployers of automated systems should seek your permissionand respect your decisions regarding collection, use, access, transfer, and deletion of your data in appropriateways and to the greatest extent possible; where not possible, alternative privacy by design safeguards should beused. Systems should not employ user experience and design decisions that obfuscate user choice or burdenusers with defaults that are privacy invasive. Consent should only be used to justify collection of data in caseswhere it can be appropriately and meaningfully given. Any consent requests should be brief, be understandablein plain language, and give you agency over data collection and the specific context of use; current hard-to-understand notice-and-choice practices for broad uses of data should be changed. Enhanced protections andrestrictions for data and inferences related to sensitive domains, including health, work, education, criminaljustice, and finance, and for data pertaining to youth should put you first. In sensitive domains, your data andrelated inferences should only be used for necessary functions, and you should be protected by ethical reviewand use prohibitions. You and your communities should be free from unchecked surveillance; surveillancetechnologies should be subject to heightened oversight that includes at least pre-deployment assessment of theirpotential harms and scope limits to protect privacy and civil liberties. Continuous surveillance and monitoringshould not be used in education, work, housing, or in other contexts where the use of such surveillancetechnologies is likely to limit rights, opportunities, or access. Whenever possible, you should have access toreporting that confirms your data decisions have been respected and provides an assessment of thepotential impact of surveillance technologies on your rights, opportunities, or access. N EOTICE AND XPLANATIONYou should know that an automated system is being used and understand how and why itcontributes to outcomes that impact you. Designers, developers, and deployers of automated systemsshould provide generally accessible plain language documentation including clear descriptions of the overallsystem functioning and the role automation plays, notice that such systems are in use, the individual or organiza-tion responsible for the system, and explanations of outcomes that are clear, timely, and accessible. Such noticeshould be kept up-to-date and people impacted by the system should be notified of significant use case or keyfunctionality changes. You should know how and why an outcome impacting you was determined by anautomated system, including when the automated system is not the sole input determining the outcome. Automated systems should provide explanations that are technically valid, meaningful and useful to you and toany operators or others who need to understand the system, and calibrated to the level of risk based on thecontext. Reporting that includes summary information about these automated systems in plain language andassessments of the clarity and quality of the notice and explanations should be made public whenever possible. H A , C , FUMAN LTERNATIVES ONSIDERATION AND ALLBACKYou should be able to opt out, where appropriate, and have access to a person who can quicklyconsider and remedy problems you encounter. You should be able to opt out from automated systems infavor of a human alternative, where appropriate. Appropriateness should be determined based on reasonableexpectations in a given context and with a focus on ensuring broad accessibility and protecting the public fromespecially harmful impacts. In some cases, a human or other alternative may be required by law. You should haveaccess to timely human consideration and remedy by a fallback and escalation process if an automated systemfails, it produces an error, or you would like to appeal or contest its impacts on you. Human consideration andfallback should be accessible, equitable, effective, maintained, accompanied by appropriate operator training, andshould not impose an unreasonable burden on the public. Automated systems with an intended use within sensi-tive domains, including, but not limited to, criminal justice, employment, education, and health, should additional-ly be tailored to the purpose, provide meaningful access for oversight, include training for any people interactingwith the system, and incorporate human consideration for adverse or high-risk decisions. Reporting that includesa description of these human governance processes and assessment of their timeliness, accessibility, outcomes, and effectiveness should be made public whenever possible. Definitions for key terms in The Blueprint for an AI Bill of Rights can be found in Applying the Blueprint for an AI Bill of Rights. Accompanying analysis and tools for actualizing each principle can be found in the Technical Companion. Applying The Blueprint for an AI Bill of Rights While many of the concerns addressed in this framework derive from the use of AI, the technicalcapabilities and specific definitions of such systems change with the speed of innovation, and the potentialharms of their use occur even with less technologically sophisticated tools. Thus, this framework uses a two-part test to determine what systems are in scope. This framework applies to (1) automated systems that (2)have the potential to meaningfully impact the American public’s rights, opportunities, or access tocritical resources or services. These rights, opportunities, and access to critical resources of services shouldbe enjoyed equally and be fully protected, regardless of the changing role that automated systems may play inour lives. This framework describes protections that should be applied with respect to all automated systems thathave the potential to meaningfully impact individuals' or communities' exercise of: R , O , AIGHTS PPORTUNITIES OR CCESSCivil rights, civil liberties, and privacy, including freedom of speech, voting, and protections from discrimi-nation, excessive punishment, unlawful surveillance, and violations of privacy and other freedoms in bothpublic and private sector contexts; Equal opportunities, including equitable access to education, housing, credit, employment, and otherprograms; or, Access to critical resources or services, such as healthcare, financial services, safety, social services, non-deceptive information about goods and services, and government benefits. A list of examples of automated systems for which these principles should be considered is provided in the Appendix. The Technical Companion, which follows, offers supportive guidance for any person or entity thatcreates, deploys, or oversees automated systems. Considered together, the five principles and associated practices of the Blueprint for an AI Bill of Rights form an overlapping set of backstops against potential harms. This purposefully overlappingframework, when taken as a whole, forms a blueprint to help protect the public from harm. The measures taken to realize the vision set forward in this framework should be proportionatewith the extent and nature of the harm, or risk of harm, to people's rights, opportunities, andaccess. RELATIONSHIP TOEXISTINGLAW ANDPOLICYThe Blueprint for an AI Bill of Rights is an exercise in envisioning a future where the American public isprotected from the potential harms, and can fully enjoy the benefits, of automated systems. It describes princi-ples that can help ensure these protections. Some of these protections are already required by the U. S. Constitu-tion or implemented under existing U. S. laws. For example, government surveillance, and data search andseizure are subject to legal requirements and judicial oversight. There are Constitutional requirements forhuman review of criminal investigative matters and statutory requirements for judicial review. Civil rights lawsprotect the American people against discrimination. Applying The Blueprint for an AI Bill of Rights RELATIONSHIP TOEXISTINGLAW ANDPOLICYThere are regulatory safety requirements for medical devices, as well as sector-, population-, or technology-spe-cific privacy and security protections. Ensuring some of the additional protections proposed in this frameworkwould require new laws to be enacted or new policies and practices to be adopted. In some cases, exceptions tothe principles described in the Blueprint for an AI Bill of Rights may be necessary to comply with existing law, conform to the practicalities of a specific use case, or balance competing public interests. In particular, lawenforcement, and other regulatory contexts may require government actors to protect civil rights, civil liberties, and privacy in a manner consistent with, but using alternate mechanisms to, the specific principles discussed inthis framework. The Blueprint for an AI Bill of Rights is meant to assist governments and the private sector inmoving principles into practice. The expectations given in the Technical Companion are meant to serve as a blueprint for the development ofadditional technical standards and practices that should be tailored for particular sectors and contexts. Whileexisting laws informed the development of the Blueprint for an AI Bill of Rights, this framework does not detailthose laws beyond providing them as examples, where appropriate, of existing protective measures. Thisframework instead shares a broad, forward-leaning vision of recommended principles for automated systemdevelopment and use to inform private and public involvement with these systems where they have the poten-tial to meaningfully impact rights, opportunities, or access. Additionally, this framework does not analyze ortake a position on legislative and regulatory proposals in municipal, state, and federal government, or those inother countries. We have seen modest progress in recent years, with some state and local governments responding to these prob-lems with legislation, and some courts extending longstanding statutory protections to new and emerging tech-nologies. There are companies working to incorporate additional protections in their design and use of auto-mated systems, and researchers developing innovative guardrails. Advocates, researchers, and governmentorganizations have proposed principles for the ethical use of AI and other automated systems. These includethe Organization for Economic Co-operation and Development’s (OECD’s) 2019 Recommendation on Artificial Intelligence, which includes principles for responsible stewardship of trustworthy AI and which the United States adopted, and Executive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government, which sets out principles that govern the federal government’s use of AI. The Blueprintfor an AI Bill of Rights is fully consistent with these principles and with the direction in Executive Order 13985on Advancing Racial Equity and Support for Underserved Communities Through the Federal Government. These principles find kinship in the Fair Information Practice Principles (FIPPs), derived from the 1973 reportof an advisory committee to the U. S. Department of Health, Education, and Welfare, Records, Computers, and the Rights of Citizens.4 While there is no single, universal articulation of the FIPPs, these coreprinciples for managing information about individuals have been incorporated into data privacy laws andpolicies across the globe.5 The Blueprint for an AI Bill of Rights embraces elements of the FIPPs that areparticularly relevant to automated systems, without articulating a specific set of FIPPs or scopingapplicability or the interests served to a single particular domain, like privacy, civil rights and civil liberties, ethics, or risk management. The Technical Companion builds on this prior work to provide practical nextsteps to move these principles into practice and promote common approaches that allow technologicalinnovation to flourish while protecting people from harm. Applying The Blueprint for an AI Bill of Rights DEFINITIONSALGORITHMIC DISCRIMINATION: “Algorithmic discrimination” occurs when automated systemscontribute to unjustified different treatment or impacts disfavoring people based on their race, color, ethnicity, sex (including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexualorientation), religion, age, national origin, disability, veteran status, genetic information, or any other classifica-tion protected by law. Depending on the specific circumstances, such algorithmic discrimination may violatelegal protections. Throughout this framework the term “algorithmic discrimination” takes this meaning (andnot a technical understanding of discrimination as distinguishing between items). AUTOMATED SYSTEM: An "automated system" is any system, software, or process that uses computation aswhole or part of a system to determine outcomes, make or aid decisions, inform policy implementation, collectdata or observations, or otherwise interact with individuals and/or communities. Automated systemsinclude, but are not limited to, systems derived from machine learning, statistics, or other data processingor artificial intelligence techniques, and exclude passive computing infrastructure. “Passive computinginfrastructure” is any intermediary technology that does not influence or determine the outcome of decision, make or aid in decisions, inform policy implementation, or collect data or observations, including webhosting, domain registration, networking, caching, data storage, or cybersecurity. Throughout thisframework, automated systems that are considered in scope are only those that have the potential tomeaningfully impact individuals’ or communi-ties’ rights, opportunities, or access. COMMUNITIES: “Communities” include: neighborhoods; social network connections (both online andoffline); families (construed broadly); people connected by affinity, identity, or shared traits; and formal organi-zational ties. This includes Tribes, Clans, Bands, Rancherias, Villages, and other Indigenous communities. AIand other data-driven automated systems most directly collect data on, make inferences about, and may causeharm to individuals. But the overall magnitude of their impacts may be most readily visible at the level of com-munities. Accordingly, the concept of community is integral to the scope of the Blueprint for an AI Bill of Rights. United States law and policy have long employed approaches for protecting the rights of individuals, but exist-ing frameworks have sometimes struggled to provide protections when effects manifest most clearly at a com-munity level. For these reasons, the Blueprint for an AI Bill of Rights asserts that the harms of automatedsystems should be evaluated, protected against, and redressed at both the individual and community levels. EQUITY: “Equity” means the consistent and systematic fair, just, and impartial treatment of all individuals. Systemic, fair, and just treatment must take into account the status of individuals who belong to underservedcommunities that have been denied such treatment, such as Black, Latino, and Indigenous and Native Americanpersons, Asian Americans and Pacific Islanders and other persons of color; members of religious minorities; women, girls, and non-binary people; lesbian, gay, bisexual, transgender, queer, and intersex (LGBTQI+)persons; older adults; persons with disabilities; persons who live in rural areas; and persons otherwise adverselyaffected by persistent poverty or inequality. RIGHTS, OPPORTUNITIES, OR ACCESS: “Rights, opportunities, or access” is used to indicate the scopingof this framework. It describes the set of: civil rights, civil liberties, and privacy, including freedom of speech, voting, and protections from discrimination, excessive punishment, unlawful surveillance, and violations ofprivacy and other freedoms in both public and private sector contexts; equal opportunities, including equitableaccess to education, housing, credit, employment, and other programs; or, access to critical resources orservices, such as healthcare, financial services, safety, social services, non-deceptive information about goodsand services, and government benefits. Applying The Blueprint for an AI Bill of Rights SENSITIVE DATA: Data and metadata are sensitive if they pertain to an individual in a sensitive domain(defined below); are generated by technologies used in a sensitive domain; can be used to infer data from asensitive domain or sensitive data about an individual (such as disability-related data, genomic data, biometricdata, behavioral data, geolocation data, data related to interaction with the criminal justice system, relationshiphistory and legal status such as custody and divorce information, and home, work, or school environmentaldata); or have the reasonable potential to be used in ways that are likely to expose individuals to meaningfulharm, such as a loss of privacy or financial harm due to identity theft. Data and metadata generated by or aboutthose who are not yet legal adults is also sensitive, even if not related to a sensitive domain. Such data includes, but is not limited to, numerical, text, image, audio, or video data. SENSITIVE DOMAINS: “Sensitive domains” are those in which activities being conducted can cause materialharms, including significant adverse effects on human rights such as autonomy and dignity, as well as civil liber-ties and civil rights. Domains that have historically been singled out as deserving of enhanced data protectionsor where such enhanced protections are reasonably expected by the public include, but are not limited to, health, family planning and care, employment, education, criminal justice, and personal finance. In the contextof this framework, such domains are considered sensitive whether or not the specifics of a system contextwould necessitate coverage under existing law, and domains and data that are considered sensitive are under-stood to change over time based on societal norms and context. SURVEILLANCE TECHNOLOGY: “Surveillance technology” refers to products or services marketed foror that can be lawfully used to detect, monitor, intercept, collect, exploit, preserve, protect, transmit, and/orretain data, identifying information, or communications concerning individuals or groups. This frameworklimits its focus to both government and commercial use of surveillance technologies when juxtaposed withreal-time or subsequent automated analysis and when such systems have a potential for meaningful impacton individuals’ or communities’ rights, opportunities, or access. UNDERSERVED COMMUNITIES: The term “underserved communities” refers to communities that havebeen systematically denied a full opportunity to participate in aspects of economic, social, and civic life, asexemplified by the list in the preceding definition of “equity.”FROMPRINCIPLESTO PRACTICEA T CECHINCAL OMPANION TOBlueprint for an THEAI B RILL OF IGHTST CABLE OF ONTENTSF ROM P RINCIPLES TO P RACTICE: A T ECHNICAL C OMPANION TO THE B LUEPRINT 12AI B RFOR AN ILL OF IGHTSU SING T HIS T ECHNICAL C OMPANION 14S AFE AND E FFECTIVE S YSTEMS 15A LGORITHMIC D ISCRIMINATION P ROTECTIONS 23D ATA P RIVACY 30N OTICE AND E XPLANATION 40H UMAN A LTERNATIVES , C ONSIDERATION , AND F ALLBACK 46A PPENDIX 53E XAMPLES OF A UTOMATED S YSTEMS 53L ISTENING TO THE A MERICAN P EOPLE 55E NDNOTES 63USING THIS TECHNICAL COMPANIONThe Blueprint for an AI Bill of Rights is a set of five principles and associated practices to help guide the design, use, and deployment of automated systems to protect the rights of the American public in the age of artificialintelligence. This technical companion considers each principle in the Blueprint for an AI Bill of Rights andprovides examples and concrete steps for communities, industry, governments, and others to take in order tobuild these protections into policy, practice, or the technological design process. Taken together, the technical protections and practices laid out in the Blueprint for an AI Bill of Rights can helpguard the American public against many of the potential and actual harms identified by researchers, technolo-gists, advocates, journalists, policymakers, and communities in the United States and around the world. Thistechnical companion is intended to be used as a reference by people across many circumstances – anyoneimpacted by automated systems, and anyone developing, designing, deploying, evaluating, or making policy togovern the use of an automated system. Each principle is accompanied by three supplemental sections:1WHY THIS PRINCIPLE IS IMPORTANT: This section provides a brief summary of the problems that the principle seeks to address and protect against, includingillustrative examples.2WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMS:• The expectations for automated systems are meant to serve as a blueprint for the development of additional technicalstandards and practices that should be tailored for particular sectors and contexts.• This section outlines practical steps that can be implemented to realize the vision of the Blueprint for an AI Bill of Rights. Theexpectations laid out often mirror existing practices for technology development, including pre-deployment testing, ongoingmonitoring, and governance structures for automated systems, but also go further to address unmet needs for change and offerconcrete directions for how those changes can be made.• Expectations about reporting are intended for the entity developing or using the automated system. The resulting reports canbe provided to the public, regulators, auditors, industry standards groups, or others engaged in independent review, and shouldbe made public as much as possible consistent with law, regulation, and policy, and noting that intellectual property, lawenforcement, or national security considerations may prevent public release. Where public reports are not possible, theinformation should be provided to oversight bodies and privacy, civil liberties, or other ethics officers charged with safeguard-ing individuals’ rights. These reporting expectations are important for transparency, so the American people can haveconfidence that their rights, opportunities, and access as well as their expectations about technologies are respected.3HOW THESE PRINCIPLES CAN MOVE INTO PRACTICE: This section provides real-life examples of how these guiding principles can become reality, through laws, policies, and practices. It describes practical technical and sociotechnical approaches to protecting rights, opportunities, and access. The examples provided are not critiques or endorsements, but rather are offered as illustrative cases to helpprovide a concrete vision for actualizing the Blueprint for an AI Bill of Rights. Effectively implementing theseprocesses require the cooperation of and collaboration among industry, civil society, researchers, policymakers, technologists, and the public. SAFE AND EFFECTIVE SYSTEMSYou should be protected from unsafe or ineffective sys-tems. Automated systems should be developed with consultationfrom diverse communities, stakeholders, and domain experts to iden-tify concerns, risks, and potential impacts of the system. Systemsshould undergo pre-deployment testing, risk identification and miti-gation, and ongoing monitoring that demonstrate they are safe andeffective based on their intended use, mitigation of unsafe outcomesincluding those beyond the intended use, and adherence to do-main-specific standards. Outcomes of these protective measuresshould include the possibility of not deploying the system or remov-ing a system from use. Automated systems should not be designedwith an intent or reasonably foreseeable possibility of endangeringyour safety or the safety of your community. They should be designedto proactively protect you from harms stemming from unintended, yet foreseeable, uses or impacts of automated systems. You should beprotected from inappropriate or irrelevant data use in the design, de-velopment, and deployment of automated systems, and from thecompounded harm of its reuse. Independent evaluation and report-ing that confirms that the system is safe and effective, including re-porting of steps taken to mitigate potential harms, should be per-formed and the results made public whenever possible. WHY THIS PRINCIPLE IS IMPORTANTThis section provides a brief summary of the problems which the principle seeks to address and protectagainst, including illustrative examples. While technologies are being deployed to solve problems across a wide array of issues, our reliance on technology canalso lead to its use in situations where it has not yet been proven to work—either at all or within an acceptable rangeof error. In other cases, technologies do not work as intended or as promised, causing substantial and unjustified harm. Automated systems sometimes rely on data from other systems, including historical data, allowing irrelevant informa-tion from past decisions to infect decision-making in unrelated situations. In some cases, technologies are purposeful-ly designed to violate the safety of others, such as technologies designed to facilitate stalking; in other cases, intendedor unintended uses lead to unintended harms. Many of the harms resulting from these technologies are preventable, and actions are already being taken to protectthe public. Some companies have put in place safeguards that have prevented harm from occurring by ensuring thatkey development decisions are vetted by an ethics review; others have identified and mitigated harms found throughpre-deployment testing and ongoing monitoring processes. Governments at all levels have existing public consulta-tion processes that may be applied when considering the use of new automated systems, and existing product develop-ment and testing practices already protect the American public from many potential harms. Still, these kinds of practices are deployed too rarely and unevenly. Expanded, proactive protections could build onthese existing practices, increase confidence in the use of automated systems, and protect the American public. Inno-vators deserve clear rules of the road that allow new ideas to flourish, and the American public deserves protectionsfrom unsafe outcomes. All can benefit from assurances that automated systems will be designed, tested, and consis-tently confirmed to work as intended, and that they will be proactively protected from foreseeable unintended harm-ful outcomes.• A proprietary model was developed to predict the likelihood of sepsis in hospitalized patients and was imple-mented at hundreds of hospitals around the country. An independent study showed that the model predictionsunderperformed relative to the designer’s claims while also causing ‘alert fatigue’ by falsely alertinglikelihood of sepsis.6• On social media, Black people who quote and criticize racist messages have had their own speech silenced whena platform’s automated moderation system failed to distinguish this “counter speech” (or other critiqueand journalism) from the original hateful messages to which such speech responded.7• A device originally developed to help people track and find lost items has been used as a tool by stalkers to trackvictims’ locations in violation of their privacy and safety. The device manufacturer took steps after release toprotect people from unwanted tracking by alerting people on their phones when a device is found to be movingwith them over time and also by having the device make an occasional noise, but not all phones are ableto receive the notification and the devices remain a safety concern due to their misuse.8• An algorithm used to deploy police was found to repeatedly send police to neighborhoods they regularly visit, even if those neighborhoods were not the ones with the highest crime rates. These incorrect crime predictionswere the result of a feedback loop generated from the reuse of data from previous arrests and algorithmpredictions.9WHY THIS PRINCIPLE IS IMPORTANTThis section provides a brief summary of the problems which the principle seeks to address and protectagainst, including illustrative examples.• AI-enabled “nudification” technology that creates images where people appear to be nude—including apps thatenable non-technical users to create or alter images of individuals without their consent—has proliferated at analarming rate. Such technology is becoming a common form of image-based abuse that disproportionatelyimpacts women. As these tools become more sophisticated, they are producing altered images that are increasing-ly realistic and are difficult for both humans and AI to detect as inauthentic. Regardless of authenticity, the expe-rience of harm to victims of non-consensual intimate images can be devastatingly real—affecting their personaland professional lives, and impacting their mental and physical health.10• A company installed AI-powered cameras in its delivery vans in order to evaluate the road safety habits of its driv-ers, but the system incorrectly penalized drivers when other cars cut them off or when other events beyondtheir control took place on the road. As a result, drivers were incorrectly ineligible to receive a bonus.11WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMSThe expectations for automated systems are meant to serve as a blueprint for the development of additionaltechnical standards and practices that are tailored for particular sectors and contexts. In order to ensure that an automated system is safe and effective, it should include safeguards to protect thepublic from harm in a proactive and ongoing manner; avoid use of data inappropriate for or irrelevant to the taskat hand, including reuse that could cause compounded harm; and demonstrate the safety and effectiveness ofthe system. These expectations are explained below. Protect the public from harm in a proactive and ongoing manner Consultation. The public should be consulted in the design, implementation, deployment, acquisition, andmaintenance phases of automated system development, with emphasis on early-stage consultation before asystem is introduced or a large change implemented. This consultation should directly engage diverse impact-ed communities to consider concerns and risks that may be unique to those communities, or disproportionate-ly prevalent or severe for them. The extent of this engagement and the form of outreach to relevant stakehold-ers may differ depending on the specific automated system and development phase, but should includesubject matter, sector-specific, and context-specific experts as well as experts on potential impacts such ascivil rights, civil liberties, and privacy experts. For private sector applications, consultations before productlaunch may need to be confidential. Government applications, particularly law enforcement applications orapplications that raise national security considerations, may require confidential or limited engagement basedon system sensitivities and preexisting oversight laws and structures. Concerns raised in this consultationshould be documented, and the automated system developers were proposing to create, use, or deploy shouldbe reconsidered based on this feedback. Testing. Systems should undergo extensive testing before deployment. This testing should followdomain-specific best practices, when available, for ensuring the technology will work in its real-worldcontext. Such testing should take into account both the specific technology used and the roles of any humanoperators or reviewers who impact system outcomes or effectiveness; testing should include both automatedsystems testing and human-led (manual) testing. Testing conditions should mirror as closely as possible theconditions in which the system will be deployed, and new testing may be required for each deployment toaccount for material differences in conditions from one deployment to another. Following testing, systemperformance should be compared with the in-place, potentially human-driven, status quo procedures, withexisting human performance considered as a performance baseline for the algorithm to meet pre-deployment, and as a lifecycle minimum performance standard. Decision possibilities resulting from performance testingshould include the possibility of not deploying the system. Risk identification and mitigation. Before deployment, and in a proactive and ongoing manner, poten-tial risks of the automated system should be identified and mitigated. Identified risks should focus on thepotential for meaningful impact on people’s rights, opportunities, or access and include those to impactedcommunities that may not be direct users of the automated system, risks resulting from purposeful misuse ofthe system, and other concerns identified via the consultation process. Assessment and, where possible, mea-surement of the impact of risks should be included and balanced such that high impact risks receive attentionand mitigation proportionate with those impacts. Automated systems with the intended purpose of violatingthe safety of others should not be developed or used; systems with such safety violations as identified unin-tended consequences should not be used until the risk can be mitigated. Ongoing risk mitigation may necessi-tate rollback or significant modification to a launched automated system. WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMSThe expectations for automated systems are meant to serve as a blueprint for the development of additionaltechnical standards and practices that are tailored for particular sectors and contexts. Ongoing monitoring. Automated systems should have ongoing monitoring procedures, including recalibra-tion procedures, in place to ensure that their performance does not fall below an acceptable level over time, based on changing real-world conditions or deployment contexts, post-deployment modification, or unexpect-ed conditions. This ongoing monitoring should include continuous evaluation of performance metrics andharm assessments, updates of any systems, and retraining of any machine learning models as necessary, as wellas ensuring that fallback mechanisms are in place to allow reversion to a previously working system. Monitor-ing should take into account the performance of both technical system components (the algorithm as well asany hardware components, data inputs, etc.) and human operators. It should include mechanisms for testingthe actual accuracy of any predictions or recommendations generated by a system, not just a human operator’sdetermination of their accuracy. Ongoing monitoring procedures should include manual, human-led monitor-ing as a check in the event there are shortcomings in automated monitoring systems. These monitoring proce-dures should be in place for the lifespan of the deployed automated system. Clear organizational oversight. Entities responsible for the development or use of automated systemsshould lay out clear governance structures and procedures. This includes clearly-stated governance proce-dures before deploying the system, as well as responsibility of specific individuals or entities to oversee ongoingassessment and mitigation. Organizational stakeholders including those with oversight of the business processor operation being automated, as well as other organizational divisions that may be affected due to the use ofthe system, should be involved in establishing governance procedures. Responsibility should rest high enoughin the organization that decisions about resources, mitigation, incident response, and potential rollback can bemade promptly, with sufficient weight given to risk mitigation objectives against competing concerns. Thoseholding this responsibility should be made aware of any use cases with the potential for meaningful impact onpeople’s rights, opportunities, or access as determined based on risk identification procedures. In some cases, it may be appropriate for an independent ethics review to be conducted before deployment. Avoid inappropriate, low-quality, or irrelevant data use and the compounded harm of itsreuse Relevant and high-quality data. Data used as part of any automated system’s creation, evaluation, ordeployment should be relevant, of high quality, and tailored to the task at hand. Relevancy should beestablished based on research-backed demonstration of the causal influence of the data to the specific use caseor justified more generally based on a reasonable expectation of usefulness in the domain and/or for thesystem design or ongoing development. Relevance of data should not be established solely by appealing toits historical connection to the outcome. High quality and tailored data should be representative of the task athand and errors from data entry or other sources should be measured and limited. Any data used as the targetof a prediction process should receive particular attention to the quality and validity of the predicted outcomeor label to ensure the goal of the automated system is appropriately identified and measured. Additionally, justification should be documented for each data attribute and source to explain why it is appropriate to usethat data to inform the results of the automated system and why such use will not violate any applicable laws. In cases of high-dimensional and/or derived attributes, such justifications can be provided as overalldescriptions of the attribute generation process and appropriateness. WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMSThe expectations for automated systems are meant to serve as a blueprint for the development of additionaltechnical standards and practices that are tailored for particular sectors and contexts. Derived data sources tracked and reviewed carefully. Data that is derived from other data throughthe use of algorithms, such as data derived or inferred from prior model outputs, should be identified andtracked, e. g., via a specialized type in a data schema. Derived data should be viewed as potentially high-riskinputs that may lead to feedback loops, compounded harm, or inaccurate results. Such sources should be care-fully validated against the risk of collateral consequences. Data reuse limits in sensitive domains. Data reuse, and especially data reuse in a new context, can resultin the spreading and scaling of harms. Data from some domains, including criminal justice data and data indi-cating adverse outcomes in domains such as finance, employment, and housing, is especially sensitive, and insome cases its reuse is limited by law. Accordingly, such data should be subject to extra oversight to ensuresafety and efficacy. Data reuse of sensitive domain data in other contexts (e. g., criminal data reuse for civil legalmatters or private sector use) should only occur where use of such data is legally authorized and, after examina-tion, has benefits for those impacted by the system that outweigh identified risks and, as appropriate, reason-able measures have been implemented to mitigate the identified risks. Such data should be clearly labeled toidentify contexts for limited reuse based on sensitivity. Where possible, aggregated datasets may be useful forreplacing individual-level sensitive data. Demonstrate the safety and effectiveness of the system Independent evaluation. Automated systems should be designed to allow for independent evaluation (e. g., via application programming interfaces). Independent evaluators, such as researchers, journalists, ethicsreview boards, inspectors general, and third-party auditors, should be given access to the system and samplesof associated data, in a manner consistent with privacy, security, law, or regulation (including, e. g., intellectualproperty law), in order to perform such evaluations. Mechanisms should be included to ensure that systemaccess for evaluation is: provided in a timely manner to the deployment-ready version of the system; trusted toprovide genuine, unfiltered access to the full system; and truly independent such that evaluator access cannotbe revoked without reasonable and verified justification. Reporting.12 Entities responsible for the development or use of automated systems should provideregularly-updated reports that include: an overview of the system, including how it is embedded in theorganization’s business processes or other activities, system goals, any human-run procedures that form apart of the system, and specific performance expectations; a description of any data used to train machinelearning models or for other purposes, including how data sources were processed and interpreted, asummary of what data might be missing, incomplete, or erroneous, and data relevancy justifications; theresults of public consultation such as concerns raised and any decisions made due to these concerns; riskidentification and management assessments and any steps taken to mitigate potential harms; the results ofperformance testing including, but not limited to, accuracy, differential demographic impact, resultingerror rates (overall and per demographic group), and comparisons to previously deployed systems; ongoing monitoring procedures and regular performance testing reports, including monitoring frequency, results, and actions taken; and the procedures for and results from independent evaluations. Reportingshould be provided in a plain language and machine-readable manner. HOW THESE PRINCIPLES CAN MOVE INTO PRACTICEReal-life examples of how these principles can become reality, through laws, policies, and practicaltechnical and sociotechnical approaches to protecting rights, opportunities, and access. Executive Order 13960 on Promoting the Use of Trustworthy Artificial Intelligence in the Federal Government requires that certain federal agencies adhere to nine principles whendesigning, developing, acquiring, or using AI for purposes other than national security ordefense. These principles—while taking into account the sensitive law enforcement and other contexts in whichthe federal government may use AI, as opposed to private sector use of AI—require that AI is: (a) lawful andrespectful of our Nation’s values; (b) purposeful and performance-driven; (c) accurate, reliable, and effective; (d)safe, secure, and resilient; (e) understandable; (f ) responsible and traceable; (g) regularly monitored; (h) transpar-ent; and, (i) accountable. The Blueprint for an AI Bill of Rights is consistent with the Executive Order. Affected agencies across the federal government have released AI use case inventories13 and are implementingplans to bring those AI systems into compliance with the Executive Order or retire them. The law and policy landscape for motor vehicles shows that strong safety regulations—andmeasures to address harms when they occur—can enhance innovation in the context of com-plex technologies. Cars, like automated digital systems, comprise a complex collection of components. The National Highway Traffic Safety Administration,14 through its rigorous standards and independentevaluation, helps make sure vehicles on our roads are safe without limiting manufacturers’ ability toinnovate.15 At the same time, rules of the road are implemented locally to impose contextually appropriaterequirements on drivers, such as slowing down near schools or playgrounds.16From large companies to start-ups, industry is providing innovative solutions that alloworganizations to mitigate risks to the safety and efficacy of AI systems, both beforedeployment and through monitoring over time.17 These innovative solutions include riskassessments, auditing mechanisms, assessment of organizational procedures, dashboards to allow for ongoingmonitoring, documentation procedures specific to model assessments, and many other strategies that aim tomitigate risks posed by the use of AI to companies’ reputation, legal responsibilities, and other product safetyand effectiveness concerns. The Office of Management and Budget (OMB) has called for an expansion of opportunitiesfor meaningful stakeholder engagement in the design of programs and services. OMB alsopoints to numerous examples of effective and proactive stakeholder engagement, including the Community-Based Participatory Research Program developed by the National Institutes of Health and the participatorytechnology assessments developed by the National Oceanic and Atmospheric Administration.18The National Institute of Standards and Technology (NIST) is developing a riskmanagement framework to better manage risks posed to individuals, organizations, andsociety by AI.19 The NIST AI Risk Management Framework, as mandated by Congress, is intended forvoluntary use to help incorporate trustworthiness considerations into the design, development, use, andevaluation of AI products, services, and systems. The NIST framework is being developed through a consensus-driven, open, transparent, and collaborative process that includes workshops and other opportunities to provideinput. The NIST framework aims to foster the development of innovative approaches to addresscharacteristics of trustworthiness including accuracy, explainability and interpretability, reliability, privacy, robustness, safety, security (resilience), and mitigation of unintended and/or harmful bias, as well as ofharmful uses. The NIST framework will consider and encompass principles such astransparency, accountability, and fairness during pre-design, design and development, deployment, use, and testing and evaluation of AI technologies and systems. It is expected to be released in the winter of 2022-23. HOW THESE PRINCIPLES CAN MOVE INTO PRACTICEReal-life examples of how these principles can become reality, through laws, policies, and practicaltechnical and sociotechnical approaches to protecting rights, opportunities, and access. Some U. S government agencies have developed specific frameworks for ethical use of AIsystems. The Department of Energy (DOE) has activated the AI Advancement Council that oversees coordina-tion and advises on implementation of the DOE AI Strategy and addresses issues and/or escalations on theethical use and development of AI systems.20 The Department of Defense has adopted Artificial Intelligence Ethical Principles, and tenets for Responsible Artificial Intelligence specifically tailored to its nationalsecurity and defense activities.21 Similarly, the U. S. Intelligence Community (IC) has developed the Principlesof Artificial Intelligence Ethics for the Intelligence Community to guide personnel on whether and how todevelop and use AI in furtherance of the IC's mission, as well as an AI Ethics Framework to help implementthese principles.22The National Science Foundation (NSF) funds extensive research to help foster thedevelopment of automated systems that adhere to and advance their safety, security andeffectiveness. Multiple NSF programs support research that directly addresses many of these principles: the National AI Research Institutes23 support research on all aspects of safe, trustworthy, fair, and explainable AI algorithms and systems; the Cyber Physical Systems24 program supports research on developing safeautonomous and cyber physical systems with AI components; the Secure and Trustworthy Cyberspace25program supports research on cybersecurity and privacy enhancing technologies in automated systems; the Formal Methods in the Field26 program supports research on rigorous formal verification and analysis ofautomated systems and machine learning, and the Designing Accountable Software Systems27 program supportsresearch on rigorous and reproducible methodologies for developing software systems with legal and regulatorycompliance in mind. Some state legislatures have placed strong transparency and validity requirements onthe use of pretrial risk assessments. The use of algorithmic pretrial risk assessments has been acause of concern for civil rights groups.28 Idaho Code Section 19-1910, enacted in 2019,29 requires that anypretrial risk assessment, before use in the state, first be "shown to be free of bias against any class ofindividuals protected from discrimination by state or federal law", that any locality using a pretrial riskassessment must first formally validate the claim of its being free of bias, that "all documents, records, andinformation used to build or validate the risk assessment shall be open to public inspection," and that assertionsof trade secrets cannot be used "to quash discovery in a criminal matter by a party to a criminal case."A D Protections LGORITHMIC ISCRIMINATIONYou should not face discrimination by algorithmsand systems should be used and designed in anequitable way. Algorithmic discrimination occurs whenautomated systems contribute to unjustified different treatment orimpacts disfavoring people based on their race, color, ethnicity, sex (including pregnancy, childbirth, and related medicalconditions, gender identity, intersex status, and sexualorientation), religion, age, national origin, disability, veteran status, genetic infor-mation, or any other classification protected by law. Depending on the specific circumstances, such algorithmicdiscrimination may violate legal protections. Designers, developers, and deployers of automated systems should take proactive andcontinuous measures to protect individuals and communitiesfrom algorithmic discrimination and to use and design systems inan equitable way. This protection should include proactive equityassessments as part of the system design, use of representative dataand protection against proxies for demographic features, ensuringaccessibility for people with disabilities in design and development, pre-deployment and ongoing disparity testing and mitigation, andclear organizational oversight. Independent evaluation and plainlanguage reporting in the form of an algorithmic impact assessment, including disparity testing results and mitigation information, should be performed and made public whenever possible to confirmthese protections. WHY THIS PRINCIPLE IS IMPORTANTThis section provides a brief summary of the problems which the principle seeks to address and protectagainst, including illustrative examples. There is extensive evidence showing that automated systems can produce inequitable outcomes and amplifyexisting inequity.30 Data that fails to account for existing systemic biases in American society can result in a range ofconsequences. For example, facial recognition technology that can contribute to wrongful and discriminatoryarrests,31 hiring algorithms that inform discriminatory decisions, and healthcare algorithms that discountthe severity of certain diseases in Black Americans. Instances of discriminatory practices built into andresulting from AI and other automated systems exist across many industries, areas, and contexts. While automatedsystems have the capacity to drive extraordinary advances and innovations, algorithmic discriminationprotections should be built into their design, deployment, and ongoing use. Many companies, non-profits, and federal government agencies are already taking steps to ensure the publicis protected from algorithmic discrimination. Some companies have instituted bias testing as part of their productquality assessment and launch procedures, and in some cases this testing has led products to be changed or notlaunched, preventing harm to the public. Federal government agencies have been developing standards and guidancefor the use of automated systems in order to help prevent bias. Non-profits and companies have developed bestpractices for audits and impact assessments to help identify potential algorithmic discrimination and providetransparency to the public in the mitigation of such biases. But there is much more work to do to protect the public from algorithmic discrimination to use and designautomated systems in an equitable way. The guardrails protecting the public from discrimination in their dailylives should include their digital lives and impacts—basic safeguards against abuse, bias, and discrimination toensure that all people are treated fairly when automated systems are used. This includes all dimensions of theirlives, from hiring to loan approvals, from medical treatment and payment to encounters with the criminaljustice system. Ensuring equity should also go beyond existing guardrails to consider the holistic impact thatautomated systems make on underserved communities and to institute proactive protections that support thesecommunities.• An automated system using nontraditional factors such as educational attainment and employment history aspart of its loan underwriting and pricing model was found to be much more likely to charge an applicant whoattended a Historically Black College or University (HBCU) higher loan prices for refinancing a student loanthan an applicant who did not attend an HBCU. This was found to be true even when controlling forother credit-related factors.32• A hiring tool that learned the features of a company's employees (predominantly men) rejected women appli-cants for spurious and discriminatory reasons; resumes with the word “women’s,” such as “women’schess club captain,” were penalized in the candidate ranking.33• A predictive model marketed as being able to predict whether students are likely to drop out of school wasused by more than 500 universities across the country. The model was found to use race directly as a predictor, and also shown to have large disparities by race; Black students were as many as four times as likely as theirotherwise similar white peers to be deemed at high risk of dropping out. These risk scores are used by advisorsto guide students towards or away from majors, and some worry that they are being used to guide Black students away from math and science subjects.34• A risk assessment tool designed to predict the risk of recidivism for individuals in federal custody showedevidence of disparity in prediction. The tool overpredicts the risk of recidivism for some groups of color on thegeneral recidivism tools, and underpredicts the risk of recidivism for some groups of color on some of theviolent recidivism tools. The Department of Justice is working to reduce these disparities and has WHY THIS PRINCIPLE IS IMPORTANTThis section provides a brief summary of the problems which the principle seeks to address and protectagainst, including illustrative examples.• An automated sentiment analyzer, a tool often used by technology platforms to determine whether a state-ment posted online expresses a positive or negative sentiment, was found to be biased against Jews and gaypeople. For example, the analyzer marked the statement “I’m a Jew” as representing a negative sentiment, while “I’m a Christian” was identified as expressing a positive sentiment.36 This could lead to thepreemptive blocking of social media comments such as: “I’m gay.” A related company with this bias concernhas made their data public to encourage researchers to help address the issue37 and has released reportsidentifying and measuring this problem as well as detailing attempts to address it.38• Searches for “Black girls,” “Asian girls,” or “Latina girls” return predominantly39 sexualized content, ratherthan role models, toys, or activities.40 Some search engines have been working to reduce the prevalence ofthese results, but the problem remains.41• Advertisement delivery systems that predict who is most likely to click on a job advertisement end up deliv-ering ads in ways that reinforce racial and gender stereotypes, such as overwhelmingly directing supermar-ket cashier ads to women and jobs with taxi companies to primarily Black people.42• Body scanners, used by TSA at airport checkpoints, require the operator to select a “male” or “female”scanning setting based on the passenger’s sex, but the setting is chosen based on the operator’s perception ofthe passenger’s gender identity. These scanners are more likely to flag transgender travelers as requiringextra screening done by a person. Transgender travelers have described degrading experiences associatedwith these extra screenings.43 TSA has recently announced plans to implement a gender-neutral algorithm44while simultaneously enhancing the security effectiveness capabilities of the existing technology.• The National Disabled Law Students Association expressed concerns that individuals with disabilities weremore likely to be flagged as potentially suspicious by remote proctoring AI systems because of their disabili-ty-specific access needs such as needing longer breaks or using screen readers or dictation software.45• An algorithm designed to identify patients with high needs for healthcare systematically assigned lowerscores (indicating that they were not as high need) to Black patients than to those of white patients, evenwhen those patients had similar numbers of chronic conditions and other markers of health.46 In addition, healthcare clinical algorithms that are used by physicians to guide clinical decisions may includesociodemographic variables that adjust or “correct” the algorithm’s output on the basis of a patient’s race orethnicity, which can lead to race-based health inequities.47WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMSThe expectations for automated systems are meant to serve as a blueprint for the development of additionaltechnical standards and practices that are tailored for particular sectors and contexts. Any automated system should be tested to help ensure it is free from algorithmic discrimination before it can besold or used. Protection against algorithmic discrimination should include designing to ensure equity, broadlyconstrued. Some algorithmic discrimination is already prohibited under existing anti-discrimination law. Theexpectations set out below describe proactive technical and policy steps that can be taken to not onlyreinforce those legal protections but extend beyond them to ensure equity for underserved communities48even in circumstances where a specific legal protection may not be clearly established. These protectionsshould be instituted throughout the design, development, and deployment process and are described belowroughly in the order in which they would be instituted. Protect the public from algorithmic discrimination in a proactive and ongoing manner Proactive assessment of equity in design. Those responsible for the development, use, or oversight ofautomated systems should conduct proactive equity assessments in the design phase of the technologyresearch and development or during its acquisition to review potential input data, associated historicalcontext, accessibility for people with disabilities, and societal goals to identify potential discrimination andeffects on equity resulting from the introduction of the technology. The assessed groups should be as inclusiveas possible of the underserved communities mentioned in the equity definition: Black, Latino, and Indigenousand Native American persons, Asian Americans and Pacific Islanders and other persons of color; members ofreligious minorities; women, girls, and non-binary people; lesbian, gay, bisexual, transgender, queer, and inter-sex (LGBTQI+) persons; older adults; persons with disabilities; persons who live in rural areas; and personsotherwise adversely affected by persistent poverty or inequality. Assessment could include both qualitativeand quantitative evaluations of the system. This equity assessment should also be considered a core part of thegoals of the consultation conducted as part of the safety and efficacy review. Representative and robust data. Any data used as part of system development or assessment should berepresentative of local communities based on the planned deployment setting and should be reviewed for biasbased on the historical and societal context of the data. Such data should be sufficiently robust to identify andhelp to mitigate biases and potential harms. Guarding against proxies. Directly using demographic information in the design, development, ordeployment of an automated system (for purposes other than evaluating a system for discrimination or usinga system to counter discrimination) runs a high risk of leading to algorithmic discrimination and should beavoided. In many cases, attributes that are highly correlated with demographic features, known as proxies, cancontribute to algorithmic discrimination. In cases where use of the demographic features themselves wouldlead to illegal algorithmic discrimination, reliance on such proxies in decision-making (such as that facilitatedby an algorithm) may also be prohibited by law. Proactive testing should be performed to identify proxies bytesting for correlation between demographic information and attributes in any data used as part of systemdesign, development, or use. If a proxy is identified, designers, developers, and deployers should remove theproxy; if needed, it may be possible to identify alternative attributes that can be used instead. At a minimum, organizations should ensure a proxy feature is not given undue weight and should monitor the system closelyfor any resulting algorithmic discrimination. WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMSThe expectations for automated systems are meant to serve as a blueprint for the development of additionaltechnical standards and practices that are tailored for particular sectors and contexts. Ensuring accessibility during design, development, and deployment. Systems should bedesigned, developed, and deployed by organizations in ways that ensure accessibility to people with disabili-ties. This should include consideration of a wide variety of disabilities, adherence to relevant accessibilitystandards, and user experience research both before and after deployment to identify and address any accessi-bility barriers to the use or effectiveness of the automated system. Disparity assessment. Automated systems should be tested using a broad set of measures to assess wheth-er the system components, both in pre-deployment testing and in-context deployment, produce disparities. The demographics of the assessed groups should be as inclusive as possible of race, color, ethnicity, sex(including pregnancy, childbirth, and related medical conditions, gender identity, intersex status, and sexualorientation), religion, age, national origin, disability, veteran status, genetic information, or any other classifi-cation protected by law. The broad set of measures assessed should include demographic performance mea-sures, overall and subgroup parity assessment, and calibration. Demographic data collected for disparityassessment should be separated from data used for the automated system and privacy protections should beinstituted; in some cases it may make sense to perform such assessment using a data sample. For everyinstance where the deployed automated system leads to different treatment or impacts disfavoring the identi-fied groups, the entity governing, implementing, or using the system should document the disparity and ajustification for any continued use of the system. Disparity mitigation. When a disparity assessment identifies a disparity against an assessed group, it maybe appropriate to take steps to mitigate or eliminate the disparity. In some cases, mitigation or elimination ofthe disparity may be required by law. Disparities that have the potential to lead to algorithmicdiscrimination, cause meaningful harm, or violate equity49 goals should be mitigated. When designing andevaluating an automated system, steps should be taken to evaluate multiple models and select the one thathas the least adverse impact, modify data input choices, or otherwise identify a system with fewerdisparities. If adequate mitigation of the disparity is not possible, then the use of the automated systemshould be reconsidered. One of the considerations in whether to use the system should be the validity of anytarget measure; unobservable targets may result in the inappropriate use of proxies. Meeting thesestandards may require instituting mitigation procedures and other protective measures to addressalgorithmic discrimination, avoid meaningful harm, and achieve equity goals. Ongoing monitoring and mitigation. Automated systems should be regularly monitored to assess algo-rithmic discrimination that might arise from unforeseen interactions of the system with inequities notaccounted for during the pre-deployment testing, changes to the system after deployment, or changes to thecontext of use or associated data. Monitoring and disparity assessment should be performed by the entitydeploying or using the automated system to examine whether the system has led to algorithmic discrimina-tion when deployed. This assessment should be performed regularly and whenever a pattern of unusualresults is occurring. It can be performed using a variety of approaches, taking into account whether and howdemographic information of impacted people is available, for example via testing with a sample of users or viaqualitative user experience research. Riskier and higher-impact systems should be monitored and assessedmore frequently. Outcomes of this assessment should include additional disparity mitigation, if needed, orfallback to earlier procedures in the case that equity standards are no longer met and can't be mitigated, andprior mechanisms provide better adherence to equity standards. WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMSThe expectations for automated systems are meant to serve as a blueprint for the development of additionaltechnical standards and practices that are tailored for particular sectors and contexts. Demonstrate that the system protects against algorithmic discrimination Independent evaluation. As described in the section on Safe and Effective Systems, entities should allowindependent evaluation of potential algorithmic discrimination caused by automated systems they use oroversee. In the case of public sector uses, these independent evaluations should be made public unless lawenforcement or national security restrictions prevent doing so. Care should be taken to balance individualprivacy with evaluation data access needs; in many cases, policy-based and/or technological innovations andcontrols allow access to such data without compromising privacy. Reporting. Entities responsible for the development or use of automated systems should providereporting of an appropriately designed algorithmic impact assessment,50 with clear specification of whoperforms the assessment, who evaluates the system, and how corrective actions are taken (if necessary) inresponse to the assessment. This algorithmic impact assessment should include at least: the results of anyconsultation, design stage equity assessments (potentially including qualitative analysis), accessibilitydesigns and testing, disparity testing, document any remaining disparities, and detail any mitigationimplementation and assessments. This algorithmic impact assessment should be made public wheneverpossible. Reporting should be provided in a clear and machine-readable manner using plain language toallow for more straightforward public accountability. HOW THESE PRINCIPLES CAN MOVE INTO PRACTICEReal-life examples of how these principles can become reality, through laws, policies, and practicaltechnical and sociotechnical approaches to protecting rights, opportunities, and access. The federal government is working to combat discrimination in mortgage lending. The Depart-ment of Justice has launched a nationwide initiative to combat redlining, which includes reviewing howlenders who may be avoiding serving communities of color are conducting targeted marketing and advertising.51This initiative will draw upon strong partnerships across federal agencies, including the Consumer Financial Protection Bureau and prudential regulators. The Action Plan to Advance Property Appraisal and Valuation Equity includes a commitment from the agencies that oversee mortgage lending to include anondiscrimination standard in the proposed rules for Automated Valuation Models.52The Equal Employment Opportunity Commission and the Department of Justice have clearlylaid out how employers’ use of AI and other automated systems can result indiscrimination against job applicants and employees with disabilities.53 The documents explainhow employers’ use of software that relies on algorithmic decision-making may violate existing requirementsunder Title I of the Americans with Disabilities Act (“ADA”). This technical assistance also provides practicaltips to employers on how to comply with the ADA, and to job applicants and employees who think that theirrights may have been violated. Disparity assessments identified harms to Black patients' healthcare access. A widelyused healthcare algorithm relied on the cost of each patient’s past medical care to predict future medical needs, recommending early interventions for the patients deemed most at risk. This process discriminatedagainst Black patients, who generally have less access to medical care and therefore have generated less costthan white patients with similar illness and need. A landmark study documented this pattern and proposedpractical ways that were shown to reduce this bias, such as focusing specifically on active chronic healthconditions or avoidable future costs related to emergency visits and hospitalization.54Large employers have developed best practices to scrutinize the data and models usedfor hiring. An industry initiative has developed Algorithmic Bias Safeguards for the Workforce, a structuredquestionnaire that businesses can use proactively when procuring software to evaluate workers. It coversspecific technical questions such as the training data used, model training process, biases identified, andmitigation steps employed.55Standards organizations have developed guidelines to incorporate accessibility criteriainto technology design processes. The most prevalent in the United States is the Access Board’s Section508 regulations,56 which are the technical standards for federal information communication technology (software, hardware, and web). Other standards include those issued by the International Organization for Standardization,57 and the World Wide Web Consortium Web Content Accessibility Guidelines,58 a globallyrecognized voluntary consensus standard for web content and other information and communicationstechnology. NIST has released Special Publication 1270, Towards a Standard for Identifying and Managing Biasin Artificial Intelligence.59 The special publication: describes the stakes and challenges of bias in artificialintelligence and provides examples of how and why it can chip away at public trust; identifies three categoriesof bias in AI – systemic, statistical, and human – and describes how and where they contribute to harms; anddescribes three broad challenges for mitigating bias – datasets, testing and evaluation, and human factors – andintroduces preliminary guidance for addressing them. Throughout, the special publication takes a socio-technical perspective to identifying and managing AI bias. D PATA RIVACYYou should be protected from abusive data practices via built-inprotections and you should have agency over how data aboutyou is used. You should be protected from violations of privacy throughdesign choices that ensure such protections are included by default, includingensuring that data collection conforms to reasonable expectations and thatonly data strictly necessary for the specific context is collected. Designers, de-velopers, and deployers of automated systems should seek your permissionand respect your decisions regarding collection, use, access, transfer, and de-letion of your data in appropriate ways and to the greatest extent possible; where not possible, alternative privacy by design safeguards should be used. Systems should not employ user experience and design decisions that obfus-cate user choice or burden users with defaults that are privacy invasive. Con-sent should only be used to justify collection of data in cases where it can beappropriately and meaningfully given. Any consent requests should be brief, be understandable in plain language, and give you agency over data collectionand the specific context of use; current hard-to-understand no-tice-and-choice practices for broad uses of data should be changed. Enhancedprotections and restrictions for data and inferences related to sensitive do-mains, including health, work, education, criminal justice, and finance, andfor data pertaining to youth should put you first. In sensitive domains, yourdata and related inferences should only be used for necessary functions, andyou should be protected by ethical review and use prohibitions. You and yourcommunities should be free from unchecked surveillance; surveillance tech-nologies should be subject to heightened oversight that includes at leastpre-deployment assessment of their potential harms and scope limits to pro-tect privacy and civil liberties. Continuous surveillance and monitoringshould not be used in education, work, housing, or in other contexts where theuse of such surveillance technologies is likely to limit rights, opportunities, oraccess. Whenever possible, you should have access to reporting that confirmsyour data decisions have been respected and provides an assessment of thepotential impact of surveillance technologies on your rights, opportunities, oraccess. WHY THIS PRINCIPLE IS IMPORTANTThis section provides a brief summary of the problems which the principle seeks to address and protectagainst, including illustrative examples. Data privacy is a foundational and cross-cutting principle required for achieving all others in this framework. Surveil-lance and data collection, sharing, use, and reuse now sit at the foundation of business models across many industries, with more and more companies tracking the behavior of the American public, building individual profiles based onthis data, and using this granular-level information as input into automated systems that further track, profile, andimpact the American public. Government agencies, particularly law enforcement agencies, also use and help developa variety of technologies that enhance and expand surveillance capabilities, which similarly collect data used as inputinto other automated systems that directly impact people’s lives. Federal law has not grown to address the expandingscale of private data collection, or of the ability of governments at all levels to access that data and leverage the meansof private collection. Meanwhile, members of the American public are often unable to access their personal data or make critical decisionsabout its collection and use. Data brokers frequently collect consumer data from numerous sources withoutconsumers’ permission or knowledge.60 Moreover, there is a risk that inaccurate and faulty data can be used tomake decisions about their lives, such as whether they will qualify for a loan or get a job. Use of surveillancetechnologies has increased in schools and workplaces, and, when coupled with consequential management andevaluation decisions, it is leading to mental health harms such as lowered self-confidence, anxiety, depression, anda reduced ability to use analytical reasoning.61 Documented patterns show that personal data is being aggregated bydata brokers to profile communities in harmful ways.62 The impact of all this data harvesting is corrosive, breeding distrust, anxiety, and other mental health problems; chilling speech, protest, and worker organizing; andthreatening our democratic process.63 The American public should be protected from these growing risks. Increasingly, some companies are taking these concerns seriously and integrating mechanisms to protect consumerprivacy into their products by design and by default, including by minimizing the data they collect, communicatingcollection and use clearly, and improving security practices. Federal government surveillance and other collection anduse of data is governed by legal protections that help to protect civil liberties and provide for limits on data retentionin some cases. Many states have also enacted consumer data privacy protection regimes to address some of theseharms. However, these are not yet standard practices, and the United States lacks a comprehensive statutory or regulatoryframework governing the rights of the public when it comes to personal data. While a patchwork of laws exists toguide the collection and use of personal data in specific contexts, including health, employment, education, and credit, it can be unclear how these laws apply in other contexts and in an increasingly automated society. Additional protec-tions would assure the American public that the automated systems they use are not monitoring their activities, collecting information on their lives, or otherwise surveilling them without context-specific consent or legal authori-ty. WHY THIS PRINCIPLE IS IMPORTANTThis section provides a brief summary of the problems which the principle seeks to address and protectagainst, including illustrative examples.• An insurer might collect data from a person's social media presence as part of deciding what lifeinsurance rates they should be offered.64• A data broker harvested large amounts of personal data and then suffered a breach, exposing hundreds ofthousands of people to potential identity theft. 65• A local public housing authority installed a facial recognition system at the entrance to housing complexes toassist law enforcement with identifying individuals viewed via camera when police reports are filed, leadingthe community, both those living in the housing complex and not, to have videos of them sent to the localpolice department and made available for scanning by its facial recognition software.66• Companies use surveillance software to track employee discussions about union activity and use theresulting data to surveil individual employees and surreptitiously intervene in discussions.67WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMSThe expectations for automated systems are meant to serve as a blueprint for the development of additionaltechnical standards and practices that are tailored for particular sectors and contexts. Traditional terms of service—the block of text that the public is accustomed to clicking through when using a web-site or digital app—are not an adequate mechanism for protecting privacy. The American public should be protect-ed via built-in privacy protections, data minimization, use and collection limitations, and transparency, in additionto being entitled to clear mechanisms to control access to and use of their data—including their metadata—in aproactive, informed, and ongoing way. Any automated system collecting, using, sharing, or storing personal datashould meet these expectations. Protect privacy by design and by default Privacy by design and by default. Automated systems should be designed and built with privacy protect-ed by default. Privacy risks should be assessed throughout the development life cycle, including privacy risksfrom reidentification, and appropriate technical and policy mitigation measures should be implemented. Thisincludes potential harms to those who are not users of the automated system, but who may be harmed byinferred data, purposeful privacy violations, or community surveillance or other community harms. Datacollection should be minimized and clearly communicated to the people whose data is collected. Data shouldonly be collected or used for the purposes of training or testing machine learning models if such collection anduse is legal and consistent with the expectations of the people whose data is collected. User experienceresearch should be conducted to confirm that people understand what data is being collected about them andhow it will be used, and that this collection matches their expectations and desires. Data collection and use-case scope limits. Data collection should be limited in scope, with specific, narrow identified goals, to avoid "mission creep." Anticipated data collection should be determined to bestrictly necessary to the identified goals and should be minimized as much as possible. Data collected based onthese identified goals and for a specific context should not be used in a different context without assessing fornew privacy risks and implementing appropriate mitigation measures, which may include express consent. Clear timelines for data retention should be established, with data deleted as soon as possible in accordancewith legal or policy-based limitations. Determined data retention timelines should be documented and justi-fied. Risk identification and mitigation. Entities that collect, use, share, or store sensitive data shouldattempt to proactively identify harms and seek to manage them so as to avoid, mitigate, and respond appropri-ately to identified risks. Appropriate responses include determining not to process data when the privacy risksoutweigh the benefits or implementing measures to mitigate acceptable risks. Appropriate responses do notinclude sharing or transferring the privacy risks to users via notice or consent requests where users could notreasonably be expected to understand the risks without further support. Privacy-preserving security. Entities creating, using, or governing automated systems should followprivacy and security best practices designed to ensure data and metadata do not leak beyond the specificconsented use case. Best practices could include using privacy-enhancing cryptography or other types ofprivacy-enhancing technologies or fine-grained permissions and access control mechanisms, along withconventional system security protocols. WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMSThe expectations for automated systems are meant to serve as a blueprint for the development of additionaltechnical standards and practices that are tailored for particular sectors and contexts. Protect the public from unchecked surveillance Heightened oversight of surveillance. Surveillance or monitoring systems should be subject toheightened oversight that includes at a minimum assessment of potential harms during design (before deploy-ment) and in an ongoing manner, to ensure that the American public’s rights, opportunities, and access areprotected. This assessment should be done before deployment and should give special attention to ensurethere is not algorithmic discrimination, especially based on community membership, when deployed in aspecific real-world context. Such assessment should then be reaffirmed in an ongoing manner as long as thesystem is in use. Limited and proportionate surveillance. Surveillance should be avoided unless it is strictly necessaryto achieve a legitimate purpose and it is proportionate to the need. Designers, developers, and deployers ofsurveillance systems should use the least invasive means of monitoring available and restrict monitoring to theminimum number of subjects possible. To the greatest extent possible consistent with law enforcement andnational security needs, individuals subject to monitoring should be provided with clear and specific noticebefore it occurs and be informed about how the data gathered through surveillance will be used. Scope limits on surveillance to protect rights and democratic values. Civil liberties and civilrights must not be limited by the threat of surveillance or harassment facilitated or aided by an automatedsystem. Surveillance systems should not be used to monitor the exercise of democratic rights, such as voting, privacy, peaceful assembly, speech, or association, in a way that limits the exercise of civil rights or civil liber-ties. Information about or algorithmically-determined assumptions related to identity should be carefullylimited if used to target or guide surveillance systems in order to avoid algorithmic discrimination; such iden-tity-related information includes group characteristics or affiliations, geographic designations, location-basedand association-based inferences, social networks, and biometrics. Continuous surveillance and monitoringsystems should not be used in physical or digital workplaces (regardless of employment status), public educa-tional institutions, and public accommodations. Continuous surveillance and monitoring systems should notbe used in a way that has the effect of limiting access to critical resources or services or suppressing the exer-cise of rights, even where the organization is not under a particular duty to protect those rights. Provide the public with mechanisms for appropriate and meaningful consent, access, andcontrol over their data Use-specific consent. Consent practices should not allow for abusive surveillance practices. Where datacollectors or automated systems seek consent, they should seek it for specific, narrow use contexts, for specif-ic time durations, and for use by specific entities. Consent should not extend if any of these conditions change; consent should be re-acquired before using data if the use case changes, a time limit elapses, or data is trans-ferred to another entity (including being shared or sold). Consent requested should be limited in scope andshould not request consent beyond what is required. Refusal to provide consent should be allowed, withoutadverse effects, to the greatest extent possible based on the needs of the use case. Brief and direct consent requests. When seeking consent from users short, plain language consentrequests should be used so that users understand for what use contexts, time span, and entities they areproviding data and metadata consent. User experience research should be performed to ensure these consentrequests meet performance standards for readability and comprehension. This includes ensuring that consentrequests are accessible to users with disabilities and are available in the language(s) and reading level appro-priate for the audience. User experience design choices that intentionally obfuscate or manipulate user WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMSThe expectations for automated systems are meant to serve as a blueprint for the development of additionaltechnical standards and practices that are tailored for particular sectors and contexts. Data access and correction. People whose data is collected, used, shared, or stored by automatedsystems should be able to access data and metadata about themselves, know who has access to this data, andbe able to correct it if necessary. Entities should receive consent before sharing data with other entities andshould keep records of what data is shared and with whom. Consent withdrawal and data deletion. Entities should allow (to the extent legally permissible) with-drawal of data access consent, resulting in the deletion of user data, metadata, and the timely removal oftheir data from any systems (e. g., machine learning models) derived from that data.68Automated system support. Entities designing, developing, and deploying automated systems shouldestablish and maintain the capabilities that will allow individuals to use their own automated systems to helpthem make consent, access, and control decisions in a complex data ecosystem. Capabilities include machinereadable data, standardized data formats, metadata or tags for expressing data processing permissions andpreferences and data provenance and lineage, context of use and access-specific tags, and training models forassessing privacy risk. Demonstrate that data privacy and user control are protected Independent evaluation. As described in the section on Safe and Effective Systems, entities should allowindependent evaluation of the claims made regarding data policies. These independent evaluations should bemade public whenever possible. Care will need to be taken to balance individual privacy with evaluation dataaccess needs. Reporting. When members of the public wish to know what data about them is being used in a system, theentity responsible for the development of the system should respond quickly with a report on the data it hascollected or stored about them. Such a report should be machine-readable, understandable by most users, andinclude, to the greatest extent allowable under law, any data and metadata about them or collected from them, when and how their data and metadata were collected, the specific ways that data or metadata are being used, who has access to their data and metadata, and what time limitations apply to these data. In cases where a userlogin is not available, identity verification may need to be performed before providing such a report to ensureuser privacy. Additionally, summary reporting should be proactively made public with general informationabout how peoples’ data and metadata is used, accessed, and stored. Summary reporting should include theresults of any surveillance pre-deployment assessment, including disparity assessment in the real-worlddeployment context, the specific identified goals of any data collection, and the assessment done to ensureonly the minimum required data is collected. It should also include documentation about the scope limitassessments, including data retention timelines and associated justification, and an assessment of theimpact of surveillance or data collection on rights, opportunities, and access. Where possible, thisassessment of the impact of surveillance should be done by an independent party. Reporting should beprovided in a clear and machine-readable manner. EPDR SXTRA ROTECTIONS FOR ATA ELATED TO ENSITIVEDOMAINSSome domains, including health, employment, education, criminal justice, and personal finance, have long beensingled out as sensitive domains deserving of enhanced data protections. This is due to the intimate nature of thesedomains as well as the inability of individuals to opt out of these domains in any meaningful way, and thehistorical discrimination that has often accompanied data knowledge.69 Domains understood by the public to besensitive also change over time, including because of technological developments. Tracking and monitoringtechnologies, personal tracking devices, and our extensive data footprints are used and misused more than everbefore; as such, the protections afforded by current legal guidelines may be inadequate. The American publicdeserves assurances that data related to such sensitive domains is protected and used appropriately and only innarrowly defined contexts with clear benefits to the individual and/or society. To this end, automated systems that collect, use, share, or store data related to these sensitive domains should meetadditional expectations. Data and metadata are sensitive if they pertain to an individual in a sensitive domain (definedbelow); are generated by technologies used in a sensitive domain; can be used to infer data from a sensitive domain orsensitive data about an individual (such as disability-related data, genomic data, biometric data, behavioral data, geolocation data, data related to interaction with the criminal justice system, relationship history and legal status suchas custody and divorce information, and home, work, or school environmental data); or have the reasonable potentialto be used in ways that are likely to expose individuals to meaningful harm, such as a loss of privacy or financial harmdue to identity theft. Data and metadata generated by or about those who are not yet legal adults is also sensitive, evenif not related to a sensitive domain. Such data includes, but is not limited to, numerical, text, image, audio, or videodata. “Sensitive domains” are those in which activities being conducted can cause material harms, including signifi-cant adverse effects on human rights such as autonomy and dignity, as well as civil liberties and civil rights. Domainsthat have historically been singled out as deserving of enhanced data protections or where such enhanced protectionsare reasonably expected by the public include, but are not limited to, health, family planning and care, employment, education, criminal justice, and personal finance. In the context of this framework, such domains are consideredsensitive whether or not the specifics of a system context would necessitate coverage under existing law, and domainsand data that are considered sensitive are understood to change over time based on societal norms and context. EPDR SXTRA ROTECTIONS FOR ATA ELATED TO ENSITIVEDOMAINS• Continuous positive airway pressure machines gather data for medical purposes, such as diagnosing sleepapnea, and send usage data to a patient’s insurance company, which may subsequently deny coverage for thedevice based on usage data. Patients were not aware that the data would be used in this way or monitoredby anyone other than their doctor.70• A department store company used predictive analytics applied to collected consumer data to determine that ateenage girl was pregnant, and sent maternity clothing ads and other baby-related advertisements to herhouse, revealing to her father that she was pregnant.71• School audio surveillance systems monitor student conversations to detect potential "stress indicators" asa warning of potential violence.72 Online proctoring systems claim to detect if a student is cheating on anexam using biometric markers.73 These systems have the potential to limit student freedom to express a rangeof emotions at school and may inappropriately flag students with disabilities who need accommodations oruse screen readers or dictation software as cheating.74• Location data, acquired from a data broker, can be used to identify people who visit abortion clinics.75• Companies collect student data such as demographic information, free or reduced lunch status, whetherthey've used drugs, or whether they've expressed interest in LGBTQI+ groups, and then use that data toforecast student success.76 Parents and education experts have expressed concern about collection of suchsensitive data without express parental consent, the lack of transparency in how such data is being used, andthe potential for resulting discriminatory impacts.• Many employers transfer employee data to third party job verification services. This information is then usedby potential future employers, banks, or landlords. In one case, a former employee alleged that acompany supplied false data about her job title which resulted in a job offer being revoked.77WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMSThe expectations for automated systems are meant to serve as a blueprint for the development of additionaltechnical standards and practices that are tailored for particular sectors and contexts. In addition to the privacy expectations above for general non-sensitive data, any system collecting, using, shar-ing, or storing sensitive data should meet the expectations below. Depending on the technological use case andbased on an ethical assessment, consent for sensitive data may need to be acquired from a guardian and/or child. Provide enhanced protections for data related to sensitive domains Necessary functions only. Sensitive data should only be used for functions strictly necessary for thatdomain or for functions that are required for administrative reasons (e. g., school attendance records), unlessconsent is acquired, if appropriate, and the additional expectations in this section are met. Consent for non-necessary functions should be optional, i. e., should not be required, incentivized, or coerced in order toreceive opportunities or access to services. In cases where data is provided to an entity (e. g., health insurancecompany) in order to facilitate payment for such a need, that data should only be used for that purpose. Ethical review and use prohibitions. Any use of sensitive data or decision process based in part on sensi-tive data that might limit rights, opportunities, or access, whether the decision is automated or not, should gothrough a thorough ethical review and monitoring, both in advance and by periodic review (e. g., via an indepen-dent ethics committee or similarly robust process). In some cases, this ethical review may determine that datashould not be used or shared for specific uses even with consent. Some novel uses of automated systems in thiscontext, where the algorithm is dynamically developing and where the science behind the use case is not wellestablished, may also count as human subject experimentation, and require special review under organizationalcompliance bodies applying medical, scientific, and academic human subject experimentation ethics rules andgovernance procedures. Data quality. In sensitive domains, entities should be especially careful to maintain the quality of data toavoid adverse consequences arising from decision-making based on flawed or inaccurate data. Such care isnecessary in a fragmented, complex data ecosystem and for datasets that have limited access such as for fraudprevention and law enforcement. It should be not left solely to individuals to carry the burden of reviewing andcorrecting data. Entities should conduct regular, independent audits and take prompt corrective measures tomaintain accurate, timely, and complete data. Limit access to sensitive data and derived data. Sensitive data and derived data should not be sold, shared, or made public as part of data brokerage or other agreements. Sensitive data includes data that can beused to infer sensitive information; even systems that are not directly marketed as sensitive domain technologiesare expected to keep sensitive data private. Access to such data should be limited based on necessity and basedon a principle of local control, such that those individuals closest to the data subject have more access whilethose who are less proximate do not (e. g., a teacher has access to their students’ daily progress data while asuperintendent does not). Reporting. In addition to the reporting on data privacy (as listed above for non-sensitive data), entities devel-oping technologies related to a sensitive domain and those collecting, using, storing, or sharing sensitive datashould, whenever appropriate, regularly provide public reports describing: any data security lapses or breachesthat resulted in sensitive data leaks; the number, type, and outcomes of ethical pre-reviews undertaken; adescription of any data sold, shared, or made public, and how that data was assessed to determine it did not pres-ent a sensitive data risk; and ongoing risk identification and management procedures, and any mitigation addedbased on these procedures. Reporting should be provided in a clear and machine-readable manner. HOW THESE PRINCIPLES CAN MOVE INTO PRACTICEReal-life examples of how these principles can become reality, through laws, policies, and practicaltechnical and sociotechnical approaches to protecting rights, opportunities, and access. The Privacy Act of 1974 requires privacy protections for personal information in federalrecords systems, including limits on data retention, and also provides individuals a generalright to access and correct their data. Among other things, the Privacy Act limits the storage of individualinformation in federal systems of records, illustrating the principle of limiting the scope of data retention. Underthe Privacy Act, federal agencies may only retain data about an individual that is “relevant and necessary” toaccomplish an agency’s statutory purpose or to comply with an Executive Order of the President. The law allowsfor individuals to be able to access any of their individual information stored in a federal system of records, if notincluded under one of the systems of records exempted pursuant to the Privacy Act. In these cases, federal agen-cies must provide a method for an individual to determine if their personal information is stored in a particularsystem of records, and must provide procedures for an individual to contest the contents of a record about them. Further, the Privacy Act allows for a cause of action for an individual to seek legal relief if a federal agency does notcomply with the Privacy Act’s requirements. Among other things, a court may order a federal agency to amend orcorrect an individual’s information in its records or award monetary damages if an inaccurate, irrelevant, untimely, or incomplete record results in an adverse determination about an individual’s “qualifications, character, rights, …opportunities…, or benefits.”NIST’s Privacy Framework provides a comprehensive, detailed and actionable approach fororganizations to manage privacy risks. The NIST Framework gives organizations ways to identify andcommunicate their privacy risks and goals to support ethical decision-making in system, product, and servicedesign or deployment, as well as the measures they are taking to demonstrate compliance with applicable lawsor regulations. It has been voluntarily adopted by organizations across many different sectors around the world.78A school board’s attempt to surveil public school students—undertaken withoutadequate community input—sparked a state-wide biometrics moratorium.79 Reacting to a plan inthe city of Lockport, New York, the state’s legislature banned the use of facial recognition systems and other“biometric identifying technology” in schools until July 1, 2022.80 The law additionally requires that a report onthe privacy, civil rights, and civil liberties implications of the use of such technologies be issued beforebiometric identification technologies can be used in New York schools. Federal law requires employers, and any consultants they may retain, to report the costsof surveilling employees in the context of a labor dispute, providing a transparencymechanism to help protect worker organizing. Employers engaging in workplace surveillance "wherean object there-of, directly or indirectly, is […] to obtain information concerning the activities of employees or alabor organization in connection with a labor dispute" must report expenditures relating to this surveillance tothe Department of Labor Office of Labor-Management Standards, and consultants who employers retain forthese purposes must also file reports regarding their activities.81Privacy choices on smartphones show that when technologies are well designed, privacyand data agency can be meaningful and not overwhelming. These choices—such as contextual, timelyalerts about location tracking—are brief, direct, and use-specific. Many of the expectations listed here forprivacy by design and use-specific consent mirror those distributed to developers as best practices whendeveloping for smart phone devices,82 such as being transparent about how user data will be used, asking for apppermissions during their use so that the use-context will be clear to users, and ensuring that the app will stillwork if users deny (or later revoke) some permissions. N EOTICE AND XPLANATIONYou should know that an automated system is being used, and understand how and why it contributes to outcomesthat impact you. Designers, developers, and deployers of automat-ed systems should provide generally accessible plain language docu-mentation including clear descriptions of the overall system func-tioning and the role automation plays, notice that such systems are inuse, the individual or organization responsible for the system, and ex-planations of outcomes that are clear, timely, and accessible. Suchnotice should be kept up-to-date and people impacted by the systemshould be notified of significant use case or key functionality chang-es. You should know how and why an outcome impacting you was de-termined by an automated system, including when the automatedsystem is not the sole input determining the outcome. Automatedsystems should provide explanations that are technically valid, meaningful and useful to you and to any operators or others whoneed to understand the system, and calibrated to the level of riskbased on the context. Reporting that includes summary informationabout these automated systems in plain language and assessments ofthe clarity and quality of the notice and explanations should be madepublic whenever possible. WHY THIS PRINCIPLE IS IMPORTANTThis section provides a brief summary of the problems which the principle seeks to address and protectagainst, including illustrative examples. Automated systems now determine opportunities, from employment to credit, and directly shape the Americanpublic’s experiences, from the courtroom to online classrooms, in ways that profoundly impact people’s lives. But thisexpansive impact is not always visible. An applicant might not know whether a person rejected their resume or ahiring algorithm moved them to the bottom of the list. A defendant in the courtroom might not know if a judge deny-ing their bail is informed by an automated system that labeled them “high risk.” From correcting errors to contestingdecisions, people are often denied the knowledge they need to address the impact of automated systems on their lives. Notice and explanations also serve an important safety and efficacy purpose, allowing experts to verify the reasonable-ness of a recommendation before enacting it. In order to guard against potential harms, the American public needs to know if an automated system is being used. Clear, brief, and understandable notice is a prerequisite for achieving the other protections in this framework. Like-wise, the public is often unable to ascertain how or why an automated system has made a decision or contributed to aparticular outcome. The decision-making processes of automated systems tend to be opaque, complex, and, therefore, unaccountable, whether by design or by omission. These factors can make explanations both more challenging andmore important, and should not be used as a pretext to avoid explaining important decisions to the people impactedby those choices. In the context of automated systems, clear and valid explanations should be recognized as a baselinerequirement. Providing notice has long been a standard practice, and in many cases is a legal requirement, when, for example, making a video recording of someone (outside of a law enforcement or national security context). In some cases, suchas credit, lenders are required to provide notice and explanation to consumers. Techniques used to automate theprocess of explaining such systems are under active research and improvement and such explanations can take manyforms. Innovative companies and researchers are rising to the challenge and creating and deploying explanatorysystems that can help the public better understand decisions that impact them. While notice and explanation requirements are already in place in some sectors or situations, the American publicdeserve to know consistently and across sectors if an automated system is being used in a way that impacts their rights, opportunities, or access. This knowledge should provide confidence in how the public is being treated, and trust in thevalidity and reasonable use of automated systems.• A lawyer representing an older client with disabilities who had been cut off from Medicaid-funded homehealth-care assistance couldn't determine why, especially since the decision went against historical accesspractices. In a court hearing, the lawyer learned from a witness that the state in which the older clientlived had recently adopted a new algorithm to determine el igibility.83 The lack of a timely explanation made itharder to understand and contest the decision.• A formal child welfare investigation is opened against a parent based on an algorithm and without the parentever being notified that data was being collected and used as part of an algorithmic child maltreatmentrisk assessment.84 The lack of notice or an explanation makes it harder for those performing childmaltreatment assessments to validate the risk assessment and denies parents knowledge that could help themcontest a decision. WHY THIS PRINCIPLE IS IMPORTANTThis section provides a brief summary of the problems which the principle seeks to address and protectagainst, including illustrative examples.• A predictive policing system claimed to identify individuals at greatest risk to commit or become the victim ofgun violence (based on automated analysis of social ties to gang members, criminal histories, previous experi-ences of gun violence, and other factors) and led to individuals being placed on a watch list with noexplanation or public transparency regarding how the system came to its conclusions.85 Both police andthe public deserve to understand why and how such a system is making these determinations.• A system awarding benefits changed its criteria invisibly. Individuals were denied benefits due to data entryerrors and other system flaws. These flaws were only revealed when an explanation of the systemwas demanded and produced.86 The lack of an explanation made it harder for errors to be corrected in atimely manner. WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMSThe expectations for automated systems are meant to serve as a blueprint for the development of additionaltechnical standards and practices that are tailored for particular sectors and contexts. An automated system should provide demonstrably clear, timely, understandable, and accessible notice of use, andexplanations as to how and why a decision was made or an action was taken by the system. These expectations areexplained below. Provide clear, timely, understandable, and accessible notice of use and explanations Generally accessible plain language documentation. The entity responsible for using the automatedsystem should ensure that documentation describing the overall system (including any human components) ispublic and easy to find. The documentation should describe, in plain language, how the system works and howany automated component is used to determine an action or decision. It should also include expectations aboutreporting described throughout this framework, such as the algorithmic impact assessments described aspart of Algorithmic Discrimination Protections. Accountable. Notices should clearly identify the entity responsible for designing each component of thesystem and the entity using it. Timely and up-to-date. Users should receive notice of the use of automated systems in advance of using orwhile being impacted by the technology. An explanation should be available with the decision itself, or soonthereafter. Notice should be kept up-to-date and people impacted by the system should be notified of use caseor key functionality changes. Brief and clear. Notices and explanations should be assessed, such as by research on users’ experiences, including user testing, to ensure that the people using or impacted by the automated system are able to easilyfind notices and explanations, read them quickly, and understand and act on them. This includes ensuring thatnotices and explanations are accessible to users with disabilities and are available in the language(s) and read-ing level appropriate for the audience. Notices and explanations may need to be available in multiple forms,(e. g., on paper, on a physical sign, or online), in order to meet these expectations and to be accessible to the American public. Provide explanations as to how and why a decision was made or an action was taken by anautomated system Tailored to the purpose. Explanations should be tailored to the specific purpose for which the user isexpected to use the explanation, and should clearly state that purpose. An informational explanation mightdiffer from an explanation provided to allow for the possibility of recourse, an appeal, or one provided in thecontext of a dispute or contestation process. For the purposes of this framework, 'explanation' should beconstrued broadly. An explanation need not be a plain-language statement about causality but could consist ofany mechanism that allows the recipient to build the necessary understanding and intuitions to achieve thestated purpose. Tailoring should be assessed (e. g., via user experience research). Tailored to the target of the explanation. Explanations should be targeted to specific audiences andclearly state that audience. An explanation provided to the subject of a decision might differ from one providedto an advocate, or to a domain expert or decision maker. Tailoring should be assessed (e. g., via user experienceresearch). WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMSThe expectations for automated systems are meant to serve as a blueprint for the development of additionaltechnical standards and practices that are tailored for particular sectors and contexts. Tailored to the level of risk. An assessment should be done to determine the level of risk of the auto-mated system. In settings where the consequences are high as determined by a risk assessment, or extensiveoversight is expected (e. g., in criminal justice or some public sector settings), explanatory mechanisms shouldbe built into the system design so that the system’s full behavior can be explained in advance (i. e., only fullytransparent models should be used), rather than as an after-the-decision interpretation. In other settings, theextent of explanation provided should be tailored to the risk level. Valid. The explanation provided by a system should accurately reflect the factors and the influences that ledto a particular decision, and should be meaningful for the particular customization based on purpose, target, and level of risk. While approximation and simplification may be necessary for the system to succeed based onthe explanatory purpose and target of the explanation, or to account for the risk of fraud or other concernsrelated to revealing decision-making information, such simplifications should be done in a scientificallysupportable way. Where appropriate based on the explanatory system, error ranges for the explanation shouldbe calculated and included in the explanation, with the choice of presentation of such information balancedwith usability and overall interface complexity concerns. Demonstrate protections for notice and explanation Reporting. Summary reporting should document the determinations made based on the above consider-ations, including: the responsible entities for accountability purposes; the goal and use cases for the system, identified users, and impacted populations; the assessment of notice clarity and timeliness; the assessment ofthe explanation's validity and accessibility; the assessment of the level of risk; and the account and assessmentof how explanations are tailored, including to the purpose, the recipient of the explanation, and the level ofrisk. Individualized profile information should be made readily available to the greatest extent possible thatincludes explanations for any system impacts or inferences. Reporting should be provided in a clear plainlanguage and machine-readable manner. HOW THESE PRINCIPLES CAN MOVE INTO PRACTICEReal-life examples of how these principles can become reality, through laws, policies, and practicaltechnical and sociotechnical approaches to protecting rights, opportunities, and access. People in Illinois are given written notice by the private sector if their biometric informa-tion is used. The Biometric Information Privacy Act enacted by the state contains a number of provisionsconcerning the use of individual biometric data and identifiers. Included among them is a provision that no privateentity may "collect, capture, purchase, receive through trade, or otherwise obtain" such information about anindividual, unless written notice is provided to that individual or their legally appointed representative. 87Major technology companies are piloting new ways to communicate with the public abouttheir automated technologies. For example, a collection of non-profit organizations and companies haveworked together to develop a framework that defines operational approaches to transparency for machinelearning systems.88 This framework, and others like it,89 inform the public about the use of these tools, goingbeyond simple notice to include reporting elements such as safety evaluations, disparity assessments, andexplanations of how the systems work. Lenders are required by federal law to notify consumers about certain decisions made aboutthem. Both the Fair Credit Reporting Act and the Equal Credit Opportunity Act require in certain circumstancesthat consumers who are denied credit receive "adverse action" notices. Anyone who relies on the information in acredit report to deny a consumer credit must, under the Fair Credit Reporting Act, provide an "adverse action"notice to the consumer, which includes "notice of the reasons a creditor took adverse action on the applicationor on an existing credit account."90 In addition, under the risk-based pricing rule,91 lenders must either informborrowers of their credit score, or else tell consumers when "they are getting worse terms because ofinformation in their credit report." The CFPB has also asserted that "[t]he law gives every applicant the right toa specific explanation if their application for credit was denied, and that right is not diminished simply becausea company uses a complex algorithm that it doesn't understand."92 Such explanations illustrate a shared valuethat certain decisions need to be explained. A California law requires that warehouse employees are provided with notice and explana-tion about quotas, potentially facilitated by automated systems, that apply to them. Warehous-ing employers in California that use quota systems (often facilitated by algorithmic monitoring systems) arerequired to provide employees with a written description of each quota that applies to the employee, including“quantified number of tasks to be performed or materials to be produced or handled, within the definedtime period, and any potential adverse employment action that could result from failure to meet the quota.”93Across the federal government, agencies are conducting and supporting research on explain-able AI systems. The NIST is conducting fundamental research on the explainability of AI systems. A multidis-ciplinary team of researchers aims to develop measurement methods and best practices to support theimplementation of core tenets of explainable AI.94 The Defense Advanced Research Projects Agency has aprogram on Explainable Artificial Intelligence that aims to create a suite of machine learning techniques thatproduce more explainable models, while maintaining a high level of learning performance (predictionaccuracy), and enable human users to understand, appropriately trust, and effectively manage the emerginggeneration of artificially intelligent partners.95 The National Science Foundation’s program on Fairness in Artificial Intelligence also includes a specific interest in research foundations for explainable AI.96H A , C , FUMAN LTERNATIVES ONSIDERATION AND ALLBACKYou should be able to opt out, where appropriate, andhave access to a person who can quickly consider andremedy problems you encounter. You should be able to optout from automated systems in favor of a human alternative, whereappropriate. Appropriateness should be determined based on rea-sonable expectations in a given context and with a focus on ensuringbroad accessibility and protecting the public from especially harm-ful impacts. In some cases, a human or other alternative may be re-quired by law. You should have access to timely human consider-ation and remedy by a fallback and escalation process if an automat-ed system fails, it produces an error, or you would like to appeal orcontest its impacts on you. Human consideration and fallbackshould be accessible, equitable, effective, maintained, accompaniedby appropriate operator training, and should not impose an unrea-sonable burden on the public. Automated systems with an intendeduse within sensitive domains, including, but not limited to, criminaljustice, employment, education, and health, should additionally betailored to the purpose, provide meaningful access for oversight, include training for any people interacting with the system, and in-corporate human consideration for adverse or high-risk decisions. Reporting that includes a description of these human governanceprocesses and assessment of their timeliness, accessibility, out-comes, and effectiveness should be made public whenever possible. WHY THIS PRINCIPLE IS IMPORTANTThis section provides a brief summary of the problems which the principle seeks to address and protectagainst, including illustrative examples. There are many reasons people may prefer not to use an automated system: the system can be flawed and can lead tounintended outcomes; it may reinforce bias or be inaccessible; it may simply be inconvenient or unavailable; or it mayreplace a paper or manual process to which people had grown accustomed. Yet members of the public are oftenpresented with no alternative, or are forced to endure a cumbersome process to reach a human decision-maker oncethey decide they no longer want to deal exclusively with the automated system or be impacted by its results. As a resultof this lack of human reconsideration, many receive delayed access, or lose access, to rights, opportunities, benefits, and critical services. The American public deserves the assurance that, when rights, opportunities, or access aremeaningfully at stake and there is a reasonable expectation of an alternative to an automated system, they can conve-niently opt out of an automated system and will not be disadvantaged for that choice. In some cases, such a human orother alternative may be required by law, for example it could be required as “reasonable accommodations” for peoplewith disabilities. In addition to being able to opt out and use a human alternative, the American public deserves a human fallbacksystem in the event that an automated system fails or causes harm. No matter how rigorously an automated system istested, there will always be situations for which the system fails. The American public deserves protection via humanreview against these outlying or unexpected scenarios. In the case of time-critical systems, the public should not haveto wait—immediate human consideration and fallback should be available. In many time-critical systems, such aremedy is already immediately available, such as a building manager who can open a door in the case an automatedcard access system fails. In the criminal justice system, employment, education, healthcare, and other sensitive domains, automated systemsare used for many purposes, from pre-trial risk assessments and parole decisions to technologies that help doctorsdiagnose disease. Absent appropriate safeguards, these technologies can lead to unfair, inaccurate, or dangerousoutcomes. These sensitive domains require extra protections. It is critically important that there is extensive humanoversight in such settings. These critical protections have been adopted in some scenarios. Where automated systems have been introduced toprovide the public access to government benefits, existing human paper and phone-based processes are generally stillin place, providing an important alternative to ensure access. Companies that have introduced automated call centersoften retain the option of dialing zero to reach an operator. When automated identity controls are in place to board anairplane or enter the country, there is a person supervising the systems who can be turned to for help or to appeal amisidentification. The American people deserve the reassurance that such procedures are in place to protect their rights, opportunities, and access. People make mistakes, and a human alternative or fallback mechanism will not always have the rightanswer, but they serve as an important check on the power and validity of automated systems.• An automated signature matching system is used as part of the voting process in many parts of the country todetermine whether the signature on a mail-in ballot matches the signature on file. These signature matchingsystems are less likely to work correctly for some voters, including voters with mental or physicaldisabilities, voters with shorter or hyphenated names, and voters who have changed their name.97 A humancuring process,98 which helps voters to confirm their signatures and correct other voting mistakes, isimportant to ensure all votes are counted,99 and it is already standard practice in much of the country forboth an election official and the voter to have the opportunity to review and correct any such issues.100WHY THIS PRINCIPLE IS IMPORTANTThis section provides a brief summary of the problems which the principle seeks to address and protectagainst, including illustrative examples.• An unemployment benefits system in Colorado required, as a condition of accessing benefits, that applicantshave a smartphone in order to verify their identity. No alternative human option was readily available, which denied many people access to benefits.101• A fraud detection system for unemployment insurance distribution incorrectly flagged entries as fraudulent, leading to people with slight discrepancies or complexities in their files having their wages withheld and taxreturns seized without any chance to explain themselves or receive a review by a person.102• A patient was wrongly denied access to pain medication when the hospital’s software confused her medica-tion history with that of her dog’s. Even after she tracked down an explanation for the problem, doctorswere afraid to override the system, and she was forced to go without pain relief due to the system’s error.103• A large corporation automated performance evaluation and other HR functions, leading to workers beingfired by an automated system without the possibility of human review, appeal or other form of recourse.104WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMSThe expectations for automated systems are meant to serve as a blueprint for the development of additionaltechnical standards and practices that are tailored for particular sectors and contexts. An automated system should provide demonstrably effective mechanisms to opt out in favor of a human alterna-tive, where appropriate, as well as timely human consideration and remedy by a fallback system, with additionalhuman oversight and safeguards for systems used in sensitive domains, and with training and assessment for anyhuman-based portions of the system to ensure effectiveness. Provide a mechanism to conveniently opt out from automated systems in favor of a humanalternative, where appropriate Brief, clear, accessible notice and instructions. Those impacted by an automated system should begiven a brief, clear notice that they are entitled to opt-out, along with clear instructions for how to opt-out. Instructions should be provided in an accessible form and should be easily findable by those impacted by theautomated system. The brevity, clarity, and accessibility of the notice and instructions should be assessed (e. g., via user experience research). Human alternatives provided when appropriate. In many scenarios, there is a reasonable expectationof human involvement in attaining rights, opportunities, or access. When automated systems make up part ofthe attainment process, alternative timely human-driven processes should be provided. The use of a humanalternative should be triggered by an opt-out process. Timely and not burdensome human alternative. Opting out should be timely and not unreasonablyburdensome in both the process of requesting to opt-out and the human-driven alternative provided. Provide timely human consideration and remedy by a fallback and escalation system in theevent that an automated system fails, produces error, or you would like to appeal or con-test its impacts on you Proportionate. The availability of human consideration and fallback, along with associated training andsafeguards against human bias, should be proportionate to the potential of the automated system to meaning-fully impact rights, opportunities, or access. Automated systems that have greater control over outcomes, provide input to high-stakes decisions, relate to sensitive domains, or otherwise have a greater potential tomeaningfully impact rights, opportunities, or access should have greater availability (e. g., staffing) and over-sight of human consideration and fallback mechanisms. Accessible. Mechanisms for human consideration and fallback, whether in-person, on paper, by phone, orotherwise provided, should be easy to find and use. These mechanisms should be tested to ensure that userswho have trouble with the automated system are able to use human consideration and fallback, with the under-standing that it may be these users who are most likely to need the human assistance. Similarly, it should betested to ensure that users with disabilities are able to find and use human consideration and fallback and alsorequest reasonable accommodations or modifications. Convenient. Mechanisms for human consideration and fallback should not be unreasonably burdensome ascompared to the automated system’s equivalent. WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMSThe expectations for automated systems are meant to serve as a blueprint for the development of additionaltechnical standards and practices that are tailored for particular sectors and contexts. Equitable. Consideration should be given to ensuring outcomes of the fallback and escalation system areequitable when compared to those of the automated system and such that the fallback and escalationsystem provides equitable access to underserved communities.105Timely. Human consideration and fallback are only useful if they are conducted and concluded in atimely manner. The determination of what is timely should be made relative to the specific automatedsystem, and the review system should be staffed and regularly assessed to ensure it is providing timelyconsideration and fallback. In time-critical systems, this mechanism should be immediately available or, where possible, available before the harm occurs. Time-critical systems include, but are not limited to, voting-related systems, automated building access and other access systems, systems that form a criticalcomponent of healthcare, and systems that have the ability to withhold wages or otherwise causeimmediate financial penalties. Effective. The organizational structure surrounding processes for consideration and fallback shouldbe designed so that if the human decision-maker charged with reassessing a decision determines that itshould be overruled, the new decision will be effectively enacted. This includes ensuring that the newdecision is entered into the automated system throughout its components, any previous repercussions fromthe old decision are also overturned, and safeguards are put in place to help ensure that future decisions donot result in the same errors. Maintained. The human consideration and fallback process and any associated automated processesshould be maintained and supported as long as the relevant automated system continues to be in use. Institute training, assessment, and oversight to combat automation bias and ensure anyhuman-based components of a system are effective. Training and assessment. Anyone administering, interacting with, or interpreting the outputs of an auto-mated system should receive training in that system, including how to properly interpret outputs of a systemin light of its intended purpose and in how to mitigate the effects of automation bias. The training should reoc-cur regularly to ensure it is up to date with the system and to ensure the system is used appropriately. Assess-ment should be ongoing to ensure that the use of the system with human involvement provides for appropri-ate results, i. e., that the involvement of people does not invalidate the system's assessment as safe and effectiveor lead to algorithmic discrimination. Oversight. Human-based systems have the potential for bias, including automation bias, as well as otherconcerns that may limit their effectiveness. The results of assessments of the efficacy and potential bias ofsuch human-based systems should be overseen by governance structures that have the potential to update theoperation of the human-based system in order to mitigate these effects. WHAT SHOULD BE EXPECTED OF AUTOMATED SYSTEMSThe expectations for automated systems are meant to serve as a blueprint for the development of additionaltechnical standards and practices that are tailored for particular sectors and contexts. Implement additional human oversight and safeguards for automated systems related tosensitive domains Automated systems used within sensitive domains, including criminal justice, employment, education, andhealth, should meet the expectations laid out throughout this framework, especially avoiding capricious, inappropriate, and discriminatory impacts of these technologies. Additionally, automated systems used withinsensitive domains should meet these expectations: Narrowly scoped data and inferences. Human oversight should ensure that automated systems insensitive domains are narrowly scoped to address a defined goal, justifying each included data item or attri-bute as relevant to the specific use case. Data included should be carefully limited to avoid algorithmicdiscrimination resulting from, e. g., use of community characteristics, social network analysis, or group-basedinferences. Tailored to the situation. Human oversight should ensure that automated systems in sensitive domainsare tailored to the specific use case and real-world deployment scenario, and evaluation testing should showthat the system is safe and effective for that specific situation. Validation testing performed based on one loca-tion or use case should not be assumed to transfer to another. Human consideration before any high-risk decision. Automated systems, where they are used insensitive domains, may play a role in directly providing information or otherwise providing positive outcomesto impacted people. However, automated systems should not be allowed to directly intervene in high-risksituations, such as sentencing decisions or medical care, without human consideration. Meaningful access to examine the system. Designers, developers, and deployers of automatedsystems should consider limited waivers of confidentiality (including those related to trade secrets) wherenecessary in order to provide meaningful oversight of systems used in sensitive domains, incorporating mea-sures to protect intellectual property and trade secrets from unwarranted disclosure as appropriate. Thisincludes (potentially private and protected) meaningful access to source code, documentation, and relateddata during any associated legal discovery, subject to effective confidentiality or court orders. Such meaning-ful access should include (but is not limited to) adhering to the principle on Notice and Explanation using thehighest level of risk so the system is designed with built-in explanations; such systems should use fully-trans-parent models where the model itself can be understood by people needing to directly examine it. Demonstrate access to human alternatives, consideration, and fallback Reporting. Reporting should include an assessment of timeliness and the extent of additional burden forhuman alternatives, aggregate statistics about who chooses the human alternative, along with the results ofthe assessment about brevity, clarity, and accessibility of notice and opt-out instructions. Reporting on theaccessibility, timeliness, and effectiveness of human consideration and fallback should be made public at regu-lar intervals for as long as the system is in use. This should include aggregated information about the numberand type of requests for consideration, fallback employed, and any repeated requests; the timeliness of thehandling of these requests, including mean wait times for different types of requests as well as maximum waittimes; and information about the procedures used to address requests for consideration along with the resultsof the evaluation of their accessibility. For systems used in sensitive domains, reporting should include infor-mation about training and governance procedures for these technologies. Reporting should also include docu-mentation of goals and assessment of meeting those goals, consideration of data included, and documentation HOW THESE PRINCIPLES CAN MOVE INTO PRACTICEReal-life examples of how these principles can become reality, through laws, policies, and practicaltechnical and sociotechnical approaches to protecting rights, opportunities, and access. Healthcare “navigators” help people find their way through online signup forms to chooseand obtain healthcare. A Navigator is “an individual or organization that's trained and able to helpconsumers, small businesses, and their employees as they look for health coverage options through the Marketplace (a government web site), including completing eligibility and enrollment forms.”106 Forthe 2022 plan year, the Biden-Harris Administration increased funding so that grantee organizations could“train and certify more than 1,500 Navigators to help uninsured consumers find affordable and comprehensivehealth coverage.”107The customer service industry has successfully integrated automated services such aschat-bots and AI-driven call response systems with escalation to a human supportteam.108 Many businesses now use partially automated customer service platforms that help answer customerquestions and compile common problems for human agents to review. These integrated human-AIsystems allow companies to provide faster customer care while maintaining human agents to answercalls or otherwise respond to complicated requests. Using both AI and human agents is viewed as key tosuccessful customer service.109Ballot curing laws in at least 24 states require a fallback system that allows voters tocorrect their ballot and have it counted in the case that a voter signature matchingalgorithm incorrectly flags their ballot as invalid or there is another issue with theirballot, and review by an election official does not rectify the problem. Some federalcourts have found that such cure procedures are constitutionally required.110 Ballotcuring processes vary among states, and include direct phone calls, emails, or mail contact by electionofficials.111 Voters are asked to provide alternative information or a new signature to verify the validity of theirballot.