

--- DOCUMENT: EU_AI_Act.pdf ---

2024/1689 12.7.2024REGULATION (EU) 2024/1689 OF THE EUROPEAN PARLIAMENT AND OF THE COUNCILof 13 June 2024laying down harmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008,(EU) No 167/2013, (EU) No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU) 2020/1828 (Artificial Intelligence Act)(Text with EEA relevance)THE EUROPEAN PARLIAMENT AND THE COUNCIL OF THE EUROPEAN UNION, Having regard to the Treaty on the Functioning of the European Union, and in particular Articles 16 and 114 thereof, Having regard to the proposal from the European Commission, After transmission of the draft legislative act to the national parliaments, Having regard to the opinion of the European Economic and Social Committee (1), Having regard to the opinion of the European Central Bank (2), Having regard to the opinion of the Committee of the Regions (3), Acting in accordance with the ordinary legislative procedure (4), Whereas:(1) The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legalframework in particular for the development, the placing on the market, the putting into service and the use ofartificial intelligence systems (AI systems) in the Union, in accordance with Union values, to promote the uptake ofhuman centric and trustworthy artificial intelligence (AI) while ensuring a high level of protection of health, safety, fundamental rights as enshrined in the Charter of Fundamental Rights of the European Union (the ‘Charter’), including democracy, the rule of law and environmental protection, to protect against the harmful effects of AIsystems in the Union, and to support innovation. This Regulation ensures the free movement, cross-border, of AI-based goods and services, thus preventing Member States from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by this Regulation.(2) This Regulation should be applied in accordance with the values of the Union enshrined as in the Charter, facilitatingthe protection of natural persons, undertakings, democracy, the rule of law and environmental protection, whileboosting innovation and employment and making the Union a leader in the uptake of trustworthy AI.(3) AI systems can be easily deployed in a large variety of sectors of the economy and many parts of society, includingacross borders, and can easily circulate throughout the Union. Certain Member States have already explored theadoption of national rules to ensure that AI is trustworthy and safe and is developed and used in accordance withfundamental rights obligations. Diverging national rules may lead to the fragmentation of the internal market andmay decrease legal certainty for operators that develop, import or use AI systems. A consistent and high level ofprotection throughout the Union should therefore be ensured in order to achieve trustworthy AI, while divergenceshampering the free circulation, innovation, deployment and the uptake of AI systems and related products andservices within the internal market should be prevented by laying down uniform obligations for operators and(1) OJ C 517, 22.12.2021, p. 56.(2) OJ C 115, 11.3.2022, p. 5.(3) OJ C 97, 28.2.2022, p. 60. guaranteeing the uniform protection of overriding reasons of public interest and of rights of persons throughout theinternal market on the basis of Article 114 of the Treaty on the Functioning of the European Union (TFEU). To theextent that this Regulation contains specific rules on the protection of individuals with regard to the processing ofpersonal data concerning restrictions of the use of AI systems for remote biometric identification for the purpose oflaw enforcement, of the use of AI systems for risk assessments of natural persons for the purpose of lawenforcement and of the use of AI systems of biometric categorisation for the purpose of law enforcement, it isappropriate to base this Regulation, in so far as those specific rules are concerned, on Article 16 TFEU. In light ofthose specific rules and the recourse to Article 16 TFEU, it is appropriate to consult the European Data Protection Board.(4) AI is a fast evolving family of technologies that contributes to a wide array of economic, environmental and societalbenefits across the entire spectrum of industries and social activities. By improving prediction, optimising operationsand resource allocation, and personalising digital solutions available for individuals and organisations, the use of AIcan provide key competitive advantages to undertakings and support socially and environmentally beneficialoutcomes, for example in healthcare, agriculture, food safety, education and training, media, sports, culture, infrastructure management, energy, transport and logistics, public services, security, justice, resource and energyefficiency, environmental monitoring, the conservation and restoration of biodiversity and ecosystems and climatechange mitigation and adaptation.(5) At the same time, depending on the circumstances regarding its specific application, use, and level of technologicaldevelopment, AI may generate risks and cause harm to public interests and fundamental rights that are protected by Union law. Such harm might be material or immaterial, including physical, psychological, societal or economicharm.(6) Given the major impact that AI can have on society and the need to build trust, it is vital for AI and its regulatoryframework to be developed in accordance with Union values as enshrined in Article 2 of the Treaty on European Union (TEU), the fundamental rights and freedoms enshrined in the Treaties and, pursuant to Article 6 TEU, the Charter. As a prerequisite, AI should be a human-centric technology. It should serve as a tool for people, with theultimate aim of increasing human well-being.(7) In order to ensure a consistent and high level of protection of public interests as regards health, safety andfundamental rights, common rules for high-risk AI systems should be established. Those rules should be consistentwith the Charter, non-discriminatory and in line with the Union’s international trade commitments. They shouldalso take into account the European Declaration on Digital Rights and Principles for the Digital Decade and the Ethics guidelines for trustworthy AI of the High-Level Expert Group on Artificial Intelligence (AI HLEG).(8) A Union legal framework laying down harmonised rules on AI is therefore needed to foster the development, useand uptake of AI in the internal market that at the same time meets a high level of protection of public interests, suchas health and safety and the protection of fundamental rights, including democracy, the rule of law andenvironmental protection as recognised and protected by Union law. To achieve that objective, rules regulating theplacing on the market, the putting into service and the use of certain AI systems should be laid down, thus ensuringthe smooth functioning of the internal market and allowing those systems to benefit from the principle of freemovement of goods and services. Those rules should be clear and robust in protecting fundamental rights, supportive of new innovative solutions, enabling a European ecosystem of public and private actors creating AIsystems in line with Union values and unlocking the potential of the digital transformation across all regions of the Union. By laying down those rules as well as measures in support of innovation with a particular focus on small andmedium enterprises (SMEs), including startups, this Regulation supports the objective of promoting the Europeanhuman-centric approach to AI and being a global leader in the development of secure, trustworthy and ethical AI asstated by the European Council (5), and it ensures the protection of ethical principles, as specifically requested by the European Parliament (6).(5) European Council, Special meeting of the European Council (1 and 2 October 2020) — Conclusions, EUCO 13/20, 2020, p. 6.(9) Harmonised rules applicable to the placing on the market, the putting into service and the use of high-risk AIsystems should be laid down consistently with Regulation (EC) No 765/2008 of the European Parliament and of the Council (7), Decision No 768/2008/EC of the European Parliament and of the Council (8) and Regulation (EU)2019/1020 of the European Parliament and of the Council (9) (New Legislative Framework). The harmonised ruleslaid down in this Regulation should apply across sectors and, in line with the New Legislative Framework, should bewithout prejudice to existing Union law, in particular on data protection, consumer protection, fundamental rights, employment, and protection of workers, and product safety, to which this Regulation is complementary. Asa consequence, all rights and remedies provided for by such Union law to consumers, and other persons on whom AI systems may have a negative impact, including as regards the compensation of possible damages pursuant to Council Directive 85/374/EEC (10) remain unaffected and fully applicable. Furthermore, in the context ofemployment and protection of workers, this Regulation should therefore not affect Union law on social policy andnational labour law, in compliance with Union law, concerning employment and working conditions, includinghealth and safety at work and the relationship between employers and workers. This Regulation should also notaffect the exercise of fundamental rights as recognised in the Member States and at Union level, including the right orfreedom to strike or to take other action covered by the specific industrial relations systems in Member States as wellas the right to negotiate, to conclude and enforce collective agreements or to take collective action in accordancewith national law. This Regulation should not affect the provisions aiming to improve working conditions inplatform work laid down in a Directive of the European Parliament and of the Council on improving workingconditions in platform work. Moreover, this Regulation aims to strengthen the effectiveness of such existing rightsand remedies by establishing specific requirements and obligations, including in respect of the transparency, technical documentation and record-keeping of AI systems. Furthermore, the obligations placed on variousoperators involved in the AI value chain under this Regulation should apply without prejudice to national law, incompliance with Union law, having the effect of limiting the use of certain AI systems where such law falls outsidethe scope of this Regulation or pursues legitimate public interest objectives other than those pursued by this Regulation. For example, national labour law and law on the protection of minors, namely persons below the age of18, taking into account the UNCRC General Comment No 25 (2021) on children’s rights in relation to the digitalenvironment, insofar as they are not specific to AI systems and pursue other legitimate public interest objectives, should not be affected by this Regulation.(10) The fundamental right to the protection of personal data is safeguarded in particular by Regulations (EU)2016/679 (11) and (EU) 2018/1725 (12) of the European Parliament and of the Council and Directive (EU) 2016/680of the European Parliament and of the Council (13). Directive 2002/58/EC of the European Parliament and of the Council (14) additionally protects private life and the confidentiality of communications, including by way ofproviding conditions for any storing of personal and non-personal data in, and access from, terminal equipment. Those Union legal acts provide the basis for sustainable and responsible data processing, including where data setsinclude a mix of personal and non-personal data. This Regulation does not seek to affect the application of existing Union law governing the processing of personal data, including the tasks and powers of the independent supervisoryauthorities competent to monitor compliance with those instruments. It also does not affect the obligations ofproviders and deployers of AI systems in their role as data controllers or processors stemming from Union ornational law on the protection of personal data in so far as the design, the development or the use of AI systemsinvolves the processing of personal data. It is also appropriate to clarify that data subjects continue to enjoy all the(7) Regulation (EC) No 765/2008 of the European Parliament and of the Council of 9 July 2008 setting out the requirements foraccreditation and repealing Regulation (EEC) No 339/93 (OJ L 218, 13.8.2008, p. 30).(8) Decision No 768/2008/EC of the European Parliament and of the Council of 9 July 2008 on a common framework for themarketing of products, and repealing Council Decision 93/465/EEC (OJ L 218, 13.8.2008, p. 82).(9) Regulation (EU) 2019/1020 of the European Parliament and of the Council of 20 June 2019 on market surveillance and complianceof products and amending Directive 2004/42/EC and Regulations (EC) No 765/2008 and (EU) No 305/2011 (OJ L 169, 25.6.2019, p. 1).(10) Council Directive 85/374/EEC of 25 July 1985 on the approximation of the laws, regulations and administrative provisions of the Member States concerning liability for defective products (OJ L 210, 7.8.1985, p. 29).(11) Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural personswith regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016, p. 1).(12) Regulation (EU) 2018/1725 of the European Parliament and of the Council of 23 October 2018 on the protection of naturalpersons with regard to the processing of personal data by the Union institutions, bodies, offices and agencies and on the freemovement of such data, and repealing Regulation (EC) No 45/2001 and Decision No 1247/2002/EC (OJ L 295, 21.11.2018, p. 39).(13) Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons withregard to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection orprosecution of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework Decision 2008/977/JHA (OJ L 119, 4.5.2016, p. 89).(14) Directive 2002/58/EC of the European Parliament and of the Council of 12 July 2002 concerning the processing of personal datarights and guarantees awarded to them by such Union law, including the rights related to solely automated individualdecision-making, including profiling. Harmonised rules for the placing on the market, the putting into service andthe use of AI systems established under this Regulation should facilitate the effective implementation and enable theexercise of the data subjects’ rights and other remedies guaranteed under Union law on the protection of personaldata and of other fundamental rights.(11) This Regulation should be without prejudice to the provisions regarding the liability of providers of intermediaryservices as set out in Regulation (EU) 2022/2065 of the European Parliament and of the Council (15).(12) The notion of ‘AI system’ in this Regulation should be clearly defined and should be closely aligned with the work ofinternational organisations working on AI to ensure legal certainty, facilitate international convergence and wideacceptance, while providing the flexibility to accommodate the rapid technological developments in this field. Moreover, the definition should be based on key characteristics of AI systems that distinguish it from simplertraditional software systems or programming approaches and should not cover systems that are based on the rulesdefined solely by natural persons to automatically execute operations. A key characteristic of AI systems is theircapability to infer. This capability to infer refers to the process of obtaining the outputs, such as predictions, content, recommendations, or decisions, which can influence physical and virtual environments, and to a capability of AIsystems to derive models or algorithms, or both, from inputs or data. The techniques that enable inference whilebuilding an AI system include machine learning approaches that learn from data how to achieve certain objectives, and logic- and knowledge-based approaches that infer from encoded knowledge or symbolic representation of thetask to be solved. The capacity of an AI system to infer transcends basic data processing by enabling learning, reasoning or modelling. The term ‘machine-based’ refers to the fact that AI systems run on machines. The referenceto explicit or implicit objectives underscores that AI systems can operate according to explicit defined objectives orto implicit objectives. The objectives of the AI system may be different from the intended purpose of the AI systemin a specific context. For the purposes of this Regulation, environments should be understood to be the contexts inwhich the AI systems operate, whereas outputs generated by the AI system reflect different functions performed by AI systems and include predictions, content, recommendations or decisions. AI systems are designed to operate withvarying levels of autonomy, meaning that they have some degree of independence of actions from humaninvolvement and of capabilities to operate without human intervention. The adaptiveness that an AI system couldexhibit after deployment, refers to self-learning capabilities, allowing the system to change while in use. AI systemscan be used on a stand-alone basis or as a component of a product, irrespective of whether the system is physicallyintegrated into the product (embedded) or serves the functionality of the product without being integrated therein(non-embedded).(13) The notion of ‘deployer’ referred to in this Regulation should be interpreted as any natural or legal person, includinga public authority, agency or other body, using an AI system under its authority, except where the AI system is usedin the course of a personal non-professional activity. Depending on the type of AI system, the use of the system mayaffect persons other than the deployer.(14) The notion of ‘biometric data’ used in this Regulation should be interpreted in light of the notion of biometric dataas defined in Article 4, point (14) of Regulation (EU) 2016/679, Article 3, point (18) of Regulation (EU) 2018/1725and Article 3, point (13) of Directive (EU) 2016/680. Biometric data can allow for the authentication, identificationor categorisation of natural persons and for the recognition of emotions of natural persons.(15) The notion of ‘biometric identification’ referred to in this Regulation should be defined as the automated recognitionof physical, physiological and behavioural human features such as the face, eye movement, body shape, voice, prosody, gait, posture, heart rate, blood pressure, odour, keystrokes characteristics, for the purpose of establishing anindividual’s identity by comparing biometric data of that individual to stored biometric data of individuals ina reference database, irrespective of whether the individual has given its consent or not. This excludes AI systemsintended to be used for biometric verification, which includes authentication, whose sole purpose is to confirm thata specific natural person is the person he or she claims to be and to confirm the identity of a natural person for thesole purpose of having access to a service, unlocking a device or having security access to premises.(16) The notion of ‘biometric categorisation’ referred to in this Regulation should be defined as assigning natural personsto specific categories on the basis of their biometric data. Such specific categories can relate to aspects such as sex, age, hair colour, eye colour, tattoos, behavioural or personality traits, language, religion, membership of a nationalminority, sexual or political orientation. This does not include biometric categorisation systems that are a purelyancillary feature intrinsically linked to another commercial service, meaning that the feature cannot, for objectivetechnical reasons, be used without the principal service, and the integration of that feature or functionality is nota means to circumvent the applicability of the rules of this Regulation. For example, filters categorising facial or bodyfeatures used on online marketplaces could constitute such an ancillary feature as they can be used only in relation tothe principal service which consists in selling a product by allowing the consumer to preview the display of theproduct on him or herself and help the consumer to make a purchase decision. Filters used on online social networkservices which categorise facial or body features to allow users to add or modify pictures or videos could also beconsidered to be ancillary feature as such filter cannot be used without the principal service of the social networkservices consisting in the sharing of content online.(17) The notion of ‘remote biometric identification system’ referred to in this Regulation should be defined functionally, as an AI system intended for the identification of natural persons without their active involvement, typically ata distance, through the comparison of a person’s biometric data with the biometric data contained in a referencedatabase, irrespectively of the particular technology, processes or types of biometric data used. Such remotebiometric identification systems are typically used to perceive multiple persons or their behaviour simultaneously inorder to facilitate significantly the identification of natural persons without their active involvement. This excludes AI systems intended to be used for biometric verification, which includes authentication, the sole purpose of whichis to confirm that a specific natural person is the person he or she claims to be and to confirm the identity ofa natural person for the sole purpose of having access to a service, unlocking a device or having security access topremises. That exclusion is justified by the fact that such systems are likely to have a minor impact on fundamentalrights of natural persons compared to the remote biometric identification systems which may be used for theprocessing of the biometric data of a large number of persons without their active involvement. In the case of‘real-time’ systems, the capturing of the biometric data, the comparison and the identification occur allinstantaneously, near-instantaneously or in any event without a significant delay. In this regard, there should be noscope for circumventing the rules of this Regulation on the ‘real-time’ use of the AI systems concerned by providingfor minor delays. ‘Real-time’ systems involve the use of ‘live’ or ‘near-live’ material, such as video footage, generatedby a camera or other device with similar functionality. In the case of ‘post’ systems, in contrast, the biometric datahas already been captured and the comparison and identification occur only after a significant delay. This involvesmaterial, such as pictures or video footage generated by closed circuit television cameras or private devices, whichhas been generated before the use of the system in respect of the natural persons concerned.(18) The notion of ‘emotion recognition system’ referred to in this Regulation should be defined as an AI system for thepurpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data. The notion refers to emotions or intentions such as happiness, sadness, anger, surprise, disgust, embarrassment, excitement, shame, contempt, satisfaction and amusement. It does not include physical states, such as pain orfatigue, including, for example, systems used in detecting the state of fatigue of professional pilots or drivers for thepurpose of preventing accidents. This does also not include the mere detection of readily apparent expressions, gestures or movements, unless they are used for identifying or inferring emotions. Those expressions can be basicfacial expressions, such as a frown or a smile, or gestures such as the movement of hands, arms or head, orcharacteristics of a person’s voice, such as a raised voice or whispering.(19) For the purposes of this Regulation the notion of ‘publicly accessible space’ should be understood as referring to anyphysical space that is accessible to an undetermined number of natural persons, and irrespective of whether thespace in question is privately or publicly owned, irrespective of the activity for which the space may be used, such asfor commerce, for example, shops, restaurants, cafés; for services, for example, banks, professional activities, hospitality; for sport, for example, swimming pools, gyms, stadiums; for transport, for example, bus, metro andrailway stations, airports, means of transport; for entertainment, for example, cinemas, theatres, museums, concertand conference halls; or for leisure or otherwise, for example, public roads and squares, parks, forests, playgrounds. A space should also be classified as being publicly accessible if, regardless of potential capacity or securityrestrictions, access is subject to certain predetermined conditions which can be fulfilled by an undetermined numberof persons, such as the purchase of a ticket or title of transport, prior registration or having a certain age. In contrast, of will by the person having the relevant authority over the space. The factual possibility of access alone, such as anunlocked door or an open gate in a fence, does not imply that the space is publicly accessible in the presence ofindications or circumstances suggesting the contrary, such as. signs prohibiting or restricting access. Company andfactory premises, as well as offices and workplaces that are intended to be accessed only by relevant employees andservice providers, are spaces that are not publicly accessible. Publicly accessible spaces should not include prisons orborder control. Some other spaces may comprise both publicly accessible and non-publicly accessible spaces, such asthe hallway of a private residential building necessary to access a doctor’s office or an airport. Online spaces are notcovered, as they are not physical spaces. Whether a given space is accessible to the public should however bedetermined on a case-by-case basis, having regard to the specificities of the individual situation at hand.(20) In order to obtain the greatest benefits from AI systems while protecting fundamental rights, health and safety and toenable democratic control, AI literacy should equip providers, deployers and affected persons with the necessarynotions to make informed decisions regarding AI systems. Those notions may vary with regard to the relevantcontext and can include understanding the correct application of technical elements during the AI system’sdevelopment phase, the measures to be applied during its use, the suitable ways in which to interpret the AI system’soutput, and, in the case of affected persons, the knowledge necessary to understand how decisions taken with theassistance of AI will have an impact on them. In the context of the application this Regulation, AI literacy shouldprovide all relevant actors in the AI value chain with the insights required to ensure the appropriate compliance andits correct enforcement. Furthermore, the wide implementation of AI literacy measures and the introduction ofappropriate follow-up actions could contribute to improving working conditions and ultimately sustain theconsolidation, and innovation path of trustworthy AI in the Union. The European Artificial Intelligence Board (the‘Board’) should support the Commission, to promote AI literacy tools, public awareness and understanding of thebenefits, risks, safeguards, rights and obligations in relation to the use of AI systems. In cooperation with the relevantstakeholders, the Commission and the Member States should facilitate the drawing up of voluntary codes of conductto advance AI literacy among persons dealing with the development, operation and use of AI.(21) In order to ensure a level playing field and an effective protection of rights and freedoms of individuals across the Union, the rules established by this Regulation should apply to providers of AI systems in a non-discriminatorymanner, irrespective of whether they are established within the Union or in a third country, and to deployers of AIsystems established within the Union.(22) In light of their digital nature, certain AI systems should fall within the scope of this Regulation even when they arenot placed on the market, put into service, or used in the Union. This is the case, for example, where an operatorestablished in the Union contracts certain services to an operator established in a third country in relation to anactivity to be performed by an AI system that would qualify as high-risk. In those circumstances, the AI system usedin a third country by the operator could process data lawfully collected in and transferred from the Union, andprovide to the contracting operator in the Union the output of that AI system resulting from that processing, without that AI system being placed on the market, put into service or used in the Union. To prevent thecircumvention of this Regulation and to ensure an effective protection of natural persons located in the Union, this Regulation should also apply to providers and deployers of AI systems that are established in a third country, to theextent the output produced by those systems is intended to be used in the Union. Nonetheless, to take into accountexisting arrangements and special needs for future cooperation with foreign partners with whom information andevidence is exchanged, this Regulation should not apply to public authorities of a third country and internationalorganisations when acting in the framework of cooperation or international agreements concluded at Union ornational level for law enforcement and judicial cooperation with the Union or the Member States, provided that therelevant third country or international organisation provides adequate safeguards with respect to the protection offundamental rights and freedoms of individuals. Where relevant, this may cover activities of entities entrusted by thethird countries to carry out specific tasks in support of such law enforcement and judicial cooperation. Suchframework for cooperation or agreements have been established bilaterally between Member States and thirdcountries or between the European Union, Europol and other Union agencies and third countries and internationalorganisations. The authorities competent for supervision of the law enforcement and judicial authorities under thisauthorities and Union institutions, bodies, offices and agencies making use of such outputs in the Union remainaccountable to ensure their use complies with Union law. When those international agreements are revised or newones are concluded in the future, the contracting parties should make utmost efforts to align those agreements withthe requirements of this Regulation.(23) This Regulation should also apply to Union institutions, bodies, offices and agencies when acting as a provider ordeployer of an AI system.(24) If, and insofar as, AI systems are placed on the market, put into service, or used with or without modification of suchsystems for military, defence or national security purposes, those should be excluded from the scope of this Regulation regardless of which type of entity is carrying out those activities, such as whether it is a public or privateentity. As regards military and defence purposes, such exclusion is justified both by Article 4(2) TEU and by thespecificities of the Member States’ and the common Union defence policy covered by Chapter 2 of Title V TEU thatare subject to public international law, which is therefore the more appropriate legal framework for the regulation of AI systems in the context of the use of lethal force and other AI systems in the context of military and defenceactivities. As regards national security purposes, the exclusion is justified both by the fact that national securityremains the sole responsibility of Member States in accordance with Article 4(2) TEU and by the specific nature andoperational needs of national security activities and specific national rules applicable to those activities. Nonetheless, if an AI system developed, placed on the market, put into service or used for military, defence or national securitypurposes is used outside those temporarily or permanently for other purposes, for example, civilian or humanitarianpurposes, law enforcement or public security purposes, such a system would fall within the scope of this Regulation. In that case, the entity using the AI system for other than military, defence or national security purposes shouldensure the compliance of the AI system with this Regulation, unless the system is already compliant with this Regulation. AI systems placed on the market or put into service for an excluded purpose, namely military, defence ornational security, and one or more non-excluded purposes, such as civilian purposes or law enforcement, fall withinthe scope of this Regulation and providers of those systems should ensure compliance with this Regulation. In thosecases, the fact that an AI system may fall within the scope of this Regulation should not affect the possibility ofentities carrying out national security, defence and military activities, regardless of the type of entity carrying outthose activities, to use AI systems for national security, military and defence purposes, the use of which is excludedfrom the scope of this Regulation. An AI system placed on the market for civilian or law enforcement purposeswhich is used with or without modification for military, defence or national security purposes should not fall withinthe scope of this Regulation, regardless of the type of entity carrying out those activities.(25) This Regulation should support innovation, should respect freedom of science, and should not undermine researchand development activity. It is therefore necessary to exclude from its scope AI systems and models specificallydeveloped and put into service for the sole purpose of scientific research and development. Moreover, it is necessaryto ensure that this Regulation does not otherwise affect scientific research and development activity on AI systems ormodels prior to being placed on the market or put into service. As regards product-oriented research, testing anddevelopment activity regarding AI systems or models, the provisions of this Regulation should also not apply priorto those systems and models being put into service or placed on the market. That exclusion is without prejudice tothe obligation to comply with this Regulation where an AI system falling into the scope of this Regulation is placedon the market or put into service as a result of such research and development activity and to the application ofprovisions on AI regulatory sandboxes and testing in real world conditions. Furthermore, without prejudice to theexclusion of AI systems specifically developed and put into service for the sole purpose of scientific research anddevelopment, any other AI system that may be used for the conduct of any research and development activity shouldremain subject to the provisions of this Regulation. In any event, any research and development activity should becarried out in accordance with recognised ethical and professional standards for scientific research and should beconducted in accordance with applicable Union law.(26) In order to introduce a proportionate and effective set of binding rules for AI systems, a clearly defined risk-basedapproach should be followed. That approach should tailor the type and content of such rules to the intensity andscope of the risks that AI systems can generate. It is therefore necessary to prohibit certain unacceptable AI practices,(27) While the risk-based approach is the basis for a proportionate and effective set of binding rules, it is important torecall the 2019 Ethics guidelines for trustworthy AI developed by the independent AI HLEG appointed by the Commission. In those guidelines, the AI HLEG developed seven non-binding ethical principles for AI which areintended to help ensure that AI is trustworthy and ethically sound. The seven principles include human agency andoversight; technical robustness and safety; privacy and data governance; transparency; diversity, non-discriminationand fairness; societal and environmental well-being and accountability. Without prejudice to the legally bindingrequirements of this Regulation and any other applicable Union law, those guidelines contribute to the design ofcoherent, trustworthy and human-centric AI, in line with the Charter and with the values on which the Union isfounded. According to the guidelines of the AI HLEG, human agency and oversight means that AI systems aredeveloped and used as a tool that serves people, respects human dignity and personal autonomy, and that isfunctioning in a way that can be appropriately controlled and overseen by humans. Technical robustness and safetymeans that AI systems are developed and used in a way that allows robustness in the case of problems and resilienceagainst attempts to alter the use or performance of the AI system so as to allow unlawful use by third parties, andminimise unintended harm. Privacy and data governance means that AI systems are developed and used inaccordance with privacy and data protection rules, while processing data that meets high standards in terms ofquality and integrity. Transparency means that AI systems are developed and used in a way that allows appropriatetraceability and explainability, while making humans aware that they communicate or interact with an AI system, aswell as duly informing deployers of the capabilities and limitations of that AI system and affected persons about theirrights. Diversity, non-discrimination and fairness means that AI systems are developed and used in a way thatincludes diverse actors and promotes equal access, gender equality and cultural diversity, while avoidingdiscriminatory impacts and unfair biases that are prohibited by Union or national law. Social and environmentalwell-being means that AI systems are developed and used in a sustainable and environmentally friendly manner aswell as in a way to benefit all human beings, while monitoring and assessing the long-term impacts on theindividual, society and democracy. The application of those principles should be translated, when possible, in thedesign and use of AI models. They should in any case serve as a basis for the drafting of codes of conduct under this Regulation. All stakeholders, including industry, academia, civil society and standardisation organisations, areencouraged to take into account, as appropriate, the ethical principles for the development of voluntary bestpractices and standards.(28) Aside from the many beneficial uses of AI, it can also be misused and provide novel and powerful tools formanipulative, exploitative and social control practices. Such practices are particularly harmful and abusive andshould be prohibited because they contradict Union values of respect for human dignity, freedom, equality, democracy and the rule of law and fundamental rights enshrined in the Charter, including the right tonon-discrimination, to data protection and to privacy and the rights of the child.(29) AI-enabled manipulative techniques can be used to persuade persons to engage in unwanted behaviours, or todeceive them by nudging them into decisions in a way that subverts and impairs their autonomy, decision-makingand free choices. The placing on the market, the putting into service or the use of certain AI systems with theobjective to or the effect of materially distorting human behaviour, whereby significant harms, in particular havingsufficiently important adverse impacts on physical, psychological health or financial interests are likely to occur, areparticularly dangerous and should therefore be prohibited. Such AI systems deploy subliminal components such asaudio, image, video stimuli that persons cannot perceive, as those stimuli are beyond human perception, or othermanipulative or deceptive techniques that subvert or impair person’s autonomy, decision-making or free choice inways that people are not consciously aware of those techniques or, where they are aware of them, can still bedeceived or are not able to control or resist them. This could be facilitated, for example, by machine-brain interfacesor virtual reality as they allow for a higher degree of control of what stimuli are presented to persons, insofar as theymay materially distort their behaviour in a significantly harmful manner. In addition, AI systems may also otherwiseexploit the vulnerabilities of a person or a specific group of persons due to their age, disability within the meaning of Directive (EU) 2019/882 of the European Parliament and of the Council (16), or a specific social or economicsituation that is likely to make those persons more vulnerable to exploitation such as persons living in extremepoverty, ethnic or religious minorities. Such AI systems can be placed on the market, put into service or used withthe objective to or the effect of materially distorting the behaviour of a person and in a manner that causes or isreasonably likely to cause significant harm to that or another person or groups of persons, including harms that maybe accumulated over time and should therefore be prohibited. It may not be possible to assume that there is anintention to distort behaviour where the distortion results from factors external to the AI system which are outsidethe control of the provider or the deployer, namely factors that may not be reasonably foreseeable and therefore notpossible for the provider or the deployer of the AI system to mitigate. In any case, it is not necessary for the provideror the deployer to have the intention to cause significant harm, provided that such harm results from themanipulative or exploitative AI-enabled practices. The prohibitions for such AI practices are complementary to theprovisions contained in Directive 2005/29/EC of the European Parliament and of the Council (17), in particular unfaircommercial practices leading to economic or financial harms to consumers are prohibited under all circumstances, irrespective of whether they are put in place through AI systems or otherwise. The prohibitions of manipulative andexploitative practices in this Regulation should not affect lawful practices in the context of medical treatment such aspsychological treatment of a mental disease or physical rehabilitation, when those practices are carried out inaccordance with the applicable law and medical standards, for example explicit consent of the individuals or theirlegal representatives. In addition, common and legitimate commercial practices, for example in the field ofadvertising, that comply with the applicable law should not, in themselves, be regarded as constituting harmfulmanipulative AI-enabled practices.(30) Biometric categorisation systems that are based on natural persons’ biometric data, such as an individual person’sface or fingerprint, to deduce or infer an individuals’ political opinions, trade union membership, religious orphilosophical beliefs, race, sex life or sexual orientation should be prohibited. That prohibition should not cover thelawful labelling, filtering or categorisation of biometric data sets acquired in line with Union or national lawaccording to biometric data, such as the sorting of images according to hair colour or eye colour, which can forexample be used in the area of law enforcement.(31) AI systems providing social scoring of natural persons by public or private actors may lead to discriminatoryoutcomes and the exclusion of certain groups. They may violate the right to dignity and non-discrimination and thevalues of equality and justice. Such AI systems evaluate or classify natural persons or groups thereof on the basis ofmultiple data points related to their social behaviour in multiple contexts or known, inferred or predicted personalor personality characteristics over certain periods of time. The social score obtained from such AI systems may leadto the detrimental or unfavourable treatment of natural persons or whole groups thereof in social contexts, whichare unrelated to the context in which the data was originally generated or collected or to a detrimental treatment thatis disproportionate or unjustified to the gravity of their social behaviour. AI systems entailing such unacceptablescoring practices and leading to such detrimental or unfavourable outcomes should therefore be prohibited. Thatprohibition should not affect lawful evaluation practices of natural persons that are carried out for a specific purposein accordance with Union and national law.(32) The use of AI systems for ‘real-time’ remote biometric identification of natural persons in publicly accessible spacesfor the purpose of law enforcement is particularly intrusive to the rights and freedoms of the concerned persons, tothe extent that it may affect the private life of a large part of the population, evoke a feeling of constant surveillanceand indirectly dissuade the exercise of the freedom of assembly and other fundamental rights. Technical inaccuraciesof AI systems intended for the remote biometric identification of natural persons can lead to biased results and entaildiscriminatory effects. Such possible biased results and discriminatory effects are particularly relevant with regard toage, ethnicity, race, sex or disabilities. In addition, the immediacy of the impact and the limited opportunities forfurther checks or corrections in relation to the use of such systems operating in real-time carry heightened risks forthe rights and freedoms of the persons concerned in the context of, or impacted by, law enforcement activities.(33) The use of those systems for the purpose of law enforcement should therefore be prohibited, except in exhaustivelylisted and narrowly defined situations, where the use is strictly necessary to achieve a substantial public interest, theimportance of which outweighs the risks. Those situations involve the search for certain victims of crime includingmissing persons; certain threats to the life or to the physical safety of natural persons or of a terrorist attack; and thelocalisation or identification of perpetrators or suspects of the criminal offences listed in an annex to this Regulation, where those criminal offences are punishable in the Member State concerned by a custodial sentence or a detention(17) Directive 2005/29/EC of the European Parliament and of the Council of 11 May 2005 concerning unfair business-to-consumercommercial practices in the internal market and amending Council Directive 84/450/EEC, Directives 97/7/EC, 98/27/EC andorder for a maximum period of at least four years and as they are defined in the law of that Member State. Sucha threshold for the custodial sentence or detention order in accordance with national law contributes to ensuringthat the offence should be serious enough to potentially justify the use of ‘real-time’ remote biometric identificationsystems. Moreover, the list of criminal offences provided in an annex to this Regulation is based on the 32 criminaloffences listed in the Council Framework Decision 2002/584/JHA (18), taking into account that some of thoseoffences are, in practice, likely to be more relevant than others, in that the recourse to ‘real-time’ remote biometricidentification could, foreseeably, be necessary and proportionate to highly varying degrees for the practical pursuitof the localisation or identification of a perpetrator or suspect of the different criminal offences listed and havingregard to the likely differences in the seriousness, probability and scale of the harm or possible negativeconsequences. An imminent threat to life or the physical safety of natural persons could also result from a seriousdisruption of critical infrastructure, as defined in Article 2, point (4) of Directive (EU) 2022/2557 of the European Parliament and of the Council (19), where the disruption or destruction of such critical infrastructure would result inan imminent threat to life or the physical safety of a person, including through serious harm to the provision of basicsupplies to the population or to the exercise of the core function of the State. In addition, this Regulation shouldpreserve the ability for law enforcement, border control, immigration or asylum authorities to carry out identitychecks in the presence of the person concerned in accordance with the conditions set out in Union and national lawfor such checks. In particular, law enforcement, border control, immigration or asylum authorities should be able touse information systems, in accordance with Union or national law, to identify persons who, during an identitycheck, either refuse to be identified or are unable to state or prove their identity, without being required by this Regulation to obtain prior authorisation. This could be, for example, a person involved in a crime, being unwilling, or unable due to an accident or a medical condition, to disclose their identity to law enforcement authorities.(34) In order to ensure that those systems are used in a responsible and proportionate manner, it is also important toestablish that, in each of those exhaustively listed and narrowly defined situations, certain elements should be takeninto account, in particular as regards the nature of the situation giving rise to the request and the consequences ofthe use for the rights and freedoms of all persons concerned and the safeguards and conditions provided for with theuse. In addition, the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for thepurpose of law enforcement should be deployed only to confirm the specifically targeted individual’s identity andshould be limited to what is strictly necessary concerning the period of time, as well as the geographic and personalscope, having regard in particular to the evidence or indications regarding the threats, the victims or perpetrator. Theuse of the real-time remote biometric identification system in publicly accessible spaces should be authorised only ifthe relevant law enforcement authority has completed a fundamental rights impact assessment and, unless providedotherwise in this Regulation, has registered the system in the database as set out in this Regulation. The referencedatabase of persons should be appropriate for each use case in each of the situations mentioned above.(35) Each use of a ‘real-time’ remote biometric identification system in publicly accessible spaces for the purpose of lawenforcement should be subject to an express and specific authorisation by a judicial authority or by an independentadministrative authority of a Member State whose decision is binding. Such authorisation should, in principle, beobtained prior to the use of the AI system with a view to identifying a person or persons. Exceptions to that ruleshould be allowed in duly justified situations on grounds of urgency, namely in situations where the need to use thesystems concerned is such as to make it effectively and objectively impossible to obtain an authorisation beforecommencing the use of the AI system. In such situations of urgency, the use of the AI system should be restricted tothe absolute minimum necessary and should be subject to appropriate safeguards and conditions, as determined innational law and specified in the context of each individual urgent use case by the law enforcement authority itself. In addition, the law enforcement authority should in such situations request such authorisation while providing thereasons for not having been able to request it earlier, without undue delay and at the latest within 24 hours. If suchan authorisation is rejected, the use of real-time biometric identification systems linked to that authorisation shouldcease with immediate effect and all the data related to such use should be discarded and deleted. Such data includes(18) Council Framework Decision 2002/584/JHA of 13 June 2002 on the European arrest warrant and the surrender proceduresbetween Member States (OJ L 190, 18.7.2002, p. 1). input data directly acquired by an AI system in the course of the use of such system as well as the results and outputsof the use linked to that authorisation. It should not include input that is legally acquired in accordance with another Union or national law. In any case, no decision producing an adverse legal effect on a person should be taken basedsolely on the output of the remote biometric identification system.(36) In order to carry out their tasks in accordance with the requirements set out in this Regulation as well as in nationalrules, the relevant market surveillance authority and the national data protection authority should be notified of eachuse of the real-time biometric identification system. Market surveillance authorities and the national data protectionauthorities that have been notified should submit to the Commission an annual report on the use of real-timebiometric identification systems.(37) Furthermore, it is appropriate to provide, within the exhaustive framework set by this Regulation that such use inthe territory of a Member State in accordance with this Regulation should only be possible where and in as far as the Member State concerned has decided to expressly provide for the possibility to authorise such use in its detailed rulesof national law. Consequently, Member States remain free under this Regulation not to provide for such a possibilityat all or to only provide for such a possibility in respect of some of the objectives capable of justifying authorised useidentified in this Regulation. Such national rules should be notified to the Commission within 30 days of theiradoption.(38) The use of AI systems for real-time remote biometric identification of natural persons in publicly accessible spacesfor the purpose of law enforcement necessarily involves the processing of biometric data. The rules of this Regulation that prohibit, subject to certain exceptions, such use, which are based on Article 16 TFEU, should applyas lex specialis in respect of the rules on the processing of biometric data contained in Article 10 of Directive (EU)2016/680, thus regulating such use and the processing of biometric data involved in an exhaustive manner. Therefore, such use and processing should be possible only in as far as it is compatible with the framework set bythis Regulation, without there being scope, outside that framework, for the competent authorities, where they act forpurpose of law enforcement, to use such systems and process such data in connection thereto on the grounds listedin Article 10 of Directive (EU) 2016/680. In that context, this Regulation is not intended to provide the legal basisfor the processing of personal data under Article 8 of Directive (EU) 2016/680. However, the use of real-time remotebiometric identification systems in publicly accessible spaces for purposes other than law enforcement, including bycompetent authorities, should not be covered by the specific framework regarding such use for the purpose of lawenforcement set by this Regulation. Such use for purposes other than law enforcement should therefore not besubject to the requirement of an authorisation under this Regulation and the applicable detailed rules of national lawthat may give effect to that authorisation.(39) Any processing of biometric data and other personal data involved in the use of AI systems for biometricidentification, other than in connection to the use of real-time remote biometric identification systems in publiclyaccessible spaces for the purpose of law enforcement as regulated by this Regulation, should continue to complywith all requirements resulting from Article 10 of Directive (EU) 2016/680. For purposes other than lawenforcement, Article 9(1) of Regulation (EU) 2016/679 and Article 10(1) of Regulation (EU) 2018/1725 prohibit theprocessing of biometric data subject to limited exceptions as provided in those Articles. In the application of Article9(1) of Regulation (EU) 2016/679, the use of remote biometric identification for purposes other than lawenforcement has already been subject to prohibition decisions by national data protection authorities.(40) In accordance with Article 6a of Protocol No 21 on the position of the United Kingdom and Ireland in respect of thearea of freedom, security and justice, as annexed to the TEU and to the TFEU, Ireland is not bound by the rules laiddown in Article 5(1), first subparagraph, point (g), to the extent it applies to the use of biometric categorisationsystems for activities in the field of police cooperation and judicial cooperation in criminal matters, Article 5(1), firstsubparagraph, point (d), to the extent it applies to the use of AI systems covered by that provision, Article 5(1), firstsubparagraph, point (h), Article 5(2) to (6) and Article 26(10) of this Regulation adopted on the basis of Article 16TFEU which relate to the processing of personal data by the Member States when carrying out activities fallingwithin the scope of Chapter 4 or Chapter 5 of Title V of Part Three of the TFEU, where Ireland is not bound by therules governing the forms of judicial cooperation in criminal matters or police cooperation which requirecompliance with the provisions laid down on the basis of Article 16 TFEU.(41) In accordance with Articles 2 and 2a of Protocol No 22 on the position of Denmark, annexed to the TEU and to the TFEU, Denmark is not bound by rules laid down in Article 5(1), first subparagraph, point (g), to the extent it appliesto the use of biometric categorisation systems for activities in the field of police cooperation and judicial cooperationadopted on the basis of Article 16 TFEU, or subject to their application, which relate to the processing of personaldata by the Member States when carrying out activities falling within the scope of Chapter 4 or Chapter 5 of Title V of Part Three of the TFEU.(42) In line with the presumption of innocence, natural persons in the Union should always be judged on their actualbehaviour. Natural persons should never be judged on AI-predicted behaviour based solely on their profiling, personality traits or characteristics, such as nationality, place of birth, place of residence, number of children, level ofdebt or type of car, without a reasonable suspicion of that person being involved in a criminal activity based onobjective verifiable facts and without human assessment thereof. Therefore, risk assessments carried out with regardto natural persons in order to assess the likelihood of their offending or to predict the occurrence of an actual orpotential criminal offence based solely on profiling them or on assessing their personality traits and characteristicsshould be prohibited. In any case, that prohibition does not refer to or touch upon risk analytics that are not basedon the profiling of individuals or on the personality traits and characteristics of individuals, such as AI systems usingrisk analytics to assess the likelihood of financial fraud by undertakings on the basis of suspicious transactions orrisk analytic tools to predict the likelihood of the localisation of narcotics or illicit goods by customs authorities, forexample on the basis of known trafficking routes.(43) The placing on the market, the putting into service for that specific purpose, or the use of AI systems that create orexpand facial recognition databases through the untargeted scraping of facial images from the internet or CCTVfootage, should be prohibited because that practice adds to the feeling of mass surveillance and can lead to grossviolations of fundamental rights, including the right to privacy.(44) There are serious concerns about the scientific basis of AI systems aiming to identify or infer emotions, particularlyas expression of emotions vary considerably across cultures and situations, and even within a single individual. Among the key shortcomings of such systems are the limited reliability, the lack of specificity and the limitedgeneralisability. Therefore, AI systems identifying or inferring emotions or intentions of natural persons on the basisof their biometric data may lead to discriminatory outcomes and can be intrusive to the rights and freedoms of theconcerned persons. Considering the imbalance of power in the context of work or education, combined with theintrusive nature of these systems, such systems could lead to detrimental or unfavourable treatment of certainnatural persons or whole groups thereof. Therefore, the placing on the market, the putting into service, or the use of AI systems intended to be used to detect the emotional state of individuals in situations related to the workplace andeducation should be prohibited. That prohibition should not cover AI systems placed on the market strictly formedical or safety reasons, such as systems intended for therapeutical use.(45) Practices that are prohibited by Union law, including data protection law, non-discrimination law, consumerprotection law, and competition law, should not be affected by this Regulation.(46) High-risk AI systems should only be placed on the Union market, put into service or used if they comply withcertain mandatory requirements. Those requirements should ensure that high-risk AI systems available in the Unionor whose output is otherwise used in the Union do not pose unacceptable risks to important Union public interestsas recognised and protected by Union law. On the basis of the New Legislative Framework, as clarified in the Commission notice ‘The “Blue Guide” on the implementation of EU product rules 2022’ (20), the general rule is thatmore than one legal act of Union harmonisation legislation, such as Regulations (EU) 2017/745 (21) and (EU)2017/746 (22) of the European Parliament and of the Council or Directive 2006/42/EC of the European Parliamentand of the Council (23), may be applicable to one product, since the making available or putting into service can take(20) OJ C 247, 29.6.2022, p. 1.(21) Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices, amending Directive2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and93/42/EEC (OJ L 117, 5.5.2017, p. 1).(22) Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 April 2017 on in vitro diagnostic medical devices andrepealing Directive 98/79/EC and Commission Decision 2010/227/EU (OJ L 117, 5.5.2017, p. 176). place only when the product complies with all applicable Union harmonisation legislation. To ensure consistencyand avoid unnecessary administrative burdens or costs, providers of a product that contains one or more high-risk AI systems, to which the requirements of this Regulation and of the Union harmonisation legislation listed in anannex to this Regulation apply, should have flexibility with regard to operational decisions on how to ensurecompliance of a product that contains one or more AI systems with all applicable requirements of the Unionharmonisation legislation in an optimal manner. AI systems identified as high-risk should be limited to those thathave a significant harmful impact on the health, safety and fundamental rights of persons in the Union and suchlimitation should minimise any potential restriction to international trade.(47) AI systems could have an adverse impact on the health and safety of persons, in particular when such systemsoperate as safety components of products. Consistent with the objectives of Union harmonisation legislation tofacilitate the free movement of products in the internal market and to ensure that only safe and otherwise compliantproducts find their way into the market, it is important that the safety risks that may be generated by a product asa whole due to its digital components, including AI systems, are duly prevented and mitigated. For instance, increasingly autonomous robots, whether in the context of manufacturing or personal assistance and care should beable to safely operate and performs their functions in complex environments. Similarly, in the health sector wherethe stakes for life and health are particularly high, increasingly sophisticated diagnostics systems and systemssupporting human decisions should be reliable and accurate.(48) The extent of the adverse impact caused by the AI system on the fundamental rights protected by the Charter is ofparticular relevance when classifying an AI system as high risk. Those rights include the right to human dignity, respect for private and family life, protection of personal data, freedom of expression and information, freedom ofassembly and of association, the right to non-discrimination, the right to education, consumer protection, workers’rights, the rights of persons with disabilities, gender equality, intellectual property rights, the right to an effectiveremedy and to a fair trial, the right of defence and the presumption of innocence, and the right to goodadministration. In addition to those rights, it is important to highlight the fact that children have specific rights asenshrined in Article 24 of the Charter and in the United Nations Convention on the Rights of the Child, furtherdeveloped in the UNCRC General Comment No 25 as regards the digital environment, both of which requireconsideration of the children’s vulnerabilities and provision of such protection and care as necessary for theirwell-being. The fundamental right to a high level of environmental protection enshrined in the Charter andimplemented in Union policies should also be considered when assessing the severity of the harm that an AI systemcan cause, including in relation to the health and safety of persons.(49) As regards high-risk AI systems that are safety components of products or systems, or which are themselvesproducts or systems falling within the scope of Regulation (EC) No 300/2008 of the European Parliament and of the Council (24), Regulation (EU) No 167/2013 of the European Parliament and of the Council (25), Regulation(EU) No 168/2013 of the European Parliament and of the Council (26), Directive 2014/90/EU of the European Parliament and of the Council (27), Directive (EU) 2016/797 of the European Parliament and of the Council (28), Regulation (EU) 2018/858 of the European Parliament and of the Council (29), Regulation (EU) 2018/1139 of the(24) Regulation (EC) No 300/2008 of the European Parliament and of the Council of 11 March 2008 on common rules in the field ofcivil aviation security and repealing Regulation (EC) No 2320/2002 (OJ L 97, 9.4.2008, p. 72).(25) Regulation (EU) No 167/2013 of the European Parliament and of the Council of 5 February 2013 on the approval and marketsurveillance of agricultural and forestry vehicles (OJ L 60, 2.3.2013, p. 1).(26) Regulation (EU) No 168/2013 of the European Parliament and of the Council of 15 January 2013 on the approval and marketsurveillance of two- or three-wheel vehicles and quadricycles (OJ L 60, 2.3.2013, p. 52).(27) Directive 2014/90/EU of the European Parliament and of the Council of 23 July 2014 on marine equipment and repealing Council Directive 96/98/EC (OJ L 257, 28.8.2014, p. 146).(28) Directive (EU) 2016/797 of the European Parliament and of the Council of 11 May 2016 on the interoperability of the rail systemwithin the European Union (OJ L 138, 26.5.2016, p. 44).(29) Regulation (EU) 2018/858 of the European Parliament and of the Council of 30 May 2018 on the approval and market surveillance European Parliament and of the Council (30), and Regulation (EU) 2019/2144 of the European Parliament and of the Council (31), it is appropriate to amend those acts to ensure that the Commission takes into account, on the basis ofthe technical and regulatory specificities of each sector, and without interfering with existing governance, conformityassessment and enforcement mechanisms and authorities established therein, the mandatory requirements forhigh-risk AI systems laid down in this Regulation when adopting any relevant delegated or implementing acts on thebasis of those acts.(50) As regards AI systems that are safety components of products, or which are themselves products, falling within thescope of certain Union harmonisation legislation listed in an annex to this Regulation, it is appropriate to classifythem as high-risk under this Regulation if the product concerned undergoes the conformity assessment procedurewith a third-party conformity assessment body pursuant to that relevant Union harmonisation legislation. Inparticular, such products are machinery, toys, lifts, equipment and protective systems intended for use in potentiallyexplosive atmospheres, radio equipment, pressure equipment, recreational craft equipment, cableway installations, appliances burning gaseous fuels, medical devices, in vitro diagnostic medical devices, automotive and aviation.(51) The classification of an AI system as high-risk pursuant to this Regulation should not necessarily mean that theproduct whose safety component is the AI system, or the AI system itself as a product, is considered to be high-riskunder the criteria established in the relevant Union harmonisation legislation that applies to the product. This is, inparticular, the case for Regulations (EU) 2017/745 and (EU) 2017/746, where a third-party conformity assessment isprovided for medium-risk and high-risk products.(52) As regards stand-alone AI systems, namely high-risk AI systems other than those that are safety components ofproducts, or that are themselves products, it is appropriate to classify them as high-risk if, in light of their intendedpurpose, they pose a high risk of harm to the health and safety or the fundamental rights of persons, taking intoaccount both the severity of the possible harm and its probability of occurrence and they are used in a number ofspecifically pre-defined areas specified in this Regulation. The identification of those systems is based on the samemethodology and criteria envisaged also for any future amendments of the list of high-risk AI systems that the Commission should be empowered to adopt, via delegated acts, to take into account the rapid pace of technologicaldevelopment, as well as the potential changes in the use of AI systems.(53) It is also important to clarify that there may be specific cases in which AI systems referred to in pre-defined areasspecified in this Regulation do not lead to a significant risk of harm to the legal interests protected under those areasbecause they do not materially influence the decision-making or do not harm those interests substantially. For thepurposes of this Regulation, an AI system that does not materially influence the outcome of decision-making shouldbe understood to be an AI system that does not have an impact on the substance, and thereby the outcome, ofdecision-making, whether human or automated. An AI system that does not materially influence the outcome ofdecision-making could include situations in which one or more of the following conditions are fulfilled. The firstsuch condition should be that the AI system is intended to perform a narrow procedural task, such as an AI systemthat transforms unstructured data into structured data, an AI system that classifies incoming documents intocategories or an AI system that is used to detect duplicates among a large number of applications. Those tasks are ofsuch narrow and limited nature that they pose only limited risks which are not increased through the use of an AIsystem in a context that is listed as a high-risk use in an annex to this Regulation. The second condition should be(30) Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common rules in the field of civilaviation and establishing a European Union Aviation Safety Agency, and amending Regulations (EC) No 2111/2005, (EC)No 1008/2008, (EU) No 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and 2014/53/EU of the European Parliamentand of the Council, and repealing Regulations (EC) No 552/2004 and (EC) No 216/2008 of the European Parliament and of the Council and Council Regulation (EEC) No 3922/91 (OJ L 212, 22.8.2018, p. 1).(31) Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November 2019 on type-approval requirementsfor motor vehicles and their trailers, and systems, components and separate technical units intended for such vehicles, as regardstheir general safety and the protection of vehicle occupants and vulnerable road users, amending Regulation (EU) 2018/858 of the European Parliament and of the Council and repealing Regulations (EC) No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 ofthe European Parliament and of the Council and Commission Regulations (EC) No 631/2009, (EU) No 406/2010, (EU)No 672/2010, (EU) No 1003/2010, (EU) No 1005/2010, (EU) No 1008/2010, (EU) No 1009/2010, (EU) No 19/2011, (EU)that the task performed by the AI system is intended to improve the result of a previously completed human activitythat may be relevant for the purposes of the high-risk uses listed in an annex to this Regulation. Considering thosecharacteristics, the AI system provides only an additional layer to a human activity with consequently lowered risk. That condition would, for example, apply to AI systems that are intended to improve the language used in previouslydrafted documents, for example in relation to professional tone, academic style of language or by aligning text toa certain brand messaging. The third condition should be that the AI system is intended to detect decision-makingpatterns or deviations from prior decision-making patterns. The risk would be lowered because the use of the AIsystem follows a previously completed human assessment which it is not meant to replace or influence, withoutproper human review. Such AI systems include for instance those that, given a certain grading pattern of a teacher, can be used to check ex post whether the teacher may have deviated from the grading pattern so as to flag potentialinconsistencies or anomalies. The fourth condition should be that the AI system is intended to perform a task that isonly preparatory to an assessment relevant for the purposes of the AI systems listed in an annex to this Regulation, thus making the possible impact of the output of the system very low in terms of representing a risk for theassessment to follow. That condition covers, inter alia, smart solutions for file handling, which include variousfunctions from indexing, searching, text and speech processing or linking data to other data sources, or AI systemsused for translation of initial documents. In any case, AI systems used in high-risk use-cases listed in an annex to this Regulation should be considered to pose significant risks of harm to the health, safety or fundamental rights if the AIsystem implies profiling within the meaning of Article 4, point (4) of Regulation (EU) 2016/679 or Article 3, point (4) of Directive (EU) 2016/680 or Article 3, point (5) of Regulation (EU) 2018/1725. To ensure traceabilityand transparency, a provider who considers that an AI system is not high-risk on the basis of the conditions referredto above should draw up documentation of the assessment before that system is placed on the market or put intoservice and should provide that documentation to national competent authorities upon request. Such a providershould be obliged to register the AI system in the EU database established under this Regulation. With a view toproviding further guidance for the practical implementation of the conditions under which the AI systems listed inan annex to this Regulation are, on an exceptional basis, non-high-risk, the Commission should, after consulting the Board, provide guidelines specifying that practical implementation, completed by a comprehensive list of practicalexamples of use cases of AI systems that are high-risk and use cases that are not.(54) As biometric data constitutes a special category of personal data, it is appropriate to classify as high-risk severalcritical-use cases of biometric systems, insofar as their use is permitted under relevant Union and national law. Technical inaccuracies of AI systems intended for the remote biometric identification of natural persons can lead tobiased results and entail discriminatory effects. The risk of such biased results and discriminatory effects isparticularly relevant with regard to age, ethnicity, race, sex or disabilities. Remote biometric identification systemsshould therefore be classified as high-risk in view of the risks that they pose. Such a classification excludes AIsystems intended to be used for biometric verification, including authentication, the sole purpose of which is toconfirm that a specific natural person is who that person claims to be and to confirm the identity of a natural personfor the sole purpose of having access to a service, unlocking a device or having secure access to premises. In addition, AI systems intended to be used for biometric categorisation according to sensitive attributes or characteristicsprotected under Article 9(1) of Regulation (EU) 2016/679 on the basis of biometric data, in so far as these are notprohibited under this Regulation, and emotion recognition systems that are not prohibited under this Regulation, should be classified as high-risk. Biometric systems which are intended to be used solely for the purpose of enablingcybersecurity and personal data protection measures should not be considered to be high-risk AI systems.(55) As regards the management and operation of critical infrastructure, it is appropriate to classify as high-risk the AIsystems intended to be used as safety components in the management and operation of critical digital infrastructureas listed in point (8) of the Annex to Directive (EU) 2022/2557, road traffic and the supply of water, gas, heating andelectricity, since their failure or malfunctioning may put at risk the life and health of persons at large scale and lead toappreciable disruptions in the ordinary conduct of social and economic activities. Safety components of criticalsystem to function. The failure or malfunctioning of such components might directly lead to risks to the physicalintegrity of critical infrastructure and thus to risks to health and safety of persons and property. Componentsintended to be used solely for cybersecurity purposes should not qualify as safety components. Examples of safetycomponents of such critical infrastructure may include systems for monitoring water pressure or fire alarmcontrolling systems in cloud computing centres.(56) The deployment of AI systems in education is important to promote high-quality digital education and training andto allow all learners and teachers to acquire and share the necessary digital skills and competences, including medialiteracy, and critical thinking, to take an active part in the economy, society, and in democratic processes. However, AI systems used in education or vocational training, in particular for determining access or admission, for assigningpersons to educational and vocational training institutions or programmes at all levels, for evaluating learningoutcomes of persons, for assessing the appropriate level of education for an individual and materially influencing thelevel of education and training that individuals will receive or will be able to access or for monitoring and detectingprohibited behaviour of students during tests should be classified as high-risk AI systems, since they may determinethe educational and professional course of a person’s life and therefore may affect that person’s ability to securea livelihood. When improperly designed and used, such systems may be particularly intrusive and may violate theright to education and training as well as the right not to be discriminated against and perpetuate historical patternsof discrimination, for example against women, certain age groups, persons with disabilities, or persons of certainracial or ethnic origins or sexual orientation.(57) AI systems used in employment, workers management and access to self-employment, in particular for therecruitment and selection of persons, for making decisions affecting terms of the work-related relationship, promotion and termination of work-related contractual relationships, for allocating tasks on the basis of individualbehaviour, personal traits or characteristics and for monitoring or evaluation of persons in work-related contractualrelationships, should also be classified as high-risk, since those systems may have an appreciable impact on futurecareer prospects, livelihoods of those persons and workers’ rights. Relevant work-related contractual relationshipsshould, in a meaningful manner, involve employees and persons providing services through platforms as referred toin the Commission Work Programme 2021. Throughout the recruitment process and in the evaluation, promotion, or retention of persons in work-related contractual relationships, such systems may perpetuate historical patterns ofdiscrimination, for example against women, certain age groups, persons with disabilities, or persons of certain racialor ethnic origins or sexual orientation. AI systems used to monitor the performance and behaviour of such personsmay also undermine their fundamental rights to data protection and privacy.(58) Another area in which the use of AI systems deserves special consideration is the access to and enjoyment of certainessential private and public services and benefits necessary for people to fully participate in society or to improveone’s standard of living. In particular, natural persons applying for or receiving essential public assistance benefitsand services from public authorities namely healthcare services, social security benefits, social services providingprotection in cases such as maternity, illness, industrial accidents, dependency or old age and loss of employmentand social and housing assistance, are typically dependent on those benefits and services and in a vulnerable positionin relation to the responsible authorities. If AI systems are used for determining whether such benefits and servicesshould be granted, denied, reduced, revoked or reclaimed by authorities, including whether beneficiaries arelegitimately entitled to such benefits or services, those systems may have a significant impact on persons’ livelihoodand may infringe their fundamental rights, such as the right to social protection, non-discrimination, human dignityor an effective remedy and should therefore be classified as high-risk. Nonetheless, this Regulation should nothamper the development and use of innovative approaches in the public administration, which would stand tobenefit from a wider use of compliant and safe AI systems, provided that those systems do not entail a high risk tolegal and natural persons. In addition, AI systems used to evaluate the credit score or creditworthiness of naturalpersons should be classified as high-risk AI systems, since they determine those persons’ access to financial resourcesor essential services such as housing, electricity, and telecommunication services. AI systems used for those purposesmay lead to discrimination between persons or groups and may perpetuate historical patterns of discrimination, such as that based on racial or ethnic origins, gender, disabilities, age or sexual orientation, or may create new formsof discriminatory impacts. However, AI systems provided for by Union law for the purpose of detecting fraud in theto be used for risk assessment and pricing in relation to natural persons for health and life insurance can also havea significant impact on persons’ livelihood and if not duly designed, developed and used, can infringe theirfundamental rights and can lead to serious consequences for people’s life and health, including financial exclusionand discrimination. Finally, AI systems used to evaluate and classify emergency calls by natural persons or todispatch or establish priority in the dispatching of emergency first response services, including by police, firefightersand medical aid, as well as of emergency healthcare patient triage systems, should also be classified as high-risk sincethey make decisions in very critical situations for the life and health of persons and their property.(59) Given their role and responsibility, actions by law enforcement authorities involving certain uses of AI systems arecharacterised by a significant degree of power imbalance and may lead to surveillance, arrest or deprivation ofa natural person’s liberty as well as other adverse impacts on fundamental rights guaranteed in the Charter. Inparticular, if the AI system is not trained with high-quality data, does not meet adequate requirements in terms of itsperformance, its accuracy or robustness, or is not properly designed and tested before being put on the market orotherwise put into service, it may single out people in a discriminatory or otherwise incorrect or unjust manner. Furthermore, the exercise of important procedural fundamental rights, such as the right to an effective remedy andto a fair trial as well as the right of defence and the presumption of innocence, could be hampered, in particular, where such AI systems are not sufficiently transparent, explainable and documented. It is therefore appropriate toclassify as high-risk, insofar as their use is permitted under relevant Union and national law, a number of AI systemsintended to be used in the law enforcement context where accuracy, reliability and transparency is particularlyimportant to avoid adverse impacts, retain public trust and ensure accountability and effective redress. In view of thenature of the activities and the risks relating thereto, those high-risk AI systems should include in particular AIsystems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies, offices, or agencies in support of law enforcement authorities for assessing the risk of a natural person to become a victim ofcriminal offences, as polygraphs and similar tools, for the evaluation of the reliability of evidence in in the course ofinvestigation or prosecution of criminal offences, and, insofar as not prohibited under this Regulation, for assessingthe risk of a natural person offending or reoffending not solely on the basis of the profiling of natural persons or theassessment of personality traits and characteristics or the past criminal behaviour of natural persons or groups, forprofiling in the course of detection, investigation or prosecution of criminal offences. AI systems specificallyintended to be used for administrative proceedings by tax and customs authorities as well as by financial intelligenceunits carrying out administrative tasks analysing information pursuant to Union anti-money laundering law shouldnot be classified as high-risk AI systems used by law enforcement authorities for the purpose of prevention, detection, investigation and prosecution of criminal offences. The use of AI tools by law enforcement and otherrelevant authorities should not become a factor of inequality, or exclusion. The impact of the use of AI tools on thedefence rights of suspects should not be ignored, in particular the difficulty in obtaining meaningful information onthe functioning of those systems and the resulting difficulty in challenging their results in court, in particular bynatural persons under investigation.(60) AI systems used in migration, asylum and border control management affect persons who are often in particularlyvulnerable position and who are dependent on the outcome of the actions of the competent public authorities. Theaccuracy, non-discriminatory nature and transparency of the AI systems used in those contexts are thereforeparticularly important to guarantee respect for the fundamental rights of the affected persons, in particular theirrights to free movement, non-discrimination, protection of private life and personal data, international protectionand good administration. It is therefore appropriate to classify as high-risk, insofar as their use is permitted underrelevant Union and national law, AI systems intended to be used by or on behalf of competent public authorities orby Union institutions, bodies, offices or agencies charged with tasks in the fields of migration, asylum and bordercontrol management as polygraphs and similar tools, for assessing certain risks posed by natural persons enteringthe territory of a Member State or applying for visa or asylum, for assisting competent public authorities for theexamination, including related assessment of the reliability of evidence, of applications for asylum, visa and residencepermits and associated complaints with regard to the objective to establish the eligibility of the natural personsapplying for a status, for the purpose of detecting, recognising or identifying natural persons in the context ofmigration, asylum and border control management, with the exception of verification of travel documents. AIof the Council (32), the Directive 2013/32/EU of the European Parliament and of the Council (33), and other relevant Union law. The use of AI systems in migration, asylum and border control management should, in no circumstances, be used by Member States or Union institutions, bodies, offices or agencies as a means to circumvent theirinternational obligations under the UN Convention relating to the Status of Refugees done at Geneva on 28 July1951 as amended by the Protocol of 31 January 1967. Nor should they be used to in any way infringe on theprinciple of non-refoulement, or to deny safe and effective legal avenues into the territory of the Union, includingthe right to international protection.(61) Certain AI systems intended for the administration of justice and democratic processes should be classified ashigh-risk, considering their potentially significant impact on democracy, the rule of law, individual freedoms as wellas the right to an effective remedy and to a fair trial. In particular, to address the risks of potential biases, errors andopacity, it is appropriate to qualify as high-risk AI systems intended to be used by a judicial authority or on its behalfto assist judicial authorities in researching and interpreting facts and the law and in applying the law to a concrete setof facts. AI systems intended to be used by alternative dispute resolution bodies for those purposes should also beconsidered to be high-risk when the outcomes of the alternative dispute resolution proceedings produce legal effectsfor the parties. The use of AI tools can support the decision-making power of judges or judicial independence, butshould not replace it: the final decision-making must remain a human-driven activity. The classification of AIsystems as high-risk should not, however, extend to AI systems intended for purely ancillary administrative activitiesthat do not affect the actual administration of justice in individual cases, such as anonymisation orpseudonymisation of judicial decisions, documents or data, communication between personnel, administrative tasks.(62) Without prejudice to the rules provided for in Regulation (EU) 2024/900 of the European Parliament and of the Council (34), and in order to address the risks of undue external interference with the right to vote enshrined in Article 39 of the Charter, and of adverse effects on democracy and the rule of law, AI systems intended to be used toinfluence the outcome of an election or referendum or the voting behaviour of natural persons in the exercise oftheir vote in elections or referenda should be classified as high-risk AI systems with the exception of AI systemswhose output natural persons are not directly exposed to, such as tools used to organise, optimise and structurepolitical campaigns from an administrative and logistical point of view.(63) The fact that an AI system is classified as a high-risk AI system under this Regulation should not be interpreted asindicating that the use of the system is lawful under other acts of Union law or under national law compatible with Union law, such as on the protection of personal data, on the use of polygraphs and similar tools or other systems todetect the emotional state of natural persons. Any such use should continue to occur solely in accordance with theapplicable requirements resulting from the Charter and from the applicable acts of secondary Union law andnational law. This Regulation should not be understood as providing for the legal ground for processing of personaldata, including special categories of personal data, where relevant, unless it is specifically otherwise provided for inthis Regulation.(64) To mitigate the risks from high-risk AI systems placed on the market or put into service and to ensure a high level oftrustworthiness, certain mandatory requirements should apply to high-risk AI systems, taking into account theintended purpose and the context of use of the AI system and according to the risk-management system to beestablished by the provider. The measures adopted by the providers to comply with the mandatory requirements ofthis Regulation should take into account the generally acknowledged state of the art on AI, be proportionate andeffective to meet the objectives of this Regulation. Based on the New Legislative Framework, as clarified in Commission notice ‘The “Blue Guide” on the implementation of EU product rules 2022’, the general rule is thatmore than one legal act of Union harmonisation legislation may be applicable to one product, since the makingavailable or putting into service can take place only when the product complies with all applicable Unionharmonisation legislation. The hazards of AI systems covered by the requirements of this Regulation concerndifferent aspects than the existing Union harmonisation legislation and therefore the requirements of this Regulationwould complement the existing body of the Union harmonisation legislation. For example, machinery or medicaldevices products incorporating an AI system might present risks not addressed by the essential health and safety(32) Regulation (EC) No 810/2009 of the European Parliament and of the Council of 13 July 2009 establishing a Community Code on Visas (Visa Code) (OJ L 243, 15.9.2009, p. 1).(33) Directive 2013/32/EU of the European Parliament and of the Council of 26 June 2013 on common procedures for granting andwithdrawing international protection (OJ L 180, 29.6.2013, p. 60). requirements set out in the relevant Union harmonised legislation, as that sectoral law does not deal with risksspecific to AI systems. This calls for a simultaneous and complementary application of the various legislative acts. Toensure consistency and to avoid an unnecessary administrative burden and unnecessary costs, providers of a productthat contains one or more high-risk AI system, to which the requirements of this Regulation and of the Unionharmonisation legislation based on the New Legislative Framework and listed in an annex to this Regulation apply, should have flexibility with regard to operational decisions on how to ensure compliance of a product that containsone or more AI systems with all the applicable requirements of that Union harmonised legislation in an optimalmanner. That flexibility could mean, for example a decision by the provider to integrate a part of the necessarytesting and reporting processes, information and documentation required under this Regulation into already existingdocumentation and procedures required under existing Union harmonisation legislation based on the New Legislative Framework and listed in an annex to this Regulation. This should not, in any way, undermine theobligation of the provider to comply with all the applicable requirements.(65) The risk-management system should consist of a continuous, iterative process that is planned and run throughoutthe entire lifecycle of a high-risk AI system. That process should be aimed at identifying and mitigating the relevantrisks of AI systems on health, safety and fundamental rights. The risk-management system should be regularlyreviewed and updated to ensure its continuing effectiveness, as well as justification and documentation of anysignificant decisions and actions taken subject to this Regulation. This process should ensure that the provideridentifies risks or adverse impacts and implements mitigation measures for the known and reasonably foreseeablerisks of AI systems to the health, safety and fundamental rights in light of their intended purpose and reasonablyforeseeable misuse, including the possible risks arising from the interaction between the AI system and theenvironment within which it operates. The risk-management system should adopt the most appropriaterisk-management measures in light of the state of the art in AI. When identifying the most appropriaterisk-management measures, the provider should document and explain the choices made and, when relevant, involve experts and external stakeholders. In identifying the reasonably foreseeable misuse of high-risk AI systems, the provider should cover uses of AI systems which, while not directly covered by the intended purpose andprovided for in the instruction for use may nevertheless be reasonably expected to result from readily predictablehuman behaviour in the context of the specific characteristics and use of a particular AI system. Any known orforeseeable circumstances related to the use of the high-risk AI system in accordance with its intended purpose orunder conditions of reasonably foreseeable misuse, which may lead to risks to the health and safety or fundamentalrights should be included in the instructions for use that are provided by the provider. This is to ensure that thedeployer is aware and takes them into account when using the high-risk AI system. Identifying and implementingrisk mitigation measures for foreseeable misuse under this Regulation should not require specific additional trainingfor the high-risk AI system by the provider to address foreseeable misuse. The providers however are encouraged toconsider such additional training measures to mitigate reasonable foreseeable misuses as necessary and appropriate.(66) Requirements should apply to high-risk AI systems as regards risk management, the quality and relevance of datasets used, technical documentation and record-keeping, transparency and the provision of information to deployers, human oversight, and robustness, accuracy and cybersecurity. Those requirements are necessary to effectivelymitigate the risks for health, safety and fundamental rights. As no other less trade restrictive measures are reasonablyavailable those requirements are not unjustified restrictions to trade.(67) High-quality data and access to high-quality data plays a vital role in providing structure and in ensuring theperformance of many AI systems, especially when techniques involving the training of models are used, with a viewto ensure that the high-risk AI system performs as intended and safely and it does not become a source ofdiscrimination prohibited by Union law. High-quality data sets for training, validation and testing require theimplementation of appropriate data governance and management practices. Data sets for training, validation andtesting, including the labels, should be relevant, sufficiently representative, and to the best extent possible free oferrors and complete in view of the intended purpose of the system. In order to facilitate compliance with Union dataprotection law, such as Regulation (EU) 2016/679, data governance and management practices should include, inthe case of personal data, transparency about the original purpose of the data collection. The data sets should alsohave the appropriate statistical properties, including as regards the persons or groups of persons in relation to whomthe high-risk AI system is intended to be used, with specific attention to the mitigation of possible biases in the data(feedback loops). Biases can for example be inherent in underlying data sets, especially when historical data is beingused, or generated when the systems are implemented in real world settings. Results provided by AI systems could beinfluenced by such inherent biases that are inclined to gradually increase and thereby perpetuate and amplify existingdiscrimination, in particular for persons belonging to certain vulnerable groups, including racial or ethnic groups. The requirement for the data sets to be to the best extent possible complete and free of errors should not affect theuse of privacy-preserving techniques in the context of the development and testing of AI systems. In particular, datasets should take into account, to the extent required by their intended purpose, the features, characteristics orelements that are particular to the specific geographical, contextual, behavioural or functional setting which the AIsystem is intended to be used. The requirements related to data governance can be complied with by having recourseto third parties that offer certified compliance services including verification of data governance, data set integrity, and data training, validation and testing practices, as far as compliance with the data requirements of this Regulationare ensured.(68) For the development and assessment of high-risk AI systems, certain actors, such as providers, notified bodies andother relevant entities, such as European Digital Innovation Hubs, testing experimentation facilities and researchers, should be able to access and use high-quality data sets within the fields of activities of those actors which are relatedto this Regulation. European common data spaces established by the Commission and the facilitation of data sharingbetween businesses and with government in the public interest will be instrumental to provide trustful, accountableand non-discriminatory access to high-quality data for the training, validation and testing of AI systems. Forexample, in health, the European health data space will facilitate non-discriminatory access to health data and thetraining of AI algorithms on those data sets, in a privacy-preserving, secure, timely, transparent and trustworthymanner, and with an appropriate institutional governance. Relevant competent authorities, including sectoral ones, providing or supporting the access to data may also support the provision of high-quality data for the training, validation and testing of AI systems.(69) The right to privacy and to protection of personal data must be guaranteed throughout the entire lifecycle of the AIsystem. In this regard, the principles of data minimisation and data protection by design and by default, as set out in Union data protection law, are applicable when personal data are processed. Measures taken by providers to ensurecompliance with those principles may include not only anonymisation and encryption, but also the use oftechnology that permits algorithms to be brought to the data and allows training of AI systems without thetransmission between parties or copying of the raw or structured data themselves, without prejudice to therequirements on data governance provided for in this Regulation.(70) In order to protect the right of others from the discrimination that might result from the bias in AI systems, theproviders should, exceptionally, to the extent that it is strictly necessary for the purpose of ensuring bias detectionand correction in relation to the high-risk AI systems, subject to appropriate safeguards for the fundamental rightsand freedoms of natural persons and following the application of all applicable conditions laid down under this Regulation in addition to the conditions laid down in Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive(EU) 2016/680, be able to process also special categories of personal data, as a matter of substantial public interestwithin the meaning of Article 9(2), point (g) of Regulation (EU) 2016/679 and Article 10(2), point (g) of Regulation(EU) 2018/1725.(71) Having comprehensible information on how high-risk AI systems have been developed and how they performthroughout their lifetime is essential to enable traceability of those systems, verify compliance with the requirementsunder this Regulation, as well as monitoring of their operations and post market monitoring. This requires keepingrecords and the availability of technical documentation, containing information which is necessary to assess thecompliance of the AI system with the relevant requirements and facilitate post market monitoring. Such informationshould include the general characteristics, capabilities and limitations of the system, algorithms, data, training, testing and validation processes used as well as documentation on the relevant risk-management system and drawnin a clear and comprehensive form. The technical documentation should be kept up to date, appropriately(72) To address concerns related to opacity and complexity of certain AI systems and help deployers to fulfil theirobligations under this Regulation, transparency should be required for high-risk AI systems before they are placedon the market or put it into service. High-risk AI systems should be designed in a manner to enable deployers tounderstand how the AI system works, evaluate its functionality, and comprehend its strengths and limitations. High-risk AI systems should be accompanied by appropriate information in the form of instructions of use. Suchinformation should include the characteristics, capabilities and limitations of performance of the AI system. Thosewould cover information on possible known and foreseeable circumstances related to the use of the high-risk AIsystem, including deployer action that may influence system behaviour and performance, under which the AI systemcan lead to risks to health, safety, and fundamental rights, on the changes that have been pre-determined andassessed for conformity by the provider and on the relevant human oversight measures, including the measures tofacilitate the interpretation of the outputs of the AI system by the deployers. Transparency, including theaccompanying instructions for use, should assist deployers in the use of the system and support informed decisionmaking by them. Deployers should, inter alia, be in a better position to make the correct choice of the system thatthey intend to use in light of the obligations applicable to them, be educated about the intended and precluded uses, and use the AI system correctly and as appropriate. In order to enhance legibility and accessibility of the informationincluded in the instructions of use, where appropriate, illustrative examples, for instance on the limitations and onthe intended and precluded uses of the AI system, should be included. Providers should ensure that alldocumentation, including the instructions for use, contains meaningful, comprehensive, accessible andunderstandable information, taking into account the needs and foreseeable knowledge of the target deployers. Instructions for use should be made available in a language which can be easily understood by target deployers, asdetermined by the Member State concerned.(73) High-risk AI systems should be designed and developed in such a way that natural persons can oversee theirfunctioning, ensure that they are used as intended and that their impacts are addressed over the system’s lifecycle. Tothat end, appropriate human oversight measures should be identified by the provider of the system before its placingon the market or putting into service. In particular, where appropriate, such measures should guarantee that thesystem is subject to in-built operational constraints that cannot be overridden by the system itself and is responsiveto the human operator, and that the natural persons to whom human oversight has been assigned have the necessarycompetence, training and authority to carry out that role. It is also essential, as appropriate, to ensure that high-risk AI systems include mechanisms to guide and inform a natural person to whom human oversight has been assignedto make informed decisions if, when and how to intervene in order to avoid negative consequences or risks, or stopthe system if it does not perform as intended. Considering the significant consequences for persons in the case of anincorrect match by certain biometric identification systems, it is appropriate to provide for an enhanced humanoversight requirement for those systems so that no action or decision may be taken by the deployer on the basis ofthe identification resulting from the system unless this has been separately verified and confirmed by at least twonatural persons. Those persons could be from one or more entities and include the person operating or using thesystem. This requirement should not pose unnecessary burden or delays and it could be sufficient that the separateverifications by the different persons are automatically recorded in the logs generated by the system. Given thespecificities of the areas of law enforcement, migration, border control and asylum, this requirement should notapply where Union or national law considers the application of that requirement to be disproportionate.(74) High-risk AI systems should perform consistently throughout their lifecycle and meet an appropriate level ofaccuracy, robustness and cybersecurity, in light of their intended purpose and in accordance with the generallyacknowledged state of the art. The Commission and relevant organisations and stakeholders are encouraged to takedue consideration of the mitigation of risks and the negative impacts of the AI system. The expected level ofperformance metrics should be declared in the accompanying instructions of use. Providers are urged tocommunicate that information to deployers in a clear and easily understandable way, free of misunderstandings ormisleading statements. Union law on legal metrology, including Directives 2014/31/EU (35) and 2014/32/EU (36) ofthe European Parliament and of the Council, aims to ensure the accuracy of measurements and to help thetransparency and fairness of commercial transactions. In that context, in cooperation with relevant stakeholders andorganisation, such as metrology and benchmarking authorities, the Commission should encourage, as appropriate, the development of benchmarks and measurement methodologies for AI systems. In doing so, the Commissionshould take note and collaborate with international partners working on metrology and relevant measurementindicators relating to AI.(35) Directive 2014/31/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation of the laws of the Member States relating to the making available on the market of non-automatic weighing instruments (OJ L 96, 29.3.2014, p. 107).(75) Technical robustness is a key requirement for high-risk AI systems. They should be resilient in relation to harmful orotherwise undesirable behaviour that may result from limitations within the systems or the environment in whichthe systems operate (e. g. errors, faults, inconsistencies, unexpected situations). Therefore, technical andorganisational measures should be taken to ensure robustness of high-risk AI systems, for example by designingand developing appropriate technical solutions to prevent or minimise harmful or otherwise undesirable behaviour. Those technical solution may include for instance mechanisms enabling the system to safely interrupt its operation(fail-safe plans) in the presence of certain anomalies or when operation takes place outside certain predeterminedboundaries. Failure to protect against these risks could lead to safety impacts or negatively affect the fundamentalrights, for example due to erroneous decisions or wrong or biased outputs generated by the AI system.(76) Cybersecurity plays a crucial role in ensuring that AI systems are resilient against attempts to alter their use, behaviour, performance or compromise their security properties by malicious third parties exploiting the system’svulnerabilities. Cyberattacks against AI systems can leverage AI specific assets, such as training data sets (e. g. datapoisoning) or trained models (e. g. adversarial attacks or membership inference), or exploit vulnerabilities in the AIsystem’s digital assets or the underlying ICT infrastructure. To ensure a level of cybersecurity appropriate to the risks, suitable measures, such as security controls, should therefore be taken by the providers of high-risk AI systems, alsotaking into account as appropriate the underlying ICT infrastructure.(77) Without prejudice to the requirements related to robustness and accuracy set out in this Regulation, high-risk AIsystems which fall within the scope of a regulation of the European Parliament and of the Council on horizontalcybersecurity requirements for products with digital elements, in accordance with that regulation may demonstratecompliance with the cybersecurity requirements of this Regulation by fulfilling the essential cybersecurityrequirements set out in that regulation. When high-risk AI systems fulfil the essential requirements of a regulation ofthe European Parliament and of the Council on horizontal cybersecurity requirements for products with digitalelements, they should be deemed compliant with the cybersecurity requirements set out in this Regulation in so faras the achievement of those requirements is demonstrated in the EU declaration of conformity or parts thereofissued under that regulation. To that end, the assessment of the cybersecurity risks, associated to a product withdigital elements classified as high-risk AI system according to this Regulation, carried out under a regulation of the European Parliament and of the Council on horizontal cybersecurity requirements for products with digitalelements, should consider risks to the cyber resilience of an AI system as regards attempts by unauthorised thirdparties to alter its use, behaviour or performance, including AI specific vulnerabilities such as data poisoning oradversarial attacks, as well as, as relevant, risks to fundamental rights as required by this Regulation.(78) The conformity assessment procedure provided by this Regulation should apply in relation to the essentialcybersecurity requirements of a product with digital elements covered by a regulation of the European Parliamentand of the Council on horizontal cybersecurity requirements for products with digital elements and classified asa high-risk AI system under this Regulation. However, this rule should not result in reducing the necessary level ofassurance for critical products with digital elements covered by a regulation of the European Parliament and of the Council on horizontal cybersecurity requirements for products with digital elements. Therefore, by way ofderogation from this rule, high-risk AI systems that fall within the scope of this Regulation and are also qualified asimportant and critical products with digital elements pursuant to a regulation of the European Parliament and of the Council on horizontal cybersecurity requirements for products with digital elements and to which the conformityassessment procedure based on internal control set out in an annex to this Regulation applies, are subject to theconformity assessment provisions of a regulation of the European Parliament and of the Council on horizontalcybersecurity requirements for products with digital elements insofar as the essential cybersecurity requirements ofthat regulation are concerned. In this case, for all the other aspects covered by this Regulation the respectiveprovisions on conformity assessment based on internal control set out in an annex to this Regulation should apply. Building on the knowledge and expertise of ENISA on the cybersecurity policy and tasks assigned to ENISA underthe Regulation (EU) 2019/881 of the European Parliament and of the Council (37), the Commission should cooperatewith ENISA on issues related to cybersecurity of AI systems.(37) Regulation (EU) 2019/881 of the European Parliament and of the Council of 17 April 2019 on ENISA (the European Union Agency(79) It is appropriate that a specific natural or legal person, defined as the provider, takes responsibility for the placing onthe market or the putting into service of a high-risk AI system, regardless of whether that natural or legal person isthe person who designed or developed the system.(80) As signatories to the United Nations Convention on the Rights of Persons with Disabilities, the Union and the Member States are legally obliged to protect persons with disabilities from discrimination and promote their equality, to ensure that persons with disabilities have access, on an equal basis with others, to information andcommunications technologies and systems, and to ensure respect for privacy for persons with disabilities. Given thegrowing importance and use of AI systems, the application of universal design principles to all new technologies andservices should ensure full and equal access for everyone potentially affected by or using AI technologies, includingpersons with disabilities, in a way that takes full account of their inherent dignity and diversity. It is thereforeessential that providers ensure full compliance with accessibility requirements, including Directive (EU) 2016/2102of the European Parliament and of the Council (38) and Directive (EU) 2019/882. Providers should ensurecompliance with these requirements by design. Therefore, the necessary measures should be integrated as much aspossible into the design of the high-risk AI system.(81) The provider should establish a sound quality management system, ensure the accomplishment of the requiredconformity assessment procedure, draw up the relevant documentation and establish a robust post-marketmonitoring system. Providers of high-risk AI systems that are subject to obligations regarding quality managementsystems under relevant sectoral Union law should have the possibility to include the elements of the qualitymanagement system provided for in this Regulation as part of the existing quality management system provided forin that other sectoral Union law. The complementarity between this Regulation and existing sectoral Union lawshould also be taken into account in future standardisation activities or guidance adopted by the Commission. Publicauthorities which put into service high-risk AI systems for their own use may adopt and implement the rules for thequality management system as part of the quality management system adopted at a national or regional level, asappropriate, taking into account the specificities of the sector and the competences and organisation of the publicauthority concerned.(82) To enable enforcement of this Regulation and create a level playing field for operators, and, taking into account thedifferent forms of making available of digital products, it is important to ensure that, under all circumstances, a person established in the Union can provide authorities with all the necessary information on the compliance of an AI system. Therefore, prior to making their AI systems available in the Union, providers established in thirdcountries should, by written mandate, appoint an authorised representative established in the Union. This authorisedrepresentative plays a pivotal role in ensuring the compliance of the high-risk AI systems placed on the market orput into service in the Union by those providers who are not established in the Union and in serving as their contactperson established in the Union.(83) In light of the nature and complexity of the value chain for AI systems and in line with the New Legislative Framework, it is essential to ensure legal certainty and facilitate the compliance with this Regulation. Therefore, it isnecessary to clarify the role and the specific obligations of relevant operators along that value chain, such asimporters and distributors who may contribute to the development of AI systems. In certain situations thoseoperators could act in more than one role at the same time and should therefore fulfil cumulatively all relevantobligations associated with those roles. For example, an operator could act as a distributor and an importer at thesame time.(84) To ensure legal certainty, it is necessary to clarify that, under certain specific conditions, any distributor, importer, deployer or other third-party should be considered to be a provider of a high-risk AI system and therefore assume allthe relevant obligations. This would be the case if that party puts its name or trademark on a high-risk AI systemalready placed on the market or put into service, without prejudice to contractual arrangements stipulating that theobligations are allocated otherwise. This would also be the case if that party makes a substantial modification toa high-risk AI system that has already been placed on the market or has already been put into service in a way that itremains a high-risk AI system in accordance with this Regulation, or if it modifies the intended purpose of an AIsystem, including a general-purpose AI system, which has not been classified as high-risk and has already beenplaced on the market or put into service, in a way that the AI system becomes a high-risk AI system in accordancewith this Regulation. Those provisions should apply without prejudice to more specific provisions established incertain Union harmonisation legislation based on the New Legislative Framework, together with which this Regulation should apply. For example, Article 16(2) of Regulation (EU) 2017/745, establishing that certain changesshould not be considered to be modifications of a device that could affect its compliance with the applicablerequirements, should continue to apply to high-risk AI systems that are medical devices within the meaning of that Regulation.(85) General-purpose AI systems may be used as high-risk AI systems by themselves or be components of other high-risk AI systems. Therefore, due to their particular nature and in order to ensure a fair sharing of responsibilities along the AI value chain, the providers of such systems should, irrespective of whether they may be used as high-risk AIsystems as such by other providers or as components of high-risk AI systems and unless provided otherwise underthis Regulation, closely cooperate with the providers of the relevant high-risk AI systems to enable their compliancewith the relevant obligations under this Regulation and with the competent authorities established under this Regulation.(86) Where, under the conditions laid down in this Regulation, the provider that initially placed the AI system on themarket or put it into service should no longer be considered to be the provider for the purposes of this Regulation, and when that provider has not expressly excluded the change of the AI system into a high-risk AI system, theformer provider should nonetheless closely cooperate and make available the necessary information and provide thereasonably expected technical access and other assistance that are required for the fulfilment of the obligations setout in this Regulation, in particular regarding the compliance with the conformity assessment of high-risk AIsystems.(87) In addition, where a high-risk AI system that is a safety component of a product which falls within the scope of Union harmonisation legislation based on the New Legislative Framework is not placed on the market or put intoservice independently from the product, the product manufacturer defined in that legislation should comply withthe obligations of the provider established in this Regulation and should, in particular, ensure that the AI systemembedded in the final product complies with the requirements of this Regulation.(88) Along the AI value chain multiple parties often supply AI systems, tools and services but also components orprocesses that are incorporated by the provider into the AI system with various objectives, including the modeltraining, model retraining, model testing and evaluation, integration into software, or other aspects of modeldevelopment. Those parties have an important role to play in the value chain towards the provider of the high-risk AI system into which their AI systems, tools, services, components or processes are integrated, and should provideby written agreement this provider with the necessary information, capabilities, technical access and other assistancebased on the generally acknowledged state of the art, in order to enable the provider to fully comply with theobligations set out in this Regulation, without compromising their own intellectual property rights or trade secrets.(89) Third parties making accessible to the public tools, services, processes, or AI components other thangeneral-purpose AI models, should not be mandated to comply with requirements targeting the responsibilitiesalong the AI value chain, in particular towards the provider that has used or integrated them, when those tools, services, processes, or AI components are made accessible under a free and open-source licence. Developers of freeand open-source tools, services, processes, or AI components other than general-purpose AI models should beencouraged to implement widely adopted documentation practices, such as model cards and data sheets, as a way toaccelerate information sharing along the AI value chain, allowing the promotion of trustworthy AI systems in the Union.(90) The Commission could develop and recommend voluntary model contractual terms between providers of high-risk AI systems and third parties that supply tools, services, components or processes that are used or integrated inhigh-risk AI systems, to facilitate the cooperation along the value chain. When developing voluntary modelcontractual terms, the Commission should also take into account possible contractual requirements applicable inspecific sectors or business cases.(91) Given the nature of AI systems and the risks to safety and fundamental rights possibly associated with their use, including as regards the need to ensure proper monitoring of the performance of an AI system in a real-life setting, itis appropriate to set specific responsibilities for deployers. Deployers should in particular take appropriate technicaland organisational measures to ensure they use high-risk AI systems in accordance with the instructions of use andcertain other obligations should be provided for with regard to monitoring of the functioning of the AI systems andcompetence, in particular an adequate level of AI literacy, training and authority to properly fulfil those tasks. Thoseobligations should be without prejudice to other deployer obligations in relation to high-risk AI systems under Union or national law.(92) This Regulation is without prejudice to obligations for employers to inform or to inform and consult workers ortheir representatives under Union or national law and practice, including Directive 2002/14/EC of the European Parliament and of the Council (39), on decisions to put into service or use AI systems. It remains necessary to ensureinformation of workers and their representatives on the planned deployment of high-risk AI systems at theworkplace where the conditions for those information or information and consultation obligations in other legalinstruments are not fulfilled. Moreover, such information right is ancillary and necessary to the objective ofprotecting fundamental rights that underlies this Regulation. Therefore, an information requirement to that effectshould be laid down in this Regulation, without affecting any existing rights of workers.(93) Whilst risks related to AI systems can result from the way such systems are designed, risks can as well stem fromhow such AI systems are used. Deployers of high-risk AI system therefore play a critical role in ensuring thatfundamental rights are protected, complementing the obligations of the provider when developing the AI system. Deployers are best placed to understand how the high-risk AI system will be used concretely and can thereforeidentify potential significant risks that were not foreseen in the development phase, due to a more precise knowledgeof the context of use, the persons or groups of persons likely to be affected, including vulnerable groups. Deployersof high-risk AI systems listed in an annex to this Regulation also play a critical role in informing natural persons andshould, when they make decisions or assist in making decisions related to natural persons, where applicable, informthe natural persons that they are subject to the use of the high-risk AI system. This information should include theintended purpose and the type of decisions it makes. The deployershould also inform the natural persons abouttheir right to an explanation provided under this Regulation. With regard to high-risk AI systems used for lawenforcement purposes, that obligation should be implemented in accordance with Article 13 of Directive (EU)2016/680.(94) Any processing of biometric data involved in the use of AI systems for biometric identification for the purpose oflaw enforcement needs to comply with Article 10 of Directive (EU) 2016/680, that allows such processing onlywhere strictly necessary, subject to appropriate safeguards for the rights and freedoms of the data subject, and whereauthorised by Union or Member State law. Such use, when authorised, also needs to respect the principles laid downin Article 4 (1) of Directive (EU) 2016/680 including lawfulness, fairness and transparency, purpose limitation, accuracy and storage limitation.(95) Without prejudice to applicable Union law, in particular Regulation (EU) 2016/679 and Directive (EU) 2016/680, considering the intrusive nature of post-remote biometric identification systems, the use of post-remote biometricidentification systems should be subject to safeguards. Post-remote biometric identification systems should always beused in a way that is proportionate, legitimate and strictly necessary, and thus targeted, in terms of the individuals tobe identified, the location, temporal scope and based on a closed data set of legally acquired video footage. In anycase, post-remote biometric identification systems should not be used in the framework of law enforcement to leadto indiscriminate surveillance. The conditions for post-remote biometric identification should in any case notprovide a basis to circumvent the conditions of the prohibition and strict exceptions for real time remote biometricidentification.(96) In order to efficiently ensure that fundamental rights are protected, deployers of high-risk AI systems that are bodiesgoverned by public law, or private entities providing public services and deployers of certain high-risk AI systemslisted in an annex to this Regulation, such as banking or insurance entities, should carry out a fundamental rightsimpact assessment prior to putting it into use. Services important for individuals that are of public nature may alsobe provided by private entities. Private entities providing such public services are linked to tasks in the public interestsuch as in the areas of education, healthcare, social services, housing, administration of justice. The aim of thefundamental rights impact assessment is for the deployer to identify the specific risks to the rights of individuals orgroups of individuals likely to be affected, identify measures to be taken in the case of a materialisation of those risks. The impact assessment should be performed prior to deploying the high-risk AI system, and should be updatedwhen the deployer considers that any of the relevant factors have changed. The impact assessment should identifythe deployer’s relevant processes in which the high-risk AI system will be used in line with its intended purpose, andshould include a description of the period of time and frequency in which the system is intended to be used as wellas of specific categories of natural persons and groups who are likely to be affected in the specific context of use. Theassessment should also include the identification of specific risks of harm likely to have an impact on thefundamental rights of those persons or groups. While performing this assessment, the deployershould take intoaccount information relevant to a proper assessment of the impact, including but not limited to the informationgiven by the provider of the high-risk AI system in the instructions for use. In light of the risks identified, deployersshould determine measures to be taken in the case of a materialisation of those risks, including for examplegovernance arrangements in that specific context of use, such as arrangements for human oversight according to theinstructions of use or, complaint handling and redress procedures, as they could be instrumental in mitigating risksto fundamental rights in concrete use-cases. After performing that impact assessment, the deployershould notify therelevant market surveillance authority. Where appropriate, to collect relevant information necessary to perform theimpact assessment, deployers of high-risk AI system, in particular when AI systems are used in the public sector, could involve relevant stakeholders, including the representatives of groups of persons likely to be affected by the AIsystem, independent experts, and civil society organisations in conducting such impact assessments and designingmeasures to be taken in the case of materialisation of the risks. The European Artificial Intelligence Office (AI Office)should develop a template for a questionnaire in order to facilitate compliance and reduce the administrative burdenfor deployers.(97) The notion of general-purpose AI models should be clearly defined and set apart from the notion of AI systems toenable legal certainty. The definition should be based on the key functional characteristics of a general-purpose AImodel, in particular the generality and the capability to competently perform a wide range of distinct tasks. Thesemodels are typically trained on large amounts of data, through various methods, such as self-supervised, unsupervised or reinforcement learning. General-purpose AI models may be placed on the market in various ways, including through libraries, application programming interfaces (APIs), as direct download, or as physical copy. These models may be further modified or fine-tuned into new models. Although AI models are essentialcomponents of AI systems, they do not constitute AI systems on their own. AI models require the addition of furthercomponents, such as for example a user interface, to become AI systems. AI models are typically integrated into andform part of AI systems. This Regulation provides specific rules for general-purpose AI models and forgeneral-purpose AI models that pose systemic risks, which should apply also when these models are integrated orform part of an AI system. It should be understood that the obligations for the providers of general-purpose AImodels should apply once the general-purpose AI models are placed on the market. When the provider ofa general-purpose AI model integrates an own model into its own AI system that is made available on the market orput into service, that model should be considered to be placed on the market and, therefore, the obligations in this Regulation for models should continue to apply in addition to those for AI systems. The obligations laid down formodels should in any case not apply when an own model is used for purely internal processes that are not essentialfor providing a product or a service to third parties and the rights of natural persons are not affected. Consideringtheir potential significantly negative effects, the general-purpose AI models with systemic risk should always besubject to the relevant obligations under this Regulation. The definition should not cover AI models used before theirplacing on the market for the sole purpose of research, development and prototyping activities. This is withoutprejudice to the obligation to comply with this Regulation when, following such activities, a model is placed on themarket.(98) Whereas the generality of a model could, inter alia, also be determined by a number of parameters, models with atleast a billion of parameters and trained with a large amount of data using self-supervision at scale should beconsidered to display significant generality and to competently perform a wide range of distinctive tasks.(99) Large generative AI models are a typical example for a general-purpose AI model, given that they allow for flexiblegeneration of content, such as in the form of text, audio, images or video, that can readily accommodate a widerange of distinctive tasks.(100) When a general-purpose AI model is integrated into or forms part of an AI system, this system should be considered(101) Providers of general-purpose AI models have a particular role and responsibility along the AI value chain, as themodels they provide may form the basis for a range of downstream systems, often provided by downstreamproviders that necessitate a good understanding of the models and their capabilities, both to enable the integration ofsuch models into their products, and to fulfil their obligations under this or other regulations. Therefore, proportionate transparency measures should be laid down, including the drawing up and keeping up to date ofdocumentation, and the provision of information on the general-purpose AI model for its usage by the downstreamproviders. Technical documentation should be prepared and kept up to date by the general-purpose AI modelprovider for the purpose of making it available, upon request, to the AI Office and the national competentauthorities. The minimal set of elements to be included in such documentation should be set out in specific annexesto this Regulation. The Commission should be empowered to amend those annexes by means of delegated acts inlight of evolving technological developments.(102) Software and data, including models, released under a free and open-source licence that allows them to be openlyshared and where users can freely access, use, modify and redistribute them or modified versions thereof, cancontribute to research and innovation in the market and can provide significant growth opportunities for the Unioneconomy. General-purpose AI models released under free and open-source licences should be considered to ensurehigh levels of transparency and openness if their parameters, including the weights, the information on the modelarchitecture, and the information on model usage are made publicly available. The licence should be considered to befree and open-source also when it allows users to run, copy, distribute, study, change and improve software and data, including models under the condition that the original provider of the model is credited, the identical or comparableterms of distribution are respected.(103) Free and open-source AI components covers the software and data, including models and general-purpose AImodels, tools, services or processes of an AI system. Free and open-source AI components can be provided throughdifferent channels, including their development on open repositories. For the purposes of this Regulation, AIcomponents that are provided against a price or otherwise monetised, including through the provision of technicalsupport or other services, including through a software platform, related to the AI component, or the use ofpersonal data for reasons other than exclusively for improving the security, compatibility or interoperability of thesoftware, with the exception of transactions between microenterprises, should not benefit from the exceptionsprovided to free and open-source AI components. The fact of making AI components available through openrepositories should not, in itself, constitute a monetisation.(104) The providers of general-purpose AI models that are released under a free and open-source licence, and whoseparameters, including the weights, the information on the model architecture, and the information on model usage, are made publicly available should be subject to exceptions as regards the transparency-related requirementsimposed on general-purpose AI models, unless they can be considered to present a systemic risk, in which case thecircumstance that the model is transparent and accompanied by an open-source license should not be considered tobe a sufficient reason to exclude compliance with the obligations under this Regulation. In any case, given that therelease of general-purpose AI models under free and open-source licence does not necessarily reveal substantialinformation on the data set used for the training or fine-tuning of the model and on how compliance of copyrightlaw was thereby ensured, the exception provided for general-purpose AI models from compliance with thetransparency-related requirements should not concern the obligation to produce a summary about the content usedfor model training and the obligation to put in place a policy to comply with Union copyright law, in particular toidentify and comply with the reservation of rights pursuant to Article 4(3) of Directive (EU) 2019/790 of the European Parliament and of the Council (40).(105) General-purpose AI models, in particular large generative AI models, capable of generating text, images, and othercontent, present unique innovation opportunities but also challenges to artists, authors, and other creators and theway their creative content is created, distributed, used and consumed. The development and training of such modelsrequire access to vast amounts of text, images, videos and other data. Text and data mining techniques may be usedextensively in this context for the retrieval and analysis of such content, which may be protected by copyright andrelated rights. Any use of copyright protected content requires the authorisation of the rightsholder concernedunless relevant copyright exceptions and limitations apply. Directive (EU) 2019/790 introduced exceptions andlimitations allowing reproductions and extractions of works or other subject matter, for the purpose of text and datamining, under certain conditions. Under these rules, rightsholders may choose to reserve their rights over theirworks or other subject matter to prevent text and data mining, unless this is done for the purposes of scientificresearch. Where the rights to opt out has been expressly reserved in an appropriate manner, providers ofgeneral-purpose AI models need to obtain an authorisation from rightsholders if they want to carry out text anddata mining over such works.(106) Providers that place general-purpose AI models on the Union market should ensure compliance with the relevantobligations in this Regulation. To that end, providers of general-purpose AI models should put in place a policy tocomply with Union law on copyright and related rights, in particular to identify and comply with the reservation ofrights expressed by rightsholders pursuant to Article 4(3) of Directive (EU) 2019/790. Any provider placinga general-purpose AI model on the Union market should comply with this obligation, regardless of the jurisdictionin which the copyright-relevant acts underpinning the training of those general-purpose AI models take place. Thisis necessary to ensure a level playing field among providers of general-purpose AI models where no provider shouldbe able to gain a competitive advantage in the Union market by applying lower copyright standards than thoseprovided in the Union.(107) In order to increase transparency on the data that is used in the pre-training and training of general-purpose AImodels, including text and data protected by copyright law, it is adequate that providers of such models draw up andmake publicly available a sufficiently detailed summary of the content used for training the general-purpose AImodel. While taking into due account the need to protect trade secrets and confidential business information, thissummary should be generally comprehensive in its scope instead of technically detailed to facilitate parties withlegitimate interests, including copyright holders, to exercise and enforce their rights under Union law, for example bylisting the main data collections or sets that went into training the model, such as large private or public databases ordata archives, and by providing a narrative explanation about other data sources used. It is appropriate for the AIOffice to provide a template for the summary, which should be simple, effective, and allow the provider to providethe required summary in narrative form.(108) With regard to the obligations imposed on providers of general-purpose AI models to put in place a policy tocomply with Union copyright law and make publicly available a summary of the content used for the training, the AIOffice should monitor whether the provider has fulfilled those obligations without verifying or proceeding toa work-by-work assessment of the training data in terms of copyright compliance. This Regulation does not affectthe enforcement of copyright rules as provided for under Union law.(109) Compliance with the obligations applicable to the providers of general-purpose AI models should be commensurateand proportionate to the type of model provider, excluding the need for compliance for persons who develop or usemodels for non-professional or scientific research purposes, who should nevertheless be encouraged to voluntarilycomply with these requirements. Without prejudice to Union copyright law, compliance with those obligationsshould take due account of the size of the provider and allow simplified ways of compliance for SMEs, includingstart-ups, that should not represent an excessive cost and not discourage the use of such models. In the case ofa modification or fine-tuning of a model, the obligations for providers of general-purpose AI models should belimited to that modification or fine-tuning, for example by complementing the already existing technicaldocumentation with information on the modifications, including new training data sources, as a means to complywith the value chain obligations provided in this Regulation.(110) General-purpose AI models could pose systemic risks which include, but are not limited to, any actual or reasonablyforeseeable negative effects in relation to major accidents, disruptions of critical sectors and serious consequences topublic health and safety; any actual or reasonably foreseeable negative effects on democratic processes, public andeconomic security; the dissemination of illegal, false, or discriminatory content. Systemic risks should be understoodthe model, its access to tools, novel or combined modalities, release and distribution strategies, the potential toremove guardrails and other factors. In particular, international approaches have so far identified the need to payattention to risks from potential intentional misuse or unintended issues of control relating to alignment withhuman intent; chemical, biological, radiological, and nuclear risks, such as the ways in which barriers to entry can belowered, including for weapons development, design acquisition, or use; offensive cyber capabilities, such as theways in vulnerability discovery, exploitation, or operational use can be enabled; the effects of interaction and tooluse, including for example the capacity to control physical systems and interfere with critical infrastructure; risksfrom models of making copies of themselves or ‘self-replicating’ or training other models; the ways in which modelscan give rise to harmful bias and discrimination with risks to individuals, communities or societies; the facilitation ofdisinformation or harming privacy with threats to democratic values and human rights; risk that a particular eventcould lead to a chain reaction with considerable negative effects that could affect up to an entire city, an entiredomain activity or an entire community.(111) It is appropriate to establish a methodology for the classification of general-purpose AI models as general-purpose AI model with systemic risks. Since systemic risks result from particularly high capabilities, a general-purpose AImodel should be considered to present systemic risks if it has high-impact capabilities, evaluated on the basis ofappropriate technical tools and methodologies, or significant impact on the internal market due to its reach. High-impact capabilities in general-purpose AI models means capabilities that match or exceed the capabilitiesrecorded in the most advanced general-purpose AI models. The full range of capabilities in a model could be betterunderstood after its placing on the market or when deployers interact with the model. According to the state of theart at the time of entry into force of this Regulation, the cumulative amount of computation used for the training ofthe general-purpose AI model measured in floating point operations is one of the relevant approximations for modelcapabilities. The cumulative amount of computation used for training includes the computation used across theactivities and methods that are intended to enhance the capabilities of the model prior to deployment, such aspre-training, synthetic data generation and fine-tuning. Therefore, an initial threshold of floating point operationsshould be set, which, if met by a general-purpose AI model, leads to a presumption that the model isa general-purpose AI model with systemic risks. This threshold should be adjusted over time to reflect technologicaland industrial changes, such as algorithmic improvements or increased hardware efficiency, and should besupplemented with benchmarks and indicators for model capability. To inform this, the AI Office should engagewith the scientific community, industry, civil society and other experts. Thresholds, as well as tools and benchmarksfor the assessment of high-impact capabilities, should be strong predictors of generality, its capabilities andassociated systemic risk of general-purpose AI models, and could take into account the way the model will be placedon the market or the number of users it may affect. To complement this system, there should be a possibility for the Commission to take individual decisions designating a general-purpose AI model as a general-purpose AI modelwith systemic risk if it is found that such model has capabilities or an impact equivalent to those captured by the setthreshold. That decision should be taken on the basis of an overall assessment of the criteria for the designation ofa general-purpose AI model with systemic risk set out in an annex to this Regulation, such as quality or size of thetraining data set, number of business and end users, its input and output modalities, its level of autonomy andscalability, or the tools it has access to. Upon a reasoned request of a provider whose model has been designated asa general-purpose AI model with systemic risk, the Commission should take the request into account and maydecide to reassess whether the general-purpose AI model can still be considered to present systemic risks.(112) It is also necessary to clarify a procedure for the classification of a general-purpose AI model with systemic risks. A general-purpose AI model that meets the applicable threshold for high-impact capabilities should be presumed tobe a general-purpose AI models with systemic risk. The provider should notify the AI Office at the latest two weeksafter the requirements are met or it becomes known that a general-purpose AI model will meet the requirementsthat lead to the presumption. This is especially relevant in relation to the threshold of floating point operationsbecause training of general-purpose AI models takes considerable planning which includes the upfront allocation ofcompute resources and, therefore, providers of general-purpose AI models are able to know if their model wouldmeet the threshold before the training is completed. In the context of that notification, the provider should be able todemonstrate that, because of its specific characteristics, a general-purpose AI model exceptionally does not presentsystemic risks, and that it thus should not be classified as a general-purpose AI model with systemic risks. Thatimportant with regard to general-purpose AI models that are planned to be released as open-source, given that, afterthe open-source model release, necessary measures to ensure compliance with the obligations under this Regulationmay be more difficult to implement.(113) If the Commission becomes aware of the fact that a general-purpose AI model meets the requirements to classify asa general-purpose AI model with systemic risk, which previously had either not been known or of which therelevant provider has failed to notify the Commission, the Commission should be empowered to designate it so. A system of qualified alerts should ensure that the AI Office is made aware by the scientific panel of general-purpose AI models that should possibly be classified as general-purpose AI models with systemic risk, in addition to themonitoring activities of the AI Office.(114) The providers of general-purpose AI models presenting systemic risks should be subject, in addition to theobligations provided for providers of general-purpose AI models, to obligations aimed at identifying and mitigatingthose risks and ensuring an adequate level of cybersecurity protection, regardless of whether it is provided asa standalone model or embedded in an AI system or a product. To achieve those objectives, this Regulation shouldrequire providers to perform the necessary model evaluations, in particular prior to its first placing on the market, including conducting and documenting adversarial testing of models, also, as appropriate, through internal orindependent external testing. In addition, providers of general-purpose AI models with systemic risks shouldcontinuously assess and mitigate systemic risks, including for example by putting in place risk-management policies, such as accountability and governance processes, implementing post-market monitoring, taking appropriatemeasures along the entire model’s lifecycle and cooperating with relevant actors along the AI value chain.(115) Providers of general-purpose AI models with systemic risks should assess and mitigate possible systemic risks. If, despite efforts to identify and prevent risks related to a general-purpose AI model that may present systemic risks, the development or use of the model causes a serious incident, the general-purpose AI model provider shouldwithout undue delay keep track of the incident and report any relevant information and possible corrective measuresto the Commission and national competent authorities. Furthermore, providers should ensure an adequate level ofcybersecurity protection for the model and its physical infrastructure, if appropriate, along the entire model lifecycle. Cybersecurity protection related to systemic risks associated with malicious use or attacks should duly consideraccidental model leakage, unauthorised releases, circumvention of safety measures, and defence against cyberattacks, unauthorised access or model theft. That protection could be facilitated by securing model weights, algorithms, servers, and data sets, such as through operational security measures for information security, specific cybersecuritypolicies, adequate technical and established solutions, and cyber and physical access controls, appropriate to therelevant circumstances and the risks involved.(116) The AI Office should encourage and facilitate the drawing up, review and adaptation of codes of practice, taking intoaccount international approaches. All providers of general-purpose AI models could be invited to participate. Toensure that the codes of practice reflect the state of the art and duly take into account a diverse set of perspectives, the AI Office should collaborate with relevant national competent authorities, and could, where appropriate, consultwith civil society organisations and other relevant stakeholders and experts, including the Scientific Panel, for thedrawing up of such codes. Codes of practice should cover obligations for providers of general-purpose AI modelsand of general-purpose AI models presenting systemic risks. In addition, as regards systemic risks, codes of practiceshould help to establish a risk taxonomy of the type and nature of the systemic risks at Union level, including theirsources. Codes of practice should also be focused on specific risk assessment and mitigation measures.(117) The codes of practice should represent a central tool for the proper compliance with the obligations provided forunder this Regulation for providers of general-purpose AI models. Providers should be able to rely on codes ofpractice to demonstrate compliance with the obligations. By means of implementing acts, the Commission maydecide to approve a code of practice and give it a general validity within the Union, or, alternatively, to providepublished and assessed as suitable to cover the relevant obligations by the AI Office, compliance with a Europeanharmonised standard should grant providers the presumption of conformity. Providers of general-purpose AImodels should furthermore be able to demonstrate compliance using alternative adequate means, if codes of practiceor harmonised standards are not available, or they choose not to rely on those.(118) This Regulation regulates AI systems and AI models by imposing certain requirements and obligations for relevantmarket actors that are placing them on the market, putting into service or use in the Union, thereby complementingobligations for providers of intermediary services that embed such systems or models into their services regulated by Regulation (EU) 2022/2065. To the extent that such systems or models are embedded into designated very largeonline platforms or very large online search engines, they are subject to the risk-management framework providedfor in Regulation (EU) 2022/2065. Consequently, the corresponding obligations of this Regulation should bepresumed to be fulfilled, unless significant systemic risks not covered by Regulation (EU) 2022/2065 emerge and areidentified in such models. Within this framework, providers of very large online platforms and very large onlinesearch engines are obliged to assess potential systemic risks stemming from the design, functioning and use of theirservices, including how the design of algorithmic systems used in the service may contribute to such risks, as well assystemic risks stemming from potential misuses. Those providers are also obliged to take appropriate mitigatingmeasures in observance of fundamental rights.(119) Considering the quick pace of innovation and the technological evolution of digital services in scope of differentinstruments of Union law in particular having in mind the usage and the perception of their recipients, the AIsystems subject to this Regulation may be provided as intermediary services or parts thereof within the meaning of Regulation (EU) 2022/2065, which should be interpreted in a technology-neutral manner. For example, AI systemsmay be used to provide online search engines, in particular, to the extent that an AI system such as an online chatbotperforms searches of, in principle, all websites, then incorporates the results into its existing knowledge and uses theupdated knowledge to generate a single output that combines different sources of information.(120) Furthermore, obligations placed on providers and deployers of certain AI systems in this Regulation to enable thedetection and disclosure that the outputs of those systems are artificially generated or manipulated are particularlyrelevant to facilitate the effective implementation of Regulation (EU) 2022/2065. This applies in particular as regardsthe obligations of providers of very large online platforms or very large online search engines to identify andmitigate systemic risks that may arise from the dissemination of content that has been artificially generated ormanipulated, in particular risk of the actual or foreseeable negative effects on democratic processes, civic discourseand electoral processes, including through disinformation.(121) Standardisation should play a key role to provide technical solutions to providers to ensure compliance with this Regulation, in line with the state of the art, to promote innovation as well as competitiveness and growth in thesingle market. Compliance with harmonised standards as defined in Article 2, point (1)(c), of Regulation (EU)No 1025/2012 of the European Parliament and of the Council (41), which are normally expected to reflect the stateof the art, should be a means for providers to demonstrate conformity with the requirements of this Regulation. A balanced representation of interests involving all relevant stakeholders in the development of standards, inparticular SMEs, consumer organisations and environmental and social stakeholders in accordance with Articles 5and 6 of Regulation (EU) No 1025/2012 should therefore be encouraged. In order to facilitate compliance, thestandardisation requests should be issued by the Commission without undue delay. When preparing thestandardisation request, the Commission should consult the advisory forum and the Board in order to collectrelevant expertise. However, in the absence of relevant references to harmonised standards, the Commission shouldbe able to establish, via implementing acts, and after consultation of the advisory forum, common specifications forcertain requirements under this Regulation. The common specification should be an exceptional fall back solution tofacilitate the provider’s obligation to comply with the requirements of this Regulation, when the standardisationrequest has not been accepted by any of the European standardisation organisations, or when the relevantharmonised standards insufficiently address fundamental rights concerns, or when the harmonised standards do notcomply with the request, or when there are delays in the adoption of an appropriate harmonised standard. Wheresuch a delay in the adoption of a harmonised standard is due to the technical complexity of that standard, this should(41) Regulation (EU) No 1025/2012 of the European Parliament and of the Council of 25 October 2012 on European standardisation, amending Council Directives 89/686/EEC and 93/15/EEC and Directives 94/9/EC, 94/25/EC, 95/16/EC, 97/23/EC, 98/34/EC, be considered by the Commission before contemplating the establishment of common specifications. Whendeveloping common specifications, the Commission is encouraged to cooperate with international partners andinternational standardisation bodies.(122) It is appropriate that, without prejudice to the use of harmonised standards and common specifications, providers ofa high-risk AI system that has been trained and tested on data reflecting the specific geographical, behavioural, contextual or functional setting within which the AI system is intended to be used, should be presumed to complywith the relevant measure provided for under the requirement on data governance set out in this Regulation. Without prejudice to the requirements related to robustness and accuracy set out in this Regulation, in accordancewith Article 54(3) of Regulation (EU) 2019/881, high-risk AI systems that have been certified or for whicha statement of conformity has been issued under a cybersecurity scheme pursuant to that Regulation and thereferences of which have been published in the Official Journal of the European Union should be presumed to complywith the cybersecurity requirement of this Regulation in so far as the cybersecurity certificate or statement ofconformity or parts thereof cover the cybersecurity requirement of this Regulation. This remains without prejudiceto the voluntary nature of that cybersecurity scheme.(123) In order to ensure a high level of trustworthiness of high-risk AI systems, those systems should be subject toa conformity assessment prior to their placing on the market or putting into service.(124) It is appropriate that, in order to minimise the burden on operators and avoid any possible duplication, for high-risk AI systems related to products which are covered by existing Union harmonisation legislation based on the New Legislative Framework, the compliance of those AI systems with the requirements of this Regulation should beassessed as part of the conformity assessment already provided for in that law. The applicability of the requirementsof this Regulation should thus not affect the specific logic, methodology or general structure of conformityassessment under the relevant Union harmonisation legislation.(125) Given the complexity of high-risk AI systems and the risks that are associated with them, it is important to developan adequate conformity assessment procedure for high-risk AI systems involving notified bodies, so-called thirdparty conformity assessment. However, given the current experience of professional pre-market certifiers in the fieldof product safety and the different nature of risks involved, it is appropriate to limit, at least in an initial phase ofapplication of this Regulation, the scope of application of third-party conformity assessment for high-risk AIsystems other than those related to products. Therefore, the conformity assessment of such systems should becarried out as a general rule by the provider under its own responsibility, with the only exception of AI systemsintended to be used for biometrics.(126) In order to carry out third-party conformity assessments when so required, notified bodies should be notified underthis Regulation by the national competent authorities, provided that they comply with a set of requirements, inparticular on independence, competence, absence of conflicts of interests and suitable cybersecurity requirements. Notification of those bodies should be sent by national competent authorities to the Commission and the other Member States by means of the electronic notification tool developed and managed by the Commission pursuant to Article R23 of Annex I to Decision No 768/2008/EC.(127) In line with Union commitments under the World Trade Organization Agreement on Technical Barriers to Trade, it isadequate to facilitate the mutual recognition of conformity assessment results produced by competent conformityassessment bodies, independent of the territory in which they are established, provided that those conformityassessment bodies established under the law of a third country meet the applicable requirements of this Regulationand the Union has concluded an agreement to that extent. In this context, the Commission should actively explorepossible international instruments for that purpose and in particular pursue the conclusion of mutual recognitionagreements with third countries.(128) In line with the commonly established notion of substantial modification for products regulated by Unionharmonisation legislation, it is appropriate that whenever a change occurs which may affect the compliance ofa high-risk AI system with this Regulation (e. g. change of operating system or software architecture), or when theintended purpose of the system changes, that AI system should be considered to be a new AI system which shouldundergo a new conformity assessment. However, changes occurring to the algorithm and the performance of AIsystems which continue to ‘learn’ after being placed on the market or put into service, namely automatically(129) High-risk AI systems should bear the CE marking to indicate their conformity with this Regulation so that they canmove freely within the internal market. For high-risk AI systems embedded in a product, a physical CE markingshould be affixed, and may be complemented by a digital CE marking. For high-risk AI systems only provideddigitally, a digital CE marking should be used. Member States should not create unjustified obstacles to the placingon the market or the putting into service of high-risk AI systems that comply with the requirements laid down inthis Regulation and bear the CE marking.(130) Under certain conditions, rapid availability of innovative technologies may be crucial for health and safety ofpersons, the protection of the environment and climate change and for society as a whole. It is thus appropriate thatunder exceptional reasons of public security or protection of life and health of natural persons, environmentalprotection and the protection of key industrial and infrastructural assets, market surveillance authorities couldauthorise the placing on the market or the putting into service of AI systems which have not undergonea conformity assessment. In duly justified situations, as provided for in this Regulation, law enforcement authoritiesor civil protection authorities may put a specific high-risk AI system into service without the authorisation of themarket surveillance authority, provided that such authorisation is requested during or after the use without unduedelay.(131) In order to facilitate the work of the Commission and the Member States in the AI field as well as to increase thetransparency towards the public, providers of high-risk AI systems other than those related to products falling withinthe scope of relevant existing Union harmonisation legislation, as well as providers who consider that an AI systemlisted in the high-risk use cases in an annex to this Regulation is not high-risk on the basis of a derogation, should berequired to register themselves and information about their AI system in an EU database, to be established andmanaged by the Commission. Before using an AI system listed in the high-risk use cases in an annex to this Regulation, deployers of high-risk AI systems that are public authorities, agencies or bodies, should registerthemselves in such database and select the system that they envisage to use. Other deployers should be entitled to doso voluntarily. This section of the EU database should be publicly accessible, free of charge, the information shouldbe easily navigable, understandable and machine-readable. The EU database should also be user-friendly, for exampleby providing search functionalities, including through keywords, allowing the general public to find relevantinformation to be submitted upon the registration of high-risk AI systems and on the use case of high-risk AIsystems, set out in an annex to this Regulation, to which the high-risk AI systems correspond. Any substantialmodification of high-risk AI systems should also be registered in the EU database. For high-risk AI systems in thearea of law enforcement, migration, asylum and border control management, the registration obligations should befulfilled in a secure non-public section of the EU database. Access to the secure non-public section should be strictlylimited to the Commission as well as to market surveillance authorities with regard to their national section of thatdatabase. High-risk AI systems in the area of critical infrastructure should only be registered at national level. The Commission should be the controller of the EU database, in accordance with Regulation (EU) 2018/1725. In orderto ensure the full functionality of the EU database, when deployed, the procedure for setting the database shouldinclude the development of functional specifications by the Commission and an independent audit report. The Commission should take into account cybersecurity risks when carrying out its tasks as data controller on the EUdatabase. In order to maximise the availability and use of the EU database by the public, the EU database, includingthe information made available through it, should comply with requirements under the Directive (EU) 2019/882.(132) Certain AI systems intended to interact with natural persons or to generate content may pose specific risks ofimpersonation or deception irrespective of whether they qualify as high-risk or not. In certain circumstances, the useof these systems should therefore be subject to specific transparency obligations without prejudice to therequirements and obligations for high-risk AI systems and subject to targeted exceptions to take into account thespecial need of law enforcement. In particular, natural persons should be notified that they are interacting with an AIsystem, unless this is obvious from the point of view of a natural person who is reasonably well-informed, observantand circumspect taking into account the circumstances and the context of use. When implementing that obligation, the characteristics of natural persons belonging to vulnerable groups due to their age or disability should be takeninto account to the extent the AI system is intended to interact with those groups as well. Moreover, natural personsshould be notified when they are exposed to AI systems that, by processing their biometric data, can identify or inferthe emotions or intentions of those persons or assign them to specific categories. Such specific categories can relate(133) A variety of AI systems can generate large quantities of synthetic content that becomes increasingly hard for humansto distinguish from human-generated and authentic content. The wide availability and increasing capabilities ofthose systems have a significant impact on the integrity and trust in the information ecosystem, raising new risks ofmisinformation and manipulation at scale, fraud, impersonation and consumer deception. In light of those impacts, the fast technological pace and the need for new methods and techniques to trace origin of information, it isappropriate to require providers of those systems to embed technical solutions that enable marking in a machinereadable format and detection that the output has been generated or manipulated by an AI system and not a human. Such techniques and methods should be sufficiently reliable, interoperable, effective and robust as far as this istechnically feasible, taking into account available techniques or a combination of such techniques, such aswatermarks, metadata identifications, cryptographic methods for proving provenance and authenticity of content, logging methods, fingerprints or other techniques, as may be appropriate. When implementing this obligation, providers should also take into account the specificities and the limitations of the different types of content and therelevant technological and market developments in the field, as reflected in the generally acknowledged state of theart. Such techniques and methods can be implemented at the level of the AI system or at the level of the AI model, including general-purpose AI models generating content, thereby facilitating fulfilment of this obligation by thedownstream provider of the AI system. To remain proportionate, it is appropriate to envisage that this markingobligation should not cover AI systems performing primarily an assistive function for standard editing or AI systemsnot substantially altering the input data provided by the deployer or the semantics thereof.(134) Further to the technical solutions employed by the providers of the AI system, deployers who use an AI system togenerate or manipulate image, audio or video content that appreciably resembles existing persons, objects, places, entities or events and would falsely appear to a person to be authentic or truthful (deep fakes), should also clearlyand distinguishably disclose that the content has been artificially created or manipulated by labelling the AI outputaccordingly and disclosing its artificial origin. Compliance with this transparency obligation should not beinterpreted as indicating that the use of the AI system or its output impedes the right to freedom of expression andthe right to freedom of the arts and sciences guaranteed in the Charter, in particular where the content is part of anevidently creative, satirical, artistic, fictional or analogous work or programme, subject to appropriate safeguards forthe rights and freedoms of third parties. In those cases, the transparency obligation for deep fakes set out in this Regulation is limited to disclosure of the existence of such generated or manipulated content in an appropriatemanner that does not hamper the display or enjoyment of the work, including its normal exploitation and use, whilemaintaining the utility and quality of the work. In addition, it is also appropriate to envisage a similar disclosureobligation in relation to AI-generated or manipulated text to the extent it is published with the purpose of informingthe public on matters of public interest unless the AI-generated content has undergone a process of human review oreditorial control and a natural or legal person holds editorial responsibility for the publication of the content.(135) Without prejudice to the mandatory nature and full applicability of the transparency obligations, the Commissionmay also encourage and facilitate the drawing up of codes of practice at Union level to facilitate the effectiveimplementation of the obligations regarding the detection and labelling of artificially generated or manipulatedcontent, including to support practical arrangements for making, as appropriate, the detection mechanismsaccessible and facilitating cooperation with other actors along the value chain, disseminating content or checking itsauthenticity and provenance to enable the public to effectively distinguish AI-generated content.(136) The obligations placed on providers and deployers of certain AI systems in this Regulation to enable the detectionand disclosure that the outputs of those systems are artificially generated or manipulated are particularly relevant tofacilitate the effective implementation of Regulation (EU) 2022/2065. This applies in particular as regards theobligations of providers of very large online platforms or very large online search engines to identify and mitigatesystemic risks that may arise from the dissemination of content that has been artificially generated or manipulated, in particular the risk of the actual or foreseeable negative effects on democratic processes, civic discourse andelectoral processes, including through disinformation. The requirement to label content generated by AI systemsunder this Regulation is without prejudice to the obligation in Article 16(6) of Regulation (EU) 2022/2065 forproviders of hosting services to process notices on illegal content received pursuant to Article 16(1) of that(137) Compliance with the transparency obligations for the AI systems covered by this Regulation should not beinterpreted as indicating that the use of the AI system or its output is lawful under this Regulation or other Unionand Member State law and should be without prejudice to other transparency obligations for deployers of AI systemslaid down in Union or national law.(138) AI is a rapidly developing family of technologies that requires regulatory oversight and a safe and controlled spacefor experimentation, while ensuring responsible innovation and integration of appropriate safeguards and riskmitigation measures. To ensure a legal framework that promotes innovation, is future-proof and resilient todisruption, Member States should ensure that their national competent authorities establish at least one AIregulatory sandbox at national level to facilitate the development and testing of innovative AI systems under strictregulatory oversight before these systems are placed on the market or otherwise put into service. Member Statescould also fulfil this obligation through participating in already existing regulatory sandboxes or establishing jointlya sandbox with one or more Member States’ competent authorities, insofar as this participation provides equivalentlevel of national coverage for the participating Member States. AI regulatory sandboxes could be established inphysical, digital or hybrid form and may accommodate physical as well as digital products. Establishing authoritiesshould also ensure that the AI regulatory sandboxes have the adequate resources for their functioning, includingfinancial and human resources.(139) The objectives of the AI regulatory sandboxes should be to foster AI innovation by establishing a controlledexperimentation and testing environment in the development and pre-marketing phase with a view to ensuringcompliance of the innovative AI systems with this Regulation and other relevant Union and national law. Moreover, the AI regulatory sandboxes should aim to enhance legal certainty for innovators and the competent authorities’oversight and understanding of the opportunities, emerging risks and the impacts of AI use, to facilitate regulatorylearning for authorities and undertakings, including with a view to future adaptions of the legal framework, tosupport cooperation and the sharing of best practices with the authorities involved in the AI regulatory sandbox, and to accelerate access to markets, including by removing barriers for SMEs, including start-ups. AI regulatorysandboxes should be widely available throughout the Union, and particular attention should be given to theiraccessibility for SMEs, including start-ups. The participation in the AI regulatory sandbox should focus on issues thatraise legal uncertainty for providers and prospective providers to innovate, experiment with AI in the Union andcontribute to evidence-based regulatory learning. The supervision of the AI systems in the AI regulatory sandboxshould therefore cover their development, training, testing and validation before the systems are placed on themarket or put into service, as well as the notion and occurrence of substantial modification that may require a newconformity assessment procedure. Any significant risks identified during the development and testing of such AIsystems should result in adequate mitigation and, failing that, in the suspension of the development and testingprocess. Where appropriate, national competent authorities establishing AI regulatory sandboxes should cooperatewith other relevant authorities, including those supervising the protection of fundamental rights, and could allow forthe involvement of other actors within the AI ecosystem such as national or European standardisation organisations, notified bodies, testing and experimentation facilities, research and experimentation labs, European Digital Innovation Hubs and relevant stakeholder and civil society organisations. To ensure uniform implementation acrossthe Union and economies of scale, it is appropriate to establish common rules for the AI regulatory sandboxes’implementation and a framework for cooperation between the relevant authorities involved in the supervision of thesandboxes. AI regulatory sandboxes established under this Regulation should be without prejudice to other lawallowing for the establishment of other sandboxes aiming to ensure compliance with law other than this Regulation. Where appropriate, relevant competent authorities in charge of those other regulatory sandboxes should considerthe benefits of using those sandboxes also for the purpose of ensuring compliance of AI systems with this Regulation. Upon agreement between the national competent authorities and the participants in the AI regulatorysandbox, testing in real world conditions may also be operated and supervised in the framework of the AI regulatorysandbox.(140) This Regulation should provide the legal basis for the providers and prospective providers in the AI regulatorysandbox to use personal data collected for other purposes for developing certain AI systems in the public interestwithin the AI regulatory sandbox, only under specified conditions, in accordance with Article 6(4) and Article 9(2), point (g), of Regulation (EU) 2016/679, and Articles 5, 6 and 10 of Regulation (EU) 2018/1725, and withoutprejudice to Article 4(2) and Article 10 of Directive (EU) 2016/680. All other obligations of data controllers andrights of data subjects under Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680 remainproviders in the AI regulatory sandbox should ensure appropriate safeguards and cooperate with the competentauthorities, including by following their guidance and acting expeditiously and in good faith to adequately mitigateany identified significant risks to safety, health, and fundamental rights that may arise during the development, testing and experimentation in that sandbox.(141) In order to accelerate the process of development and the placing on the market of the high-risk AI systems listed inan annex to this Regulation, it is important that providers or prospective providers of such systems may also benefitfrom a specific regime for testing those systems in real world conditions, without participating in an AI regulatorysandbox. However, in such cases, taking into account the possible consequences of such testing on individuals, itshould be ensured that appropriate and sufficient guarantees and conditions are introduced by this Regulation forproviders or prospective providers. Such guarantees should include, inter alia, requesting informed consent ofnatural persons to participate in testing in real world conditions, with the exception of law enforcement where theseeking of informed consent would prevent the AI system from being tested. Consent of subjects to participate insuch testing under this Regulation is distinct from, and without prejudice to, consent of data subjects for theprocessing of their personal data under the relevant data protection law. It is also important to minimise the risksand enable oversight by competent authorities and therefore require prospective providers to have a real-worldtesting plan submitted to competent market surveillance authority, register the testing in dedicated sections in the EUdatabase subject to some limited exceptions, set limitations on the period for which the testing can be done andrequire additional safeguards for persons belonging to certain vulnerable groups, as well as a written agreementdefining the roles and responsibilities of prospective providers and deployers and effective oversight by competentpersonnel involved in the real world testing. Furthermore, it is appropriate to envisage additional safeguards toensure that the predictions, recommendations or decisions of the AI system can be effectively reversed anddisregarded and that personal data is protected and is deleted when the subjects have withdrawn their consent toparticipate in the testing without prejudice to their rights as data subjects under the Union data protection law. Asregards transfer of data, it is also appropriate to envisage that data collected and processed for the purpose of testingin real-world conditions should be transferred to third countries only where appropriate and applicable safeguardsunder Union law are implemented, in particular in accordance with bases for transfer of personal data under Unionlaw on data protection, while for non-personal data appropriate safeguards are put in place in accordance with Union law, such as Regulations (EU) 2022/868 (42) and (EU) 2023/2854 (43) of the European Parliament and of the Council.(142) To ensure that AI leads to socially and environmentally beneficial outcomes, Member States are encouraged tosupport and promote research and development of AI solutions in support of socially and environmentallybeneficial outcomes, such as AI-based solutions to increase accessibility for persons with disabilities, tacklesocio-economic inequalities, or meet environmental targets, by allocating sufficient resources, including public and Union funding, and, where appropriate and provided that the eligibility and selection criteria are fulfilled, considering in particular projects which pursue such objectives. Such projects should be based on the principle ofinterdisciplinary cooperation between AI developers, experts on inequality and non-discrimination, accessibility, consumer, environmental, and digital rights, as well as academics.(143) In order to promote and protect innovation, it is important that the interests of SMEs, including start-ups, that areproviders or deployers of AI systems are taken into particular account. To that end, Member States should developinitiatives, which are targeted at those operators, including on awareness raising and information communication. Member States should provide SMEs, including start-ups, that have a registered office or a branch in the Union, withpriority access to the AI regulatory sandboxes provided that they fulfil the eligibility conditions and selection criteriaand without precluding other providers and prospective providers to access the sandboxes provided the sameconditions and criteria are fulfilled. Member States should utilise existing channels and where appropriate, establishnew dedicated channels for communication with SMEs, including start-ups, deployers, other innovators and, asappropriate, local public authorities, to support SMEs throughout their development path by providing guidanceand responding to queries about the implementation of this Regulation. Where appropriate, these channels shouldwork together to create synergies and ensure homogeneity in their guidance to SMEs, including start-ups, anddeployers. Additionally, Member States should facilitate the participation of SMEs and other relevant stakeholders inthe standardisation development processes. Moreover, the specific interests and needs of providers that are SMEs,(42) Regulation (EU) 2022/868 of the European Parliament and of the Council of 30 May 2022 on European data governance andamending Regulation (EU) 2018/1724 (Data Governance Act) (OJ L 152, 3.6.2022, p. 1).(43) Regulation (EU) 2023/2854 of the European Parliament and of the Council of 13 December 2023 on harmonised rules on fairincluding start-ups, should be taken into account when notified bodies set conformity assessment fees. The Commission should regularly assess the certification and compliance costs for SMEs, including start-ups, throughtransparent consultations and should work with Member States to lower such costs. For example, translation costsrelated to mandatory documentation and communication with authorities may constitute a significant cost forproviders and other operators, in particular those of a smaller scale. Member States should possibly ensure that oneof the languages determined and accepted by them for relevant providers’ documentation and for communicationwith operators is one which is broadly understood by the largest possible number of cross-border deployers. In orderto address the specific needs of SMEs, including start-ups, the Commission should provide standardised templates forthe areas covered by this Regulation, upon request of the Board. Additionally, the Commission should complement Member States’ efforts by providing a single information platform with easy-to-use information with regards to this Regulation for all providers and deployers, by organising appropriate communication campaigns to raise awarenessabout the obligations arising from this Regulation, and by evaluating and promoting the convergence of bestpractices in public procurement procedures in relation to AI systems. Medium-sized enterprises which until recentlyqualified as small enterprises within the meaning of the Annex to Commission Recommendation 2003/361/EC (44)should have access to those support measures, as those new medium-sized enterprises may sometimes lack the legalresources and training necessary to ensure proper understanding of, and compliance with, this Regulation.(144) In order to promote and protect innovation, the AI-on-demand platform, all relevant Union funding programmesand projects, such as Digital Europe Programme, Horizon Europe, implemented by the Commission and the Member States at Union or national level should, as appropriate, contribute to the achievement of the objectives of this Regulation.(145) In order to minimise the risks to implementation resulting from lack of knowledge and expertise in the market aswell as to facilitate compliance of providers, in particular SMEs, including start-ups, and notified bodies with theirobligations under this Regulation, the AI-on-demand platform, the European Digital Innovation Hubs and thetesting and experimentation facilities established by the Commission and the Member States at Union or nationallevel should contribute to the implementation of this Regulation. Within their respective mission and fields ofcompetence, the AI-on-demand platform, the European Digital Innovation Hubs and the testing andexperimentation Facilities are able to provide in particular technical and scientific support to providers andnotified bodies.(146) Moreover, in light of the very small size of some operators and in order to ensure proportionality regarding costs ofinnovation, it is appropriate to allow microenterprises to fulfil one of the most costly obligations, namely toestablish a quality management system, in a simplified manner which would reduce the administrative burden andthe costs for those enterprises without affecting the level of protection and the need for compliance with therequirements for high-risk AI systems. The Commission should develop guidelines to specify the elements of thequality management system to be fulfilled in this simplified manner by microenterprises.(147) It is appropriate that the Commission facilitates, to the extent possible, access to testing and experimentationfacilities to bodies, groups or laboratories established or accredited pursuant to any relevant Union harmonisationlegislation and which fulfil tasks in the context of conformity assessment of products or devices covered by that Union harmonisation legislation. This is, in particular, the case as regards expert panels, expert laboratories andreference laboratories in the field of medical devices pursuant to Regulations (EU) 2017/745 and (EU) 2017/746.(148) This Regulation should establish a governance framework that both allows to coordinate and support theapplication of this Regulation at national level, as well as build capabilities at Union level and integrate stakeholdersin the field of AI. The effective implementation and enforcement of this Regulation require a governance frameworkthat allows to coordinate and build up central expertise at Union level. The AI Office was established by Commission Decision (45) and has as its mission to develop Union expertise and capabilities in the field of AI and to contribute tothe implementation of Union law on AI. Member States should facilitate the tasks of the AI Office with a view tosupport the development of Union expertise and capabilities at Union level and to strengthen the functioning of thedigital single market. Furthermore, a Board composed of representatives of the Member States, a scientific panel tointegrate the scientific community and an advisory forum to contribute stakeholder input to the implementation ofthis Regulation, at Union and national level, should be established. The development of Union expertise and(44) Commission Recommendation of 6 May 2003 concerning the definition of micro, small and medium-sized enterprises (OJ L 124, capabilities should also include making use of existing resources and expertise, in particular through synergies withstructures built up in the context of the Union level enforcement of other law and synergies with related initiatives at Union level, such as the Euro HPC Joint Undertaking and the AI testing and experimentation facilities under the Digital Europe Programme.(149) In order to facilitate a smooth, effective and harmonised implementation of this Regulation a Board should beestablished. The Board should reflect the various interests of the AI eco-system and be composed of representativesof the Member States. The Board should be responsible for a number of advisory tasks, including issuing opinions, recommendations, advice or contributing to guidance on matters related to the implementation of this Regulation, including on enforcement matters, technical specifications or existing standards regarding the requirementsestablished in this Regulation and providing advice to the Commission and the Member States and their nationalcompetent authorities on specific questions related to AI. In order to give some flexibility to Member States in thedesignation of their representatives in the Board, such representatives may be any persons belonging to publicentities who should have the relevant competences and powers to facilitate coordination at national level andcontribute to the achievement of the Board’s tasks. The Board should establish two standing sub-groups to providea platform for cooperation and exchange among market surveillance authorities and notifying authorities on issuesrelated, respectively, to market surveillance and notified bodies. The standing subgroup for market surveillanceshould act as the administrative cooperation group (ADCO) for this Regulation within the meaning of Article 30 of Regulation (EU) 2019/1020. In accordance with Article 33 of that Regulation, the Commission should support theactivities of the standing subgroup for market surveillance by undertaking market evaluations or studies, inparticular with a view to identifying aspects of this Regulation requiring specific and urgent coordination amongmarket surveillance authorities. The Board may establish other standing or temporary sub-groups as appropriate forthe purpose of examining specific issues. The Board should also cooperate, as appropriate, with relevant Unionbodies, experts groups and networks active in the context of relevant Union law, including in particular those activeunder relevant Union law on data, digital products and services.(150) With a view to ensuring the involvement of stakeholders in the implementation and application of this Regulation, an advisory forum should be established to advise and provide technical expertise to the Board and the Commission. To ensure a varied and balanced stakeholder representation between commercial and non-commercial interest and, within the category of commercial interests, with regards to SMEs and other undertakings, the advisory forumshould comprise inter alia industry, start-ups, SMEs, academia, civil society, including the social partners, as well asthe Fundamental Rights Agency, ENISA, the European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC) and the European Telecommunications Standards Institute (ETSI).(151) To support the implementation and enforcement of this Regulation, in particular the monitoring activities of the AIOffice as regards general-purpose AI models, a scientific panel of independent experts should be established. Theindependent experts constituting the scientific panel should be selected on the basis of up-to-date scientific ortechnical expertise in the field of AI and should perform their tasks with impartiality, objectivity and ensure theconfidentiality of information and data obtained in carrying out their tasks and activities. To allow the reinforcementof national capacities necessary for the effective enforcement of this Regulation, Member States should be able torequest support from the pool of experts constituting the scientific panel for their enforcement activities.(152) In order to support adequate enforcement as regards AI systems and reinforce the capacities of the Member States, Union AI testing support structures should be established and made available to the Member States.(153) Member States hold a key role in the application and enforcement of this Regulation. In that respect, each Member State should designate at least one notifying authority and at least one market surveillance authority as nationalcompetent authorities for the purpose of supervising the application and implementation of this Regulation. Member States may decide to appoint any kind of public entity to perform the tasks of the national competentauthorities within the meaning of this Regulation, in accordance with their specific national organisationalcharacteristics and needs. In order to increase organisation efficiency on the side of Member States and to set a single(154) The national competent authorities should exercise their powers independently, impartially and without bias, so asto safeguard the principles of objectivity of their activities and tasks and to ensure the application andimplementation of this Regulation. The members of these authorities should refrain from any action incompatiblewith their duties and should be subject to confidentiality rules under this Regulation.(155) In order to ensure that providers of high-risk AI systems can take into account the experience on the use of high-risk AI systems for improving their systems and the design and development process or can take any possible correctiveaction in a timely manner, all providers should have a post-market monitoring system in place. Where relevant, post-market monitoring should include an analysis of the interaction with other AI systems including other devicesand software. Post-market monitoring should not cover sensitive operational data of deployers which are lawenforcement authorities. This system is also key to ensure that the possible risks emerging from AI systems whichcontinue to ‘learn’ after being placed on the market or put into service can be more efficiently and timely addressed. In this context, providers should also be required to have a system in place to report to the relevant authorities anyserious incidents resulting from the use of their AI systems, meaning incident or malfunctioning leading to death orserious damage to health, serious and irreversible disruption of the management and operation of criticalinfrastructure, infringements of obligations under Union law intended to protect fundamental rights or seriousdamage to property or the environment.(156) In order to ensure an appropriate and effective enforcement of the requirements and obligations set out by this Regulation, which is Union harmonisation legislation, the system of market surveillance and compliance of productsestablished by Regulation (EU) 2019/1020 should apply in its entirety. Market surveillance authorities designatedpursuant to this Regulation should have all enforcement powers laid down in this Regulation and in Regulation (EU)2019/1020 and should exercise their powers and carry out their duties independently, impartially and without bias. Although the majority of AI systems are not subject to specific requirements and obligations under this Regulation, market surveillance authorities may take measures in relation to all AI systems when they present a risk inaccordance with this Regulation. Due to the specific nature of Union institutions, agencies and bodies falling withinthe scope of this Regulation, it is appropriate to designate the European Data Protection Supervisor as a competentmarket surveillance authority for them. This should be without prejudice to the designation of national competentauthorities by the Member States. Market surveillance activities should not affect the ability of the supervised entitiesto carry out their tasks independently, when such independence is required by Union law.(157) This Regulation is without prejudice to the competences, tasks, powers and independence of relevant national publicauthorities or bodies which supervise the application of Union law protecting fundamental rights, including equalitybodies and data protection authorities. Where necessary for their mandate, those national public authorities orbodies should also have access to any documentation created under this Regulation. A specific safeguard procedureshould be set for ensuring adequate and timely enforcement against AI systems presenting a risk to health, safety andfundamental rights. The procedure for such AI systems presenting a risk should be applied to high-risk AI systemspresenting a risk, prohibited systems which have been placed on the market, put into service or used in violation ofthe prohibited practices laid down in this Regulation and AI systems which have been made available in violation ofthe transparency requirements laid down in this Regulation and present a risk.(158) Union financial services law includes internal governance and risk-management rules and requirements which areapplicable to regulated financial institutions in the course of provision of those services, including when they makeuse of AI systems. In order to ensure coherent application and enforcement of the obligations under this Regulationand relevant rules and requirements of the Union financial services legal acts, the competent authorities for thesupervision and enforcement of those legal acts, in particular competent authorities as defined in Regulation (EU)No 575/2013 of the European Parliament and of the Council (46) and Directives 2008/48/EC (47), 2009/138/EC (48),(46) Regulation (EU) No 575/2013 of the European Parliament and of the Council of 26 June 2013 on prudential requirements for creditinstitutions and investment firms and amending Regulation (EU) No 648/2012 (OJ L 176, 27.6.2013, p. 1).(47) Directive 2008/48/EC of the European Parliament and of the Council of 23 April 2008 on credit agreements for consumers andrepealing Council Directive 87/102/EEC (OJ L 133, 22.5.2008, p. 66).(48) Directive 2009/138/EC of the European Parliament and of the Council of 25 November 2009 on the taking-up and pursuit of thebusiness of Insurance and Reinsurance (Solvency II) (OJ L 335, 17.12.2009, p. 1).2013/36/EU (49), 2014/17/EU (50) and (EU) 2016/97 (51) of the European Parliament and of the Council, should bedesignated, within their respective competences, as competent authorities for the purpose of supervising theimplementation of this Regulation, including for market surveillance activities, as regards AI systems provided orused by regulated and supervised financial institutions unless Member States decide to designate another authority tofulfil these market surveillance tasks. Those competent authorities should have all powers under this Regulation and Regulation (EU) 2019/1020 to enforce the requirements and obligations of this Regulation, including powers tocarry our ex post market surveillance activities that can be integrated, as appropriate, into their existing supervisorymechanisms and procedures under the relevant Union financial services law. It is appropriate to envisage that, whenacting as market surveillance authorities under this Regulation, the national authorities responsible for thesupervision of credit institutions regulated under Directive 2013/36/EU, which are participating in the Single Supervisory Mechanism established by Council Regulation (EU) No 1024/2013 (52), should report, without delay, tothe European Central Bank any information identified in the course of their market surveillance activities that maybe of potential interest for the European Central Bank’s prudential supervisory tasks as specified in that Regulation. To further enhance the consistency between this Regulation and the rules applicable to credit institutions regulatedunder Directive 2013/36/EU, it is also appropriate to integrate some of the providers’ procedural obligations inrelation to risk management, post marketing monitoring and documentation into the existing obligations andprocedures under Directive 2013/36/EU. In order to avoid overlaps, limited derogations should also be envisaged inrelation to the quality management system of providers and the monitoring obligation placed on deployers ofhigh-risk AI systems to the extent that these apply to credit institutions regulated by Directive 2013/36/EU. Thesame regime should apply to insurance and re-insurance undertakings and insurance holding companies under Directive 2009/138/EC and the insurance intermediaries under Directive (EU) 2016/97 and other types of financialinstitutions subject to requirements regarding internal governance, arrangements or processes established pursuantto the relevant Union financial services law to ensure consistency and equal treatment in the financial sector.(159) Each market surveillance authority for high-risk AI systems in the area of biometrics, as listed in an annex to this Regulation insofar as those systems are used for the purposes of law enforcement, migration, asylum and bordercontrol management, or the administration of justice and democratic processes, should have effective investigativeand corrective powers, including at least the power to obtain access to all personal data that are being processed andto all information necessary for the performance of its tasks. The market surveillance authorities should be able toexercise their powers by acting with complete independence. Any limitations of their access to sensitive operationaldata under this Regulation should be without prejudice to the powers conferred to them by Directive(EU) 2016/680. No exclusion on disclosing data to national data protection authorities under this Regulation shouldaffect the current or future powers of those authorities beyond the scope of this Regulation.(160) The market surveillance authorities and the Commission should be able to propose joint activities, including jointinvestigations, to be conducted by market surveillance authorities or market surveillance authorities jointly with the Commission, that have the aim of promoting compliance, identifying non-compliance, raising awareness andproviding guidance in relation to this Regulation with respect to specific categories of high-risk AI systems that arefound to present a serious risk across two or more Member States. Joint activities to promote compliance should becarried out in accordance with Article 9 of Regulation (EU) 2019/1020. The AI Office should provide coordinationsupport for joint investigations.(161) It is necessary to clarify the responsibilities and competences at Union and national level as regards AI systems thatare built on general-purpose AI models. To avoid overlapping competences, where an AI system is based ona general-purpose AI model and the model and system are provided by the same provider, the supervision should(49) Directive 2013/36/EU of the European Parliament and of the Council of 26 June 2013 on access to the activity of credit institutionsand the prudential supervision of credit institutions and investment firms, amending Directive 2002/87/EC and repealing Directives 2006/48/EC and 2006/49/EC (OJ L 176, 27.6.2013, p. 338).(50) Directive 2014/17/EU of the European Parliament and of the Council of 4 February 2014 on credit agreements for consumersrelating to residential immovable property and amending Directives 2008/48/EC and 2013/36/EU and Regulation (EU)No 1093/2010 (OJ L 60, 28.2.2014, p. 34).(51) Directive (EU) 2016/97 of the European Parliament and of the Council of 20 January 2016 on insurance distribution (OJ L 26,2.2.2016, p. 19). take place at Union level through the AI Office, which should have the powers of a market surveillance authoritywithin the meaning of Regulation (EU) 2019/1020 for this purpose. In all other cases, national market surveillanceauthorities remain responsible for the supervision of AI systems. However, for general-purpose AI systems that canbe used directly by deployers for at least one purpose that is classified as high-risk, market surveillance authoritiesshould cooperate with the AI Office to carry out evaluations of compliance and inform the Board and other marketsurveillance authorities accordingly. Furthermore, market surveillance authorities should be able to requestassistance from the AI Office where the market surveillance authority is unable to conclude an investigation ona high-risk AI system because of its inability to access certain information related to the general-purpose AI modelon which the high-risk AI system is built. In such cases, the procedure regarding mutual assistance in cross-bordercases in Chapter VI of Regulation (EU) 2019/1020 should apply mutatis mutandis.(162) To make best use of the centralised Union expertise and synergies at Union level, the powers of supervision andenforcement of the obligations on providers of general-purpose AI models should be a competence of the Commission. The AI Office should be able to carry out all necessary actions to monitor the effective implementationof this Regulation as regards general-purpose AI models. It should be able to investigate possible infringements ofthe rules on providers of general-purpose AI models both on its own initiative, following the results of itsmonitoring activities, or upon request from market surveillance authorities in line with the conditions set out in this Regulation. To support effective monitoring of the AI Office, it should provide for the possibility that downstreamproviders lodge complaints about possible infringements of the rules on providers of general-purpose AI models andsystems.(163) With a view to complementing the governance systems for general-purpose AI models, the scientific panel shouldsupport the monitoring activities of the AI Office and may, in certain cases, provide qualified alerts to the AI Officewhich trigger follow-ups, such as investigations. This should be the case where the scientific panel has reason tosuspect that a general-purpose AI model poses a concrete and identifiable risk at Union level. Furthermore, thisshould be the case where the scientific panel has reason to suspect that a general-purpose AI model meets the criteriathat would lead to a classification as general-purpose AI model with systemic risk. To equip the scientific panel withthe information necessary for the performance of those tasks, there should be a mechanism whereby the scientificpanel can request the Commission to require documentation or information from a provider.(164) The AI Office should be able to take the necessary actions to monitor the effective implementation of andcompliance with the obligations for providers of general-purpose AI models laid down in this Regulation. The AIOffice should be able to investigate possible infringements in accordance with the powers provided for in this Regulation, including by requesting documentation and information, by conducting evaluations, as well as byrequesting measures from providers of general-purpose AI models. When conducting evaluations, in order to makeuse of independent expertise, the AI Office should be able to involve independent experts to carry out theevaluations on its behalf. Compliance with the obligations should be enforceable, inter alia, through requests to takeappropriate measures, including risk mitigation measures in the case of identified systemic risks as well as restrictingthe making available on the market, withdrawing or recalling the model. As a safeguard, where needed beyond theprocedural rights provided for in this Regulation, providers of general-purpose AI models should have theprocedural rights provided for in Article 18 of Regulation (EU) 2019/1020, which should apply mutatis mutandis, without prejudice to more specific procedural rights provided for by this Regulation.(165) The development of AI systems other than high-risk AI systems in accordance with the requirements of this Regulation may lead to a larger uptake of ethical and trustworthy AI in the Union. Providers of AI systems that arenot high-risk should be encouraged to create codes of conduct, including related governance mechanisms, intendedto foster the voluntary application of some or all of the mandatory requirements applicable to high-risk AI systems, adapted in light of the intended purpose of the systems and the lower risk involved and taking into account theavailable technical solutions and industry best practices such as model and data cards. Providers and, as appropriate, environmental sustainability, AI literacy measures, inclusive and diverse design and development of AI systems, including attention to vulnerable persons and accessibility to persons with disability, stakeholders’ participation withthe involvement, as appropriate, of relevant stakeholders such as business and civil society organisations, academia, research organisations, trade unions and consumer protection organisations in the design and development of AIsystems, and diversity of the development teams, including gender balance. To ensure that the voluntary codes ofconduct are effective, they should be based on clear objectives and key performance indicators to measure theachievement of those objectives. They should also be developed in an inclusive way, as appropriate, with theinvolvement of relevant stakeholders such as business and civil society organisations, academia, researchorganisations, trade unions and consumer protection organisation. The Commission may develop initiatives, including of a sectoral nature, to facilitate the lowering of technical barriers hindering cross-border exchange of datafor AI development, including on data access infrastructure, semantic and technical interoperability of different typesof data.(166) It is important that AI systems related to products that are not high-risk in accordance with this Regulation and thusare not required to comply with the requirements set out for high-risk AI systems are nevertheless safe when placedon the market or put into service. To contribute to this objective, Regulation (EU) 2023/988 of the European Parliament and of the Council (53) would apply as a safety net.(167) In order to ensure trustful and constructive cooperation of competent authorities on Union and national level, allparties involved in the application of this Regulation should respect the confidentiality of information and dataobtained in carrying out their tasks, in accordance with Union or national law. They should carry out their tasks andactivities in such a manner as to protect, in particular, intellectual property rights, confidential business informationand trade secrets, the effective implementation of this Regulation, public and national security interests, the integrityof criminal and administrative proceedings, and the integrity of classified information.(168) Compliance with this Regulation should be enforceable by means of the imposition of penalties and otherenforcement measures. Member States should take all necessary measures to ensure that the provisions of this Regulation are implemented, including by laying down effective, proportionate and dissuasive penalties for theirinfringement, and to respect the ne bis in idem principle. In order to strengthen and harmonise administrativepenalties for infringement of this Regulation, the upper limits for setting the administrative fines for certain specificinfringements should be laid down. When assessing the amount of the fines, Member States should, in eachindividual case, take into account all relevant circumstances of the specific situation, with due regard in particular tothe nature, gravity and duration of the infringement and of its consequences and to the size of the provider, inparticular if the provider is an SME, including a start-up. The European Data Protection Supervisor should have thepower to impose fines on Union institutions, agencies and bodies falling within the scope of this Regulation.(169) Compliance with the obligations on providers of general-purpose AI models imposed under this Regulation shouldbe enforceable, inter alia, by means of fines. To that end, appropriate levels of fines should also be laid down forinfringement of those obligations, including the failure to comply with measures requested by the Commission inaccordance with this Regulation, subject to appropriate limitation periods in accordance with the principle ofproportionality. All decisions taken by the Commission under this Regulation are subject to review by the Court of Justice of the European Union in accordance with the TFEU, including the unlimited jurisdiction of the Court of Justice with regard to penalties pursuant to Article 261 TFEU.(170) Union and national law already provide effective remedies to natural and legal persons whose rights and freedomsare adversely affected by the use of AI systems. Without prejudice to those remedies, any natural or legal person thathas grounds to consider that there has been an infringement of this Regulation should be entitled to lodgea complaint to the relevant market surveillance authority.(171) Affected persons should have the right to obtain an explanation where a deployer’s decision is based mainly uponthe output from certain high-risk AI systems that fall within the scope of this Regulation and where that decisionproduces legal effects or similarly significantly affects those persons in a way that they consider to have an adverse(53) Regulation (EU) 2023/988 of the European Parliament and of the Council of 10 May 2023 on general product safety, amending Regulation (EU) No 1025/2012 of the European Parliament and of the Council and Directive (EU) 2020/1828 of the Europeanimpact on their health, safety or fundamental rights. That explanation should be clear and meaningful and shouldprovide a basis on which the affected persons are able to exercise their rights. The right to obtain an explanationshould not apply to the use of AI systems for which exceptions or restrictions follow from Union or national lawand should apply only to the extent this right is not already provided for under Union law.(172) Persons acting as whistleblowers on the infringements of this Regulation should be protected under the Union law. Directive (EU) 2019/1937 of the European Parliament and of the Council (54) should therefore apply to the reportingof infringements of this Regulation and the protection of persons reporting such infringements.(173) In order to ensure that the regulatory framework can be adapted where necessary, the power to adopt acts inaccordance with Article 290 TFEU should be delegated to the Commission to amend the conditions under which an AI system is not to be considered to be high-risk, the list of high-risk AI systems, the provisions regarding technicaldocumentation, the content of the EU declaration of conformity the provisions regarding the conformity assessmentprocedures, the provisions establishing the high-risk AI systems to which the conformity assessment procedurebased on assessment of the quality management system and assessment of the technical documentation shouldapply, the threshold, benchmarks and indicators, including by supplementing those benchmarks and indicators, inthe rules for the classification of general-purpose AI models with systemic risk, the criteria for the designation ofgeneral-purpose AI models with systemic risk, the technical documentation for providers of general-purpose AImodels and the transparency information for providers of general-purpose AI models. It is of particular importancethat the Commission carry out appropriate consultations during its preparatory work, including at expert level, andthat those consultations be conducted in accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making (55). In particular, to ensure equal participation in thepreparation of delegated acts, the European Parliament and the Council receive all documents at the same time as Member States’ experts, and their experts systematically have access to meetings of Commission expert groupsdealing with the preparation of delegated acts.(174) Given the rapid technological developments and the technical expertise required to effectively apply this Regulation, the Commission should evaluate and review this Regulation by 2 August 2029 and every four years thereafter andreport to the European Parliament and the Council. In addition, taking into account the implications for the scope ofthis Regulation, the Commission should carry out an assessment of the need to amend the list of high-risk AIsystems and the list of prohibited practices once a year. Moreover, by 2 August 2028 and every four years thereafter, the Commission should evaluate and report to the European Parliament and to the Council on the need to amendthe list of high-risk areas headings in the annex to this Regulation, the AI systems within the scope of thetransparency obligations, the effectiveness of the supervision and governance system and the progress on thedevelopment of standardisation deliverables on energy efficient development of general-purpose AI models, including the need for further measures or actions. Finally, by 2 August 2028 and every three years thereafter, the Commission should evaluate the impact and effectiveness of voluntary codes of conduct to foster the application ofthe requirements provided for high-risk AI systems in the case of AI systems other than high-risk AI systems andpossibly other additional requirements for such AI systems.(175) In order to ensure uniform conditions for the implementation of this Regulation, implementing powers should beconferred on the Commission. Those powers should be exercised in accordance with Regulation (EU) No 182/2011of the European Parliament and of the Council (56).(176) Since the objective of this Regulation, namely to improve the functioning of the internal market and to promote theuptake of human centric and trustworthy AI, while ensuring a high level of protection of health, safety, fundamentalrights enshrined in the Charter, including democracy, the rule of law and environmental protection against harmfuleffects of AI systems in the Union and supporting innovation, cannot be sufficiently achieved by the Member Statesand can rather, by reason of the scale or effects of the action, be better achieved at Union level, the Union may adopt(54) Directive (EU) 2019/1937 of the European Parliament and of the Council of 23 October 2019 on the protection of persons whoreport breaches of Union law (OJ L 305, 26.11.2019, p. 17).(55) OJ L 123, 12.5.2016, p. 1.(56) Regulation (EU) No 182/2011 of the European Parliament and of the Council of 16 February 2011 laying down the rules andmeasures in accordance with the principle of subsidiarity as set out in Article 5 TEU. In accordance with theprinciple of proportionality as set out in that Article, this Regulation does not go beyond what is necessary in orderto achieve that objective.(177) In order to ensure legal certainty, ensure an appropriate adaptation period for operators and avoid disruption to themarket, including by ensuring continuity of the use of AI systems, it is appropriate that this Regulation applies to thehigh-risk AI systems that have been placed on the market or put into service before the general date of applicationthereof, only if, from that date, those systems are subject to significant changes in their design or intended purpose. It is appropriate to clarify that, in this respect, the concept of significant change should be understood as equivalentin substance to the notion of substantial modification, which is used with regard only to high-risk AI systemspursuant to this Regulation. On an exceptional basis and in light of public accountability, operators of AI systemswhich are components of the large-scale IT systems established by the legal acts listed in an annex to this Regulationand operators of high-risk AI systems that are intended to be used by public authorities should, respectively, take thenecessary steps to comply with the requirements of this Regulation by end of 2030 and by 2 August 2030.(178) Providers of high-risk AI systems are encouraged to start to comply, on a voluntary basis, with the relevantobligations of this Regulation already during the transitional period.(179) This Regulation should apply from 2 August 2026. However, taking into account the unacceptable risk associatedwith the use of AI in certain ways, the prohibitions as well as the general provisions of this Regulation should alreadyapply from 2 February 2025. While the full effect of those prohibitions follows with the establishment of thegovernance and enforcement of this Regulation, anticipating the application of the prohibitions is important to takeaccount of unacceptable risks and to have an effect on other procedures, such as in civil law. Moreover, theinfrastructure related to the governance and the conformity assessment system should be operational before2 August 2026, therefore the provisions on notified bodies and governance structure should apply from 2 August2025. Given the rapid pace of technological advancements and adoption of general-purpose AI models, obligationsfor providers of general-purpose AI models should apply from 2 August 2025. Codes of practice should be ready by2 May 2025 in view of enabling providers to demonstrate compliance on time. The AI Office should ensure thatclassification rules and procedures are up to date in light of technological developments. In addition, Member Statesshould lay down and notify to the Commission the rules on penalties, including administrative fines, and ensure thatthey are properly and effectively implemented by the date of application of this Regulation. Therefore the provisionson penalties should apply from 2 August 2025.(180) The European Data Protection Supervisor and the European Data Protection Board were consulted in accordancewith Article 42(1) and (2) of Regulation (EU) 2018/1725 and delivered their joint opinion on 18 June 2021, HAVE ADOPTED THIS REGULATION: CHAPTER IGENERAL PROVISIONSArticle 1Subject matter`1. The purpose of this Regulation is to improve the functioning of the internal market and promote the uptake ofhuman-centric and trustworthy artificial intelligence (AI), while ensuring a high level of protection of health, safety, fundamental rights enshrined in the Charter, including democracy, the rule of law and environmental protection, againstthe harmful effects of AI systems in the Union and supporting innovation.2. This Regulation lays down:(b) prohibitions of certain AI practices;(c) specific requirements for high-risk AI systems and obligations for operators of such systems;(d) harmonised transparency rules for certain AI systems;(e) harmonised rules for the placing on the market of general-purpose AI models;(f) rules on market monitoring, market surveillance, governance and enforcement;(g) measures to support innovation, with a particular focus on SMEs, including start-ups. Article 2Scope1. This Regulation applies to:(a) providers placing on the market or putting into service AI systems or placing on the market general-purpose AI modelsin the Union, irrespective of whether those providers are established or located within the Union or in a third country;(b) deployers of AI systems that have their place of establishment or are located within the Union;(c) providers and deployers of AI systems that have their place of establishment or are located in a third country, where theoutput produced by the AI system is used in the Union;(d) importers and distributors of AI systems;(e) product manufacturers placing on the market or putting into service an AI system together with their product andunder their own name or trademark;(f) authorised representatives of providers, which are not established in the Union;(g) affected persons that are located in the Union.2. For AI systems classified as high-risk AI systems in accordance with Article 6(1) related to products covered by the Union harmonisation legislation listed in Section B of Annex I, only Article 6(1), Articles 102 to 109 and Article 112 apply. Article 57 applies only in so far as the requirements for high-risk AI systems under this Regulation have been integrated inthat Union harmonisation legislation.3. This Regulation does not apply to areas outside the scope of Union law, and shall not, in any event, affect thecompetences of the Member States concerning national security, regardless of the type of entity entrusted by the Member States with carrying out tasks in relation to those competences. This Regulation does not apply to AI systems where and in so far they are placed on the market, put into service, or usedwith or without modification exclusively for military, defence or national security purposes, regardless of the type of entitycarrying out those activities. This Regulation does not apply to AI systems which are not placed on the market or put into service in the Union, wherethe output is used in the Union exclusively for military, defence or national security purposes, regardless of the type ofentity carrying out those activities.4. This Regulation applies neither to public authorities in a third country nor to international organisations fallingwithin the scope of this Regulation pursuant to paragraph 1, where those authorities or organisations use AI systems in theframework of international cooperation or agreements for law enforcement and judicial cooperation with the Union orwith one or more Member States, provided that such a third country or international organisation provides adequatesafeguards with respect to the protection of fundamental rights and freedoms of individuals.6. This Regulation does not apply to AI systems or AI models, including their output, specifically developed and put intoservice for the sole purpose of scientific research and development.7. Union law on the protection of personal data, privacy and the confidentiality of communications applies to personaldata processed in connection with the rights and obligations laid down in this Regulation. This Regulation shall not affect Regulation (EU) 2016/679 or (EU) 2018/1725, or Directive 2002/58/EC or (EU) 2016/680, without prejudice to Article10(5) and Article 59 of this Regulation.8. This Regulation does not apply to any research, testing or development activity regarding AI systems or AI modelsprior to their being placed on the market or put into service. Such activities shall be conducted in accordance withapplicable Union law. Testing in real world conditions shall not be covered by that exclusion.9. This Regulation is without prejudice to the rules laid down by other Union legal acts related to consumer protectionand product safety.10. This Regulation does not apply to obligations of deployers who are natural persons using AI systems in the course ofa purely personal non-professional activity.11. This Regulation does not preclude the Union or Member States from maintaining or introducing laws, regulations oradministrative provisions which are more favourable to workers in terms of protecting their rights in respect of the use of AI systems by employers, or from encouraging or allowing the application of collective agreements which are morefavourable to workers.12. This Regulation does not apply to AI systems released under free and open-source licences, unless they are placed onthe market or put into service as high-risk AI systems or as an AI system that falls under Article 5 or 50. Article 3Definitions For the purposes of this Regulation, the following definitions apply:(1) ‘AI system’ means a machine-based system that is designed to operate with varying levels of autonomy and that mayexhibit adaptiveness after deployment, and that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical orvirtual environments;(2) ‘risk’ means the combination of the probability of an occurrence of harm and the severity of that harm;(3) ‘provider’ means a natural or legal person, public authority, agency or other body that develops an AI system ora general-purpose AI model or that has an AI system or a general-purpose AI model developed and places it on themarket or puts the AI system into service under its own name or trademark, whether for payment or free of charge;(4) ‘deployer’ means a natural or legal person, public authority, agency or other body using an AI system under itsauthority except where the AI system is used in the course of a personal non-professional activity;(5) ‘authorised representative’ means a natural or legal person located or established in the Union who has received andaccepted a written mandate from a provider of an AI system or a general-purpose AI model to, respectively, performand carry out on its behalf the obligations and procedures established by this Regulation;(6) ‘importer’ means a natural or legal person located or established in the Union that places on the market an AI systemthat bears the name or trademark of a natural or legal person established in a third country;(7) ‘distributor’ means a natural or legal person in the supply chain, other than the provider or the importer, that makesan AI system available on the Union market;(9) ‘placing on the market’ means the first making available of an AI system or a general-purpose AI model on the Unionmarket;(10) ‘making available on the market’ means the supply of an AI system or a general-purpose AI model for distribution oruse on the Union market in the course of a commercial activity, whether in return for payment or free of charge;(11) ‘putting into service’ means the supply of an AI system for first use directly to the deployer or for own use in the Unionfor its intended purpose;(12) ‘intended purpose’ means the use for which an AI system is intended by the provider, including the specific contextand conditions of use, as specified in the information supplied by the provider in the instructions for use, promotionalor sales materials and statements, as well as in the technical documentation;(13) ‘reasonably foreseeable misuse’ means the use of an AI system in a way that is not in accordance with its intendedpurpose, but which may result from reasonably foreseeable human behaviour or interaction with other systems, including other AI systems;(14) ‘safety component’ means a component of a product or of an AI system which fulfils a safety function for that productor AI system, or the failure or malfunctioning of which endangers the health and safety of persons or property;(15) ‘instructions for use’ means the information provided by the provider to inform the deployer of, in particular, an AIsystem’s intended purpose and proper use;(16) ‘recall of an AI system’ means any measure aiming to achieve the return to the provider or taking out of service ordisabling the use of an AI system made available to deployers;(17) ‘withdrawal of an AI system’ means any measure aiming to prevent an AI system in the supply chain being madeavailable on the market;(18) ‘performance of an AI system’ means the ability of an AI system to achieve its intended purpose;(19) ‘notifying authority’ means the national authority responsible for setting up and carrying out the necessary proceduresfor the assessment, designation and notification of conformity assessment bodies and for their monitoring;(20) ‘conformity assessment’ means the process of demonstrating whether the requirements set out in Chapter III, Section 2relating to a high-risk AI system have been fulfilled;(21) ‘conformity assessment body’ means a body that performs third-party conformity assessment activities, includingtesting, certification and inspection;(22) ‘notified body’ means a conformity assessment body notified in accordance with this Regulation and other relevant Union harmonisation legislation;(23) ‘substantial modification’ means a change to an AI system after its placing on the market or putting into service whichis not foreseen or planned in the initial conformity assessment carried out by the provider and as a result of which thecompliance of the AI system with the requirements set out in Chapter III, Section 2 is affected or results ina modification to the intended purpose for which the AI system has been assessed;(24) ‘CE marking’ means a marking by which a provider indicates that an AI system is in conformity with the requirementsset out in Chapter III, Section 2 and other applicable Union harmonisation legislation providing for its affixing;(25) ‘post-market monitoring system’ means all activities carried out by providers of AI systems to collect and reviewexperience gained from the use of AI systems they place on the market or put into service for the purpose ofidentifying any need to immediately apply any necessary corrective or preventive actions;(27) ‘harmonised standard’ means a harmonised standard as defined in Article 2(1), point (c), of Regulation (EU)No 1025/2012;(28) ‘common specification’ means a set of technical specifications as defined in Article 2, point (4) of Regulation (EU)No 1025/2012, providing means to comply with certain requirements established under this Regulation;(29) ‘training data’ means data used for training an AI system through fitting its learnable parameters;(30) ‘validation data’ means data used for providing an evaluation of the trained AI system and for tuning its non-learnableparameters and its learning process in order, inter alia, to prevent underfitting or overfitting;(31) ‘validation data set’ means a separate data set or part of the training data set, either as a fixed or variable split;(32) ‘testing data’ means data used for providing an independent evaluation of the AI system in order to confirm theexpected performance of that system before its placing on the market or putting into service;(33) ‘input data’ means data provided to or directly acquired by an AI system on the basis of which the system produces anoutput;(34) ‘biometric data’ means personal data resulting from specific technical processing relating to the physical, physiologicalor behavioural characteristics of a natural person, such as facial images or dactyloscopic data;(35) ‘biometric identification’ means the automated recognition of physical, physiological, behavioural, or psychologicalhuman features for the purpose of establishing the identity of a natural person by comparing biometric data of thatindividual to biometric data of individuals stored in a database;(36) ‘biometric verification’ means the automated, one-to-one verification, including authentication, of the identity ofnatural persons by comparing their biometric data to previously provided biometric data;(37) ‘special categories of personal data’ means the categories of personal data referred to in Article 9(1) of Regulation (EU)2016/679, Article 10 of Directive (EU) 2016/680 and Article 10(1) of Regulation (EU) 2018/1725;(38) ‘sensitive operational data’ means operational data related to activities of prevention, detection, investigation orprosecution of criminal offences, the disclosure of which could jeopardise the integrity of criminal proceedings;(39) ‘emotion recognition system’ means an AI system for the purpose of identifying or inferring emotions or intentions ofnatural persons on the basis of their biometric data;(40) ‘biometric categorisation system’ means an AI system for the purpose of assigning natural persons to specificcategories on the basis of their biometric data, unless it is ancillary to another commercial service and strictlynecessary for objective technical reasons;(41) ‘remote biometric identification system’ means an AI system for the purpose of identifying natural persons, withouttheir active involvement, typically at a distance through the comparison of a person’s biometric data with thebiometric data contained in a reference database;(42) ‘real-time remote biometric identification system’ means a remote biometric identification system, whereby thecapturing of biometric data, the comparison and the identification all occur without a significant delay, comprisingnot only instant identification, but also limited short delays in order to avoid circumvention;(43) ‘post-remote biometric identification system’ means a remote biometric identification system other than a real-timeremote biometric identification system;(44) ‘publicly accessible space’ means any publicly or privately owned physical place accessible to an undetermined number(45) ‘law enforcement authority’ means:(a) any public authority competent for the prevention, investigation, detection or prosecution of criminal offences orthe execution of criminal penalties, including the safeguarding against and the prevention of threats to publicsecurity; or(b) any other body or entity entrusted by Member State law to exercise public authority and public powers for thepurposes of the prevention, investigation, detection or prosecution of criminal offences or the execution ofcriminal penalties, including the safeguarding against and the prevention of threats to public security;(46) ‘law enforcement’ means activities carried out by law enforcement authorities or on their behalf for the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, includingsafeguarding against and preventing threats to public security;(47) ‘AI Office’ means the Commission’s function of contributing to the implementation, monitoring and supervision of AIsystems and general-purpose AI models, and AI governance, provided for in Commission Decision of 24 January2024; references in this Regulation to the AI Office shall be construed as references to the Commission;(48) ‘national competent authority’ means a notifying authority or a market surveillance authority; as regards AI systemsput into service or used by Union institutions, agencies, offices and bodies, references to national competentauthorities or market surveillance authorities in this Regulation shall be construed as references to the European Data Protection Supervisor;(49) ‘serious incident’ means an incident or malfunctioning of an AI system that directly or indirectly leads to any of thefollowing:(a) the death of a person, or serious harm to a person’s health;(b) a serious and irreversible disruption of the management or operation of critical infrastructure;(c) the infringement of obligations under Union law intended to protect fundamental rights;(d) serious harm to property or the environment;(50) ‘personal data’ means personal data as defined in Article 4, point (1), of Regulation (EU) 2016/679;(51) ‘non-personal data’ means data other than personal data as defined in Article 4, point (1), of Regulation (EU)2016/679;(52) ‘profiling’ means profiling as defined in Article 4, point (4), of Regulation (EU) 2016/679;(53) ‘real-world testing plan’ means a document that describes the objectives, methodology, geographical, population andtemporal scope, monitoring, organisation and conduct of testing in real-world conditions;(54) ‘sandbox plan’ means a document agreed between the participating provider and the competent authority describingthe objectives, conditions, timeframe, methodology and requirements for the activities carried out within the sandbox;(55) ‘AI regulatory sandbox’ means a controlled framework set up by a competent authority which offers providers orprospective providers of AI systems the possibility to develop, train, validate and test, where appropriate in real-worldconditions, an innovative AI system, pursuant to a sandbox plan for a limited time under regulatory supervision;(56) ‘AI literacy’ means skills, knowledge and understanding that allow providers, deployers and affected persons, taking(57) ‘testing in real-world conditions’ means the temporary testing of an AI system for its intended purpose in real-worldconditions outside a laboratory or otherwise simulated environment, with a view to gathering reliable and robust dataand to assessing and verifying the conformity of the AI system with the requirements of this Regulation and it doesnot qualify as placing the AI system on the market or putting it into service within the meaning of this Regulation, provided that all the conditions laid down in Article 57 or 60 are fulfilled;(58) ‘subject’, for the purpose of real-world testing, means a natural person who participates in testing in real-worldconditions;(59) ‘informed consent’ means a subject’s freely given, specific, unambiguous and voluntary expression of his or herwillingness to participate in a particular testing in real-world conditions, after having been informed of all aspects ofthe testing that are relevant to the subject’s decision to participate;(60) ‘deep fake’ means AI-generated or manipulated image, audio or video content that resembles existing persons, objects, places, entities or events and would falsely appear to a person to be authentic or truthful;(61) ‘widespread infringement’ means any act or omission contrary to Union law protecting the interest of individuals, which:(a) has harmed or is likely to harm the collective interests of individuals residing in at least two Member States otherthan the Member State in which:(i) the act or omission originated or took place;(ii) the provider concerned, or, where applicable, its authorised representative is located or established; or(iii) the deployer is established, when the infringement is committed by the deployer;(b) has caused, causes or is likely to cause harm to the collective interests of individuals and has common features, including the same unlawful practice or the same interest being infringed, and is occurring concurrently, committed by the same operator, in at least three Member States;(62) ‘critical infrastructure’ means critical infrastructure as defined in Article 2, point (4), of Directive (EU) 2022/2557;(63) ‘general-purpose AI model’ means an AI model, including where such an AI model is trained with a large amount ofdata using self-supervision at scale, that displays significant generality and is capable of competently performinga wide range of distinct tasks regardless of the way the model is placed on the market and that can be integrated intoa variety of downstream systems or applications, except AI models that are used for research, development orprototyping activities before they are placed on the market;(64) ‘high-impact capabilities’ means capabilities that match or exceed the capabilities recorded in the most advancedgeneral-purpose AI models;(65) ‘systemic risk’ means a risk that is specific to the high-impact capabilities of general-purpose AI models, havinga significant impact on the Union market due to their reach, or due to actual or reasonably foreseeable negative effectson public health, safety, public security, fundamental rights, or the society as a whole, that can be propagated at scaleacross the value chain;(66) ‘general-purpose AI system’ means an AI system which is based on a general-purpose AI model and which has thecapability to serve a variety of purposes, both for direct use as well as for integration in other AI systems;(67) ‘floating-point operation’ means any mathematical operation or assignment involving floating-point numbers, whichare a subset of the real numbers typically represented on computers by an integer of fixed precision scaled by aninteger exponent of a fixed base;(68) ‘downstream provider’ means a provider of an AI system, including a general-purpose AI system, which integrates an Article 4AI literacy Providers and deployers of AI systems shall take measures to ensure, to their best extent, a sufficient level of AI literacy oftheir staff and other persons dealing with the operation and use of AI systems on their behalf, taking into account theirtechnical knowledge, experience, education and training and the context the AI systems are to be used in, and consideringthe persons or groups of persons on whom the AI systems are to be used. CHAPTER IIPROHIBITED AI PRACTICESArticle 5Prohibited AI practices1. The following AI practices shall be prohibited:(a) the placing on the market, the putting into service or the use of an AI system that deploys subliminal techniques beyonda person’s consciousness or purposefully manipulative or deceptive techniques, with the objective, or the effect ofmaterially distorting the behaviour of a person or a group of persons by appreciably impairing their ability to make aninformed decision, thereby causing them to take a decision that they would not have otherwise taken in a manner thatcauses or is reasonably likely to cause that person, another person or group of persons significant harm;(b) the placing on the market, the putting into service or the use of an AI system that exploits any of the vulnerabilities ofa natural person or a specific group of persons due to their age, disability or a specific social or economic situation, withthe objective, or the effect, of materially distorting the behaviour of that person or a person belonging to that group ina manner that causes or is reasonably likely to cause that person or another person significant harm;(c) the placing on the market, the putting into service or the use of AI systems for the evaluation or classification of naturalpersons or groups of persons over a certain period of time based on their social behaviour or known, inferred orpredicted personal or personality characteristics, with the social score leading to either or both of the following:(i) detrimental or unfavourable treatment of certain natural persons or groups of persons in social contexts that areunrelated to the contexts in which the data was originally generated or collected;(ii) detrimental or unfavourable treatment of certain natural persons or groups of persons that is unjustified ordisproportionate to their social behaviour or its gravity;(d) the placing on the market, the putting into service for this specific purpose, or the use of an AI system for making riskassessments of natural persons in order to assess or predict the risk of a natural person committing a criminal offence, based solely on the profiling of a natural person or on assessing their personality traits and characteristics; thisprohibition shall not apply to AI systems used to support the human assessment of the involvement of a person ina criminal activity, which is already based on objective and verifiable facts directly linked to a criminal activity;(e) the placing on the market, the putting into service for this specific purpose, or the use of AI systems that create orexpand facial recognition databases through the untargeted scraping of facial images from the internet or CCTV footage;(f) the placing on the market, the putting into service for this specific purpose, or the use of AI systems to infer emotions(g) the placing on the market, the putting into service for this specific purpose, or the use of biometric categorisationsystems that categorise individually natural persons based on their biometric data to deduce or infer their race, politicalopinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation; this prohibition doesnot cover any labelling or filtering of lawfully acquired biometric datasets, such as images, based on biometric data orcategorizing of biometric data in the area of law enforcement;(h) the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purposes of lawenforcement, unless and in so far as such use is strictly necessary for one of the following objectives:(i) the targeted search for specific victims of abduction, trafficking in human beings or sexual exploitation of humanbeings, as well as the search for missing persons;(ii) the prevention of a specific, substantial and imminent threat to the life or physical safety of natural persons ora genuine and present or genuine and foreseeable threat of a terrorist attack;(iii) the localisation or identification of a person suspected of having committed a criminal offence, for the purpose ofconducting a criminal investigation or prosecution or executing a criminal penalty for offences referred to in Annex II and punishable in the Member State concerned by a custodial sentence or a detention order fora maximum period of at least four years. Point (h) of the first subparagraph is without prejudice to Article 9 of Regulation (EU) 2016/679 for the processing ofbiometric data for purposes other than law enforcement.2. The use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purposes of lawenforcement for any of the objectives referred to in paragraph 1, first subparagraph, point (h), shall be deployed for thepurposes set out in that point only to confirm the identity of the specifically targeted individual, and it shall take intoaccount the following elements:(a) the nature of the situation giving rise to the possible use, in particular the seriousness, probability and scale of the harmthat would be caused if the system were not used;(b) the consequences of the use of the system for the rights and freedoms of all persons concerned, in particular theseriousness, probability and scale of those consequences. In addition, the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for the purposes oflaw enforcement for any of the objectives referred to in paragraph 1, first subparagraph, point (h), of this Article shallcomply with necessary and proportionate safeguards and conditions in relation to the use in accordance with the nationallaw authorising the use thereof, in particular as regards the temporal, geographic and personal limitations. The use of the‘real-time’ remote biometric identification system in publicly accessible spaces shall be authorised only if the lawenforcement authority has completed a fundamental rights impact assessment as provided for in Article 27 and hasregistered the system in the EU database according to Article 49. However, in duly justified cases of urgency, the use of suchsystems may be commenced without the registration in the EU database, provided that such registration is completedwithout undue delay.3. For the purposes of paragraph 1, first subparagraph, point (h) and paragraph 2, each use for the purposes of lawenforcement of a ‘real-time’ remote biometric identification system in publicly accessible spaces shall be subject to a priorauthorisation granted by a judicial authority or an independent administrative authority whose decision is binding of the Member State in which the use is to take place, issued upon a reasoned request and in accordance with the detailed rules ofnational law referred to in paragraph 5. However, in a duly justified situation of urgency, the use of such system may becommenced without an authorisation provided that such authorisation is requested without undue delay, at the latestwithin 24 hours. If such authorisation is rejected, the use shall be stopped with immediate effect and all the data, as well asthe results and outputs of that use shall be immediately discarded and deleted. The competent judicial authority or an independent administrative authority whose decision is binding shall grant theobjectives specified in paragraph 1, first subparagraph, point (h), as identified in the request and, in particular, remainslimited to what is strictly necessary concerning the period of time as well as the geographic and personal scope. In decidingon the request, that authority shall take into account the elements referred to in paragraph 2. No decision that produces anadverse legal effect on a person may be taken based solely on the output of the ‘real-time’ remote biometric identificationsystem.4. Without prejudice to paragraph 3, each use of a ‘real-time’ remote biometric identification system in publiclyaccessible spaces for law enforcement purposes shall be notified to the relevant market surveillance authority and thenational data protection authority in accordance with the national rules referred to in paragraph 5. The notification shall, asa minimum, contain the information specified under paragraph 6 and shall not include sensitive operational data.5. A Member State may decide to provide for the possibility to fully or partially authorise the use of ‘real-time’ remotebiometric identification systems in publicly accessible spaces for the purposes of law enforcement within the limits andunder the conditions listed in paragraph 1, first subparagraph, point (h), and paragraphs 2 and 3. Member States concernedshall lay down in their national law the necessary detailed rules for the request, issuance and exercise of, as well assupervision and reporting relating to, the authorisations referred to in paragraph 3. Those rules shall also specify in respectof which of the objectives listed in paragraph 1, first subparagraph, point (h), including which of the criminal offencesreferred to in point (h)(iii) thereof, the competent authorities may be authorised to use those systems for the purposes oflaw enforcement. Member States shall notify those rules to the Commission at the latest 30 days following the adoptionthereof. Member States may introduce, in accordance with Union law, more restrictive laws on the use of remote biometricidentification systems.6. National market surveillance authorities and the national data protection authorities of Member States that have beennotified of the use of ‘real-time’ remote biometric identification systems in publicly accessible spaces for law enforcementpurposes pursuant to paragraph 4 shall submit to the Commission annual reports on such use. For that purpose, the Commission shall provide Member States and national market surveillance and data protection authorities with a template, including information on the number of the decisions taken by competent judicial authorities or an independentadministrative authority whose decision is binding upon requests for authorisations in accordance with paragraph 3 andtheir result.7. The Commission shall publish annual reports on the use of real-time remote biometric identification systems inpublicly accessible spaces for law enforcement purposes, based on aggregated data in Member States on the basis of theannual reports referred to in paragraph 6. Those annual reports shall not include sensitive operational data of the relatedlaw enforcement activities.8. This Article shall not affect the prohibitions that apply where an AI practice infringes other Union law. CHAPTER IIIHIGH-RISK AI SYSTEMSSECTION 1Classification of AI systems as high-risk Article 6Classification rules for high-risk AI systems1. Irrespective of whether an AI system is placed on the market or put into service independently of the productsreferred to in points (a) and (b), that AI system shall be considered to be high-risk where both of the following conditionsare fulfilled:(a) the AI system is intended to be used as a safety component of a product, or the AI system is itself a product, covered bythe Union harmonisation legislation listed in Annex I;(b) the product whose safety component pursuant to point (a) is the AI system, or the AI system itself as a product, isrequired to undergo a third-party conformity assessment, with a view to the placing on the market or the putting intoservice of that product pursuant to the Union harmonisation legislation listed in Annex I.3. By derogation from paragraph 2, an AI system referred to in Annex III shall not be considered to be high-risk where itdoes not pose a significant risk of harm to the health, safety or fundamental rights of natural persons, including by notmaterially influencing the outcome of decision making. The first subparagraph shall apply where any of the following conditions is fulfilled:(a) the AI system is intended to perform a narrow procedural task;(b) the AI system is intended to improve the result of a previously completed human activity;(c) the AI system is intended to detect decision-making patterns or deviations from prior decision-making patterns and isnot meant to replace or influence the previously completed human assessment, without proper human review; or(d) the AI system is intended to perform a preparatory task to an assessment relevant for the purposes of the use caseslisted in Annex III. Notwithstanding the first subparagraph, an AI system referred to in Annex III shall always be considered to be high-riskwhere the AI system performs profiling of natural persons.4. A provider who considers that an AI system referred to in Annex III is not high-risk shall document its assessmentbefore that system is placed on the market or put into service. Such provider shall be subject to the registration obligationset out in Article 49(2). Upon request of national competent authorities, the provider shall provide the documentation ofthe assessment.5. The Commission shall, after consulting the European Artificial Intelligence Board (the ‘Board’), and no later than2 February 2026, provide guidelines specifying the practical implementation of this Article in line with Article 96 togetherwith a comprehensive list of practical examples of use cases of AI systems that are high-risk and not high-risk.6. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend paragraph 3, second subparagraph, of this Article by adding new conditions to those laid down therein, or by modifying them, wherethere is concrete and reliable evidence of the existence of AI systems that fall under the scope of Annex III, but do not posea significant risk of harm to the health, safety or fundamental rights of natural persons.7. The Commission shall adopt delegated acts in accordance with Article 97 in order to amend paragraph 3, secondsubparagraph, of this Article by deleting any of the conditions laid down therein, where there is concrete and reliableevidence that this is necessary to maintain the level of protection of health, safety and fundamental rights provided for bythis Regulation.8. Any amendment to the conditions laid down in paragraph 3, second subparagraph, adopted in accordance withparagraphs 6 and 7 of this Article shall not decrease the overall level of protection of health, safety and fundamental rightsprovided for by this Regulation and shall ensure consistency with the delegated acts adopted pursuant to Article 7(1), andtake account of market and technological developments. Article 7Amendments to Annex III1. The Commission is empowered to adopt delegated acts in accordance with Article 97 to amend Annex III by addingor modifying use-cases of high-risk AI systems where both of the following conditions are fulfilled:(a) the AI systems are intended to be used in any of the areas listed in Annex III;(b) the AI systems pose a risk of harm to health and safety, or an adverse impact on fundamental rights, and that risk is2. When assessing the condition under paragraph 1, point (b), the Commission shall take into account the followingcriteria:(a) the intended purpose of the AI system;(b) the extent to which an AI system has been used or is likely to be used;(c) the nature and amount of the data processed and used by the AI system, in particular whether special categories ofpersonal data are processed;(d) the extent to which the AI system acts autonomously and the possibility for a human to override a decision orrecommendations that may lead to potential harm;(e) the extent to which the use of an AI system has already caused harm to health and safety, has had an adverse impact onfundamental rights or has given rise to significant concerns in relation to the likelihood of such harm or adverse impact, as demonstrated, for example, by reports or documented allegations submitted to national competent authorities or byother reports, as appropriate;(f) the potential extent of such harm or such adverse impact, in particular in terms of its intensity and its ability to affectmultiple persons or to disproportionately affect a particular group of persons;(g) the extent to which persons who are potentially harmed or suffer an adverse impact are dependent on the outcomeproduced with an AI system, in particular because for practical or legal reasons it is not reasonably possible to opt-outfrom that outcome;(h) the extent to which there is an imbalance of power, or the persons who are potentially harmed or suffer an adverseimpact are in a vulnerable position in relation to the deployer of an AI system, in particular due to status, authority, knowledge, economic or social circumstances, or age;(i) the extent to which the outcome produced involving an AI system is easily corrigible or reversible, taking into accountthe technical solutions available to correct or reverse it, whereby outcomes having an adverse impact on health, safety orfundamental rights, shall not be considered to be easily corrigible or reversible;(j) the magnitude and likelihood of benefit of the deployment of the AI system for individuals, groups, or society at large, including possible improvements in product safety;(k) the extent to which existing Union law provides for:(i) effective measures of redress in relation to the risks posed by an AI system, with the exclusion of claims fordamages;(ii) effective measures to prevent or substantially minimise those risks.3. The Commission is empowered to adopt delegated acts in accordance with Article 97 to amend the list in Annex IIIby removing high-risk AI systems where both of the following conditions are fulfilled:(a) the high-risk AI system concerned no longer poses any significant risks to fundamental rights, health or safety, takinginto account the criteria listed in paragraph 2;(b) the deletion does not decrease the overall level of protection of health, safety and fundamental rights under Union law. SECTION 2Requirements for high-risk AI systems Article 8Compliance with the requirements1. High-risk AI systems shall comply with the requirements laid down in this Section, taking into account their intended2. Where a product contains an AI system, to which the requirements of this Regulation as well as requirements of the Union harmonisation legislation listed in Section A of Annex I apply, providers shall be responsible for ensuring that theirproduct is fully compliant with all applicable requirements under applicable Union harmonisation legislation. In ensuringthe compliance of high-risk AI systems referred to in paragraph 1 with the requirements set out in this Section, and in orderto ensure consistency, avoid duplication and minimise additional burdens, providers shall have a choice of integrating, asappropriate, the necessary testing and reporting processes, information and documentation they provide with regard totheir product into documentation and procedures that already exist and are required under the Union harmonisationlegislation listed in Section A of Annex I. Article 9Risk management system1. A risk management system shall be established, implemented, documented and maintained in relation to high-risk AIsystems.2. The risk management system shall be understood as a continuous iterative process planned and run throughout theentire lifecycle of a high-risk AI system, requiring regular systematic review and updating. It shall comprise the followingsteps:(a) the identification and analysis of the known and the reasonably foreseeable risks that the high-risk AI system can poseto health, safety or fundamental rights when the high-risk AI system is used in accordance with its intended purpose;(b) the estimation and evaluation of the risks that may emerge when the high-risk AI system is used in accordance with itsintended purpose, and under conditions of reasonably foreseeable misuse;(c) the evaluation of other risks possibly arising, based on the analysis of data gathered from the post-market monitoringsystem referred to in Article 72;(d) the adoption of appropriate and targeted risk management measures designed to address the risks identified pursuant topoint (a).3. The risks referred to in this Article shall concern only those which may be reasonably mitigated or eliminated throughthe development or design of the high-risk AI system, or the provision of adequate technical information.4. The risk management measures referred to in paragraph 2, point (d), shall give due consideration to the effects andpossible interaction resulting from the combined application of the requirements set out in this Section, with a view tominimising risks more effectively while achieving an appropriate balance in implementing the measures to fulfil thoserequirements.5. The risk management measures referred to in paragraph 2, point (d), shall be such that the relevant residual riskassociated with each hazard, as well as the overall residual risk of the high-risk AI systems is judged to be acceptable. In identifying the most appropriate risk management measures, the following shall be ensured:(a) elimination or reduction of risks identified and evaluated pursuant to paragraph 2 in as far as technically feasiblethrough adequate design and development of the high-risk AI system;(b) where appropriate, implementation of adequate mitigation and control measures addressing risks that cannot beeliminated;(c) provision of information required pursuant to Article 13 and, where appropriate, training to deployers. With a view to eliminating or reducing risks related to the use of the high-risk AI system, due consideration shall be given6. High-risk AI systems shall be tested for the purpose of identifying the most appropriate and targeted risk managementmeasures. Testing shall ensure that high-risk AI systems perform consistently for their intended purpose and that they are incompliance with the requirements set out in this Section.7. Testing procedures may include testing in real-world conditions in accordance with Article 60.8. The testing of high-risk AI systems shall be performed, as appropriate, at any time throughout the developmentprocess, and, in any event, prior to their being placed on the market or put into service. Testing shall be carried out againstprior defined metrics and probabilistic thresholds that are appropriate to the intended purpose of the high-risk AI system.9. When implementing the risk management system as provided for in paragraphs 1 to 7, providers shall giveconsideration to whether in view of its intended purpose the high-risk AI system is likely to have an adverse impact onpersons under the age of 18 and, as appropriate, other vulnerable groups.10. For providers of high-risk AI systems that are subject to requirements regarding internal risk management processesunder other relevant provisions of Union law, the aspects provided in paragraphs 1 to 9 may be part of, or combined with, the risk management procedures established pursuant to that law. Article 10Data and data governance1. High-risk AI systems which make use of techniques involving the training of AI models with data shall be developedon the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5whenever such data sets are used.2. Training, validation and testing data sets shall be subject to data governance and management practices appropriatefor the intended purpose of the high-risk AI system. Those practices shall concern in particular:(a) the relevant design choices;(b) data collection processes and the origin of data, and in the case of personal data, the original purpose of the datacollection;(c) relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment andaggregation;(d) the formulation of assumptions, in particular with respect to the information that the data are supposed to measure andrepresent;(e) an assessment of the availability, quantity and suitability of the data sets that are needed;(f) examination in view of possible biases that are likely to affect the health and safety of persons, have a negative impacton fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influenceinputs for future operations;(g) appropriate measures to detect, prevent and mitigate possible biases identified according to point (f);(h) the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how thosegaps and shortcomings can be addressed.3. Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible, free of errors and complete in view of the intended purpose. They shall have the appropriate statistical properties, including, where applicable, as regards the persons or groups of persons in relation to whom the high-risk AI system is intended to beused. Those characteristics of the data sets may be met at the level of individual data sets or at the level of a combinationthereof.4. Data sets shall take into account, to the extent required by the intended purpose, the characteristics or elements that5. To the extent that it is strictly necessary for the purpose of ensuring bias detection and correction in relation to thehigh-risk AI systems in accordance with paragraph (2), points (f) and (g) of this Article, the providers of such systems mayexceptionally process special categories of personal data, subject to appropriate safeguards for the fundamental rights andfreedoms of natural persons. In addition to the provisions set out in Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, all the following conditions must be met in order for such processing to occur:(a) the bias detection and correction cannot be effectively fulfilled by processing other data, including synthetic oranonymised data;(b) the special categories of personal data are subject to technical limitations on the re-use of the personal data, andstate-of-the-art security and privacy-preserving measures, including pseudonymisation;(c) the special categories of personal data are subject to measures to ensure that the personal data processed are secured, protected, subject to suitable safeguards, including strict controls and documentation of the access, to avoid misuse andensure that only authorised persons have access to those personal data with appropriate confidentiality obligations;(d) the special categories of personal data are not to be transmitted, transferred or otherwise accessed by other parties;(e) the special categories of personal data are deleted once the bias has been corrected or the personal data has reached theend of its retention period, whichever comes first;(f) the records of processing activities pursuant to Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU)2016/680 include the reasons why the processing of special categories of personal data was strictly necessary to detectand correct biases, and why that objective could not be achieved by processing other data.6. For the development of high-risk AI systems not using techniques involving the training of AI models, paragraphs 2to 5 apply only to the testing data sets. Article 11Technical documentation1. The technical documentation of a high-risk AI system shall be drawn up before that system is placed on the market orput into service and shall be kept up-to date. The technical documentation shall be drawn up in such a way as to demonstrate that the high-risk AI system complies withthe requirements set out in this Section and to provide national competent authorities and notified bodies with thenecessary information in a clear and comprehensive form to assess the compliance of the AI system with thoserequirements. It shall contain, at a minimum, the elements set out in Annex IV. SMEs, including start-ups, may provide theelements of the technical documentation specified in Annex IV in a simplified manner. To that end, the Commission shallestablish a simplified technical documentation form targeted at the needs of small and microenterprises. Where an SME, including a start-up, opts to provide the information required in Annex IV in a simplified manner, it shall use the formreferred to in this paragraph. Notified bodies shall accept the form for the purposes of the conformity assessment.2. Where a high-risk AI system related to a product covered by the Union harmonisation legislation listed in Section A of Annex I is placed on the market or put into service, a single set of technical documentation shall be drawn upcontaining all the information set out in paragraph 1, as well as the information required under those legal acts.3. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend Annex IV, Article 12Record-keeping1. High-risk AI systems shall technically allow for the automatic recording of events (logs) over the lifetime of thesystem.2. In order to ensure a level of traceability of the functioning of a high-risk AI system that is appropriate to the intendedpurpose of the system, logging capabilities shall enable the recording of events relevant for:(a) identifying situations that may result in the high-risk AI system presenting a risk within the meaning of Article 79(1) orin a substantial modification;(b) facilitating the post-market monitoring referred to in Article 72; and(c) monitoring the operation of high-risk AI systems referred to in Article 26(5).3. For high-risk AI systems referred to in point 1 (a), of Annex III, the logging capabilities shall provide, at a minimum:(a) recording of the period of each use of the system (start date and time and end date and time of each use);(b) the reference database against which input data has been checked by the system;(c) the input data for which the search has led to a match;(d) the identification of the natural persons involved in the verification of the results, as referred to in Article 14(5). Article 13Transparency and provision of information to deployers1. High-risk AI systems shall be designed and developed in such a way as to ensure that their operation is sufficientlytransparent to enable deployers to interpret a system’s output and use it appropriately. An appropriate type and degree oftransparency shall be ensured with a view to achieving compliance with the relevant obligations of the provider anddeployerset out in Section 3.2. High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise thatinclude concise, complete, correct and clear information that is relevant, accessible and comprehensible to deployers.3. The instructions for use shall contain at least the following information:(a) the identity and the contact details of the provider and, where applicable, of its authorised representative;(b) the characteristics, capabilities and limitations of performance of the high-risk AI system, including:(i) its intended purpose;(ii) the level of accuracy, including its metrics, robustness and cybersecurity referred to in Article 15 against which thehigh-risk AI system has been tested and validated and which can be expected, and any known and foreseeablecircumstances that may have an impact on that expected level of accuracy, robustness and cybersecurity;(iii) any known or foreseeable circumstance, related to the use of the high-risk AI system in accordance with itsintended purpose or under conditions of reasonably foreseeable misuse, which may lead to risks to the health andsafety or fundamental rights referred to in Article 9(2);(v) when appropriate, its performance regarding specific persons or groups of persons on which the system isintended to be used;(vi) when appropriate, specifications for the input data, or any other relevant information in terms of the training, validation and testing data sets used, taking into account the intended purpose of the high-risk AI system;(vii) where applicable, information to enable deployers to interpret the output of the high-risk AI system and use itappropriately;(c) the changes to the high-risk AI system and its performance which have been pre-determined by the provider at themoment of the initial conformity assessment, if any;(d) the human oversight measures referred to in Article 14, including the technical measures put in place to facilitate theinterpretation of the outputs of the high-risk AI systems by the deployers;(e) the computational and hardware resources needed, the expected lifetime of the high-risk AI system and any necessarymaintenance and care measures, including their frequency, to ensure the proper functioning of that AI system, includingas regards software updates;(f) where relevant, a description of the mechanisms included within the high-risk AI system that allows deployers toproperly collect, store and interpret the logs in accordance with Article 12. Article 14Human oversight1. High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machineinterface tools, that they can be effectively overseen by natural persons during the period in which they are in use.2. Human oversight shall aim to prevent or minimise the risks to health, safety or fundamental rights that may emergewhen a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeablemisuse, in particular where such risks persist despite the application of other requirements set out in this Section.3. The oversight measures shall be commensurate with the risks, level of autonomy and context of use of the high-risk AI system, and shall be ensured through either one or both of the following types of measures:(a) measures identified and built, when technically feasible, into the high-risk AI system by the provider before it is placedon the market or put into service;(b) measures identified by the provider before placing the high-risk AI system on the market or putting it into service andthat are appropriate to be implemented by the deployer.4. For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be provided to the deployer insuch a way that natural persons to whom human oversight is assigned are enabled, as appropriate and proportionate:(a) to properly understand the relevant capacities and limitations of the high-risk AI system and be able to duly monitor itsoperation, including in view of detecting and addressing anomalies, dysfunctions and unexpected performance;(b) to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system (automation bias), in particular for high-risk AI systems used to provide information or recommendations fordecisions to be taken by natural persons;(d) to decide, in any particular situation, not to use the high-risk AI system or to otherwise disregard, override or reversethe output of the high-risk AI system;(e) to intervene in the operation of the high-risk AI system or interrupt the system through a ‘stop’ button or a similarprocedure that allows the system to come to a halt in a safe state.5. For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 of this Articleshall be such as to ensure that, in addition, no action or decision is taken by the deployer on the basis of the identificationresulting from the system unless that identification has been separately verified and confirmed by at least two naturalpersons with the necessary competence, training and authority. The requirement for a separate verification by at least two natural persons shall not apply to high-risk AI systems used forthe purposes of law enforcement, migration, border control or asylum, where Union or national law considers theapplication of this requirement to be disproportionate. Article 15Accuracy, robustness and cybersecurity1. High-risk AI systems shall be designed and developed in such a way that they achieve an appropriate level of accuracy, robustness, and cybersecurity, and that they perform consistently in those respects throughout their lifecycle.2. To address the technical aspects of how to measure the appropriate levels of accuracy and robustness set out inparagraph 1 and any other relevant performance metrics, the Commission shall, in cooperation with relevant stakeholdersand organisations such as metrology and benchmarking authorities, encourage, as appropriate, the development ofbenchmarks and measurement methodologies.3. The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be declared in the accompanyinginstructions of use.4. High-risk AI systems shall be as resilient as possible regarding errors, faults or inconsistencies that may occur withinthe system or the environment in which the system operates, in particular due to their interaction with natural persons orother systems. Technical and organisational measures shall be taken in this regard. The robustness of high-risk AI systems may be achieved through technical redundancy solutions, which may includebackup or fail-safe plans. High-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in sucha way as to eliminate or reduce as far as possible the risk of possibly biased outputs influencing input for future operations(feedback loops), and as to ensure that any such feedback loops are duly addressed with appropriate mitigation measures.5. High-risk AI systems shall be resilient against attempts by unauthorised third parties to alter their use, outputs orperformance by exploiting system vulnerabilities. The technical solutions aiming to ensure the cybersecurity of high-risk AI systems shall be appropriate to the relevantcircumstances and the risks. The technical solutions to address AI specific vulnerabilities shall include, where appropriate, measures to prevent, detect, respond to, resolve and control for attacks trying to manipulate the training data set (data poisoning), or pre-trained SECTION 3Obligations of providers and deployers of high-risk AI systems and other parties Article 16Obligations of providers of high-risk AI systems Providers of high-risk AI systems shall:(a) ensure that their high-risk AI systems are compliant with the requirements set out in Section 2;(b) indicate on the high-risk AI system or, where that is not possible, on its packaging or its accompanying documentation, as applicable, their name, registered trade name or registered trade mark, the address at which they can be contacted;(c) have a quality management system in place which complies with Article 17;(d) keep the documentation referred to in Article 18;(e) when under their control, keep the logs automatically generated by their high-risk AI systems as referred to in Article 19;(f) ensure that the high-risk AI system undergoes the relevant conformity assessment procedure as referred to in Article 43, prior to its being placed on the market or put into service;(g) draw up an EU declaration of conformity in accordance with Article 47;(h) affix the CE marking to the high-risk AI system or, where that is not possible, on its packaging or its accompanyingdocumentation, to indicate conformity with this Regulation, in accordance with Article 48;(i) comply with the registration obligations referred to in Article 49(1);(j) take the necessary corrective actions and provide information as required in Article 20;(k) upon a reasoned request of a national competent authority, demonstrate the conformity of the high-risk AI system withthe requirements set out in Section 2;(l) ensure that the high-risk AI system complies with accessibility requirements in accordance with Directives (EU)2016/2102 and (EU) 2019/882. Article 17Quality management system1. Providers of high-risk AI systems shall put a quality management system in place that ensures compliance with this Regulation. That system shall be documented in a systematic and orderly manner in the form of written policies, proceduresand instructions, and shall include at least the following aspects:(a) a strategy for regulatory compliance, including compliance with conformity assessment procedures and procedures forthe management of modifications to the high-risk AI system;(b) techniques, procedures and systematic actions to be used for the design, design control and design verification of thehigh-risk AI system;(c) techniques, procedures and systematic actions to be used for the development, quality control and quality assurance ofthe high-risk AI system;(e) technical specifications, including standards, to be applied and, where the relevant harmonised standards are notapplied in full or do not cover all of the relevant requirements set out in Section 2, the means to be used to ensure thatthe high-risk AI system complies with those requirements;(f) systems and procedures for data management, including data acquisition, data collection, data analysis, data labelling, data storage, data filtration, data mining, data aggregation, data retention and any other operation regarding the datathat is performed before and for the purpose of the placing on the market or the putting into service of high-risk AIsystems;(g) the risk management system referred to in Article 9;(h) the setting-up, implementation and maintenance of a post-market monitoring system, in accordance with Article 72;(i) procedures related to the reporting of a serious incident in accordance with Article 73;(j) the handling of communication with national competent authorities, other relevant authorities, including thoseproviding or supporting the access to data, notified bodies, other operators, customers or other interested parties;(k) systems and procedures for record-keeping of all relevant documentation and information;(l) resource management, including security-of-supply related measures;(m) an accountability framework setting out the responsibilities of the management and other staff with regard to all theaspects listed in this paragraph.2. The implementation of the aspects referred to in paragraph 1 shall be proportionate to the size of the provider’sorganisation. Providers shall, in any event, respect the degree of rigour and the level of protection required to ensure thecompliance of their high-risk AI systems with this Regulation.3. Providers of high-risk AI systems that are subject to obligations regarding quality management systems or anequivalent function under relevant sectoral Union law may include the aspects listed in paragraph 1 as part of the qualitymanagement systems pursuant to that law.4. For providers that are financial institutions subject to requirements regarding their internal governance, arrangementsor processes under Union financial services law, the obligation to put in place a quality management system, with theexception of paragraph 1, points (g), (h) and (i) of this Article, shall be deemed to be fulfilled by complying with the rules oninternal governance arrangements or processes pursuant to the relevant Union financial services law. To that end, anyharmonised standards referred to in Article 40 shall be taken into account. Article 18Documentation keeping1. The provider shall, for a period ending 10 years after the high-risk AI system has been placed on the market or putinto service, keep at the disposal of the national competent authorities:(a) the technical documentation referred to in Article 11;(b) the documentation concerning the quality management system referred to in Article 17;(c) the documentation concerning the changes approved by notified bodies, where applicable;(d) the decisions and other documents issued by the notified bodies, where applicable;2. Each Member State shall determine conditions under which the documentation referred to in paragraph 1 remains atthe disposal of the national competent authorities for the period indicated in that paragraph for the cases when a provideror its authorised representative established on its territory goes bankrupt or ceases its activity prior to the end of thatperiod.3. Providers that are financial institutions subject to requirements regarding their internal governance, arrangements orprocesses under Union financial services law shall maintain the technical documentation as part of the documentation keptunder the relevant Union financial services law. Article 19Automatically generated logs1. Providers of high-risk AI systems shall keep the logs referred to in Article 12(1), automatically generated by theirhigh-risk AI systems, to the extent such logs are under their control. Without prejudice to applicable Union or national law, the logs shall be kept for a period appropriate to the intended purpose of the high-risk AI system, of at least six months, unless provided otherwise in the applicable Union or national law, in particular in Union law on the protection of personaldata.2. Providers that are financial institutions subject to requirements regarding their internal governance, arrangements orprocesses under Union financial services law shall maintain the logs automatically generated by their high-risk AI systemsas part of the documentation kept under the relevant financial services law. Article 20Corrective actions and duty of information1. Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system that they haveplaced on the market or put into service is not in conformity with this Regulation shall immediately take the necessarycorrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate. They shallinform the distributors of the high-risk AI system concerned and, where applicable, the deployers, the authorisedrepresentative and importers accordingly.2. Where the high-risk AI system presents a risk within the meaning of Article 79(1) and the provider becomes aware ofthat risk, it shall immediately investigate the causes, in collaboration with the reporting deployer, where applicable, andinform the market surveillance authorities competent for the high-risk AI system concerned and, where applicable, thenotified body that issued a certificate for that high-risk AI system in accordance with Article 44, in particular, of the natureof the non-compliance and of any relevant corrective action taken. Article 21Cooperation with competent authorities1. Providers of high-risk AI systems shall, upon a reasoned request by a competent authority, provide that authority allthe information and documentation necessary to demonstrate the conformity of the high-risk AI system with therequirements set out in Section 2, in a language which can be easily understood by the authority in one of the officiallanguages of the institutions of the Union as indicated by the Member State concerned.2. Upon a reasoned request by a competent authority, providers shall also give the requesting competent authority, asapplicable, access to the automatically generated logs of the high-risk AI system referred to in Article 12(1), to the extentsuch logs are under their control. Article 22Authorised representatives of providers of high-risk AI systems1. Prior to making their high-risk AI systems available on the Union market, providers established in third countriesshall, by written mandate, appoint an authorised representative which is established in the Union.2. The provider shall enable its authorised representative to perform the tasks specified in the mandate received from theprovider.3. The authorised representative shall perform the tasks specified in the mandate received from the provider. It shallprovide a copy of the mandate to the market surveillance authorities upon request, in one of the official languages of theinstitutions of the Union, as indicated by the competent authority. For the purposes of this Regulation, the mandate shallempower the authorised representative to carry out the following tasks:(a) verify that the EU declaration of conformity referred to in Article 47 and the technical documentation referred to in Article 11 have been drawn up and that an appropriate conformity assessment procedure has been carried out by theprovider;(b) keep at the disposal of the competent authorities and national authorities or bodies referred to in Article 74(10), fora period of 10 years after the high-risk AI system has been placed on the market or put into service, the contact detailsof the provider that appointed the authorised representative, a copy of the EU declaration of conformity referred to in Article 47, the technical documentation and, if applicable, the certificate issued by the notified body;(c) provide a competent authority, upon a reasoned request, with all the information and documentation, including thatreferred to in point (b) of this subparagraph, necessary to demonstrate the conformity of a high-risk AI system with therequirements set out in Section 2, including access to the logs, as referred to in Article 12(1), automatically generated bythe high-risk AI system, to the extent such logs are under the control of the provider;(d) cooperate with competent authorities, upon a reasoned request, in any action the latter take in relation to the high-risk AI system, in particular to reduce and mitigate the risks posed by the high-risk AI system;(e) where applicable, comply with the registration obligations referred to in Article 49(1), or, if the registration is carriedout by the provider itself, ensure that the information referred to in point 3 of Section A of Annex VIII is correct. The mandate shall empower the authorised representative to be addressed, in addition to or instead of the provider, by thecompetent authorities, on all issues related to ensuring compliance with this Regulation.4. The authorised representative shall terminate the mandate if it considers or has reason to consider the provider to beacting contrary to its obligations pursuant to this Regulation. In such a case, it shall immediately inform the relevant marketsurveillance authority, as well as, where applicable, the relevant notified body, about the termination of the mandate and thereasons therefor. Article 23Obligations of importers1. Before placing a high-risk AI system on the market, importers shall ensure that the system is in conformity with this Regulation by verifying that:(a) the relevant conformity assessment procedure referred to in Article 43 has been carried out by the provider of thehigh-risk AI system;(b) the provider has drawn up the technical documentation in accordance with Article 11 and Annex IV;(c) the system bears the required CE marking and is accompanied by the EU declaration of conformity referred to in Article 47 and instructions for use;2. Where an importer has sufficient reason to consider that a high-risk AI system is not in conformity with this Regulation, or is falsified, or accompanied by falsified documentation, it shall not place the system on the market until it hasbeen brought into conformity. Where the high-risk AI system presents a risk within the meaning of Article 79(1), theimporter shall inform the provider of the system, the authorised representative and the market surveillance authorities tothat effect.3. Importers shall indicate their name, registered trade name or registered trade mark, and the address at which they canbe contacted on the high-risk AI system and on its packaging or its accompanying documentation, where applicable.4. Importers shall ensure that, while a high-risk AI system is under their responsibility, storage or transport conditions, where applicable, do not jeopardise its compliance with the requirements set out in Section 2.5. Importers shall keep, for a period of 10 years after the high-risk AI system has been placed on the market or put intoservice, a copy of the certificate issued by the notified body, where applicable, of the instructions for use, and of the EUdeclaration of conformity referred to in Article 47.6. Importers shall provide the relevant competent authorities, upon a reasoned request, with all the necessaryinformation and documentation, including that referred to in paragraph 5, to demonstrate the conformity of a high-risk AIsystem with the requirements set out in Section 2 in a language which can be easily understood by them. For this purpose, they shall also ensure that the technical documentation can be made available to those authorities.7. Importers shall cooperate with the relevant competent authorities in any action those authorities take in relation toa high-risk AI system placed on the market by the importers, in particular to reduce and mitigate the risks posed by it. Article 24Obligations of distributors1. Before making a high-risk AI system available on the market, distributors shall verify that it bears the required CEmarking, that it is accompanied by a copy of the EU declaration of conformity referred to in Article 47 and instructions foruse, and that the provider and the importer of that system, as applicable, have complied with their respective obligations aslaid down in Article 16, points (b) and (c) and Article 23(3).2. Where a distributor considers or has reason to consider, on the basis of the information in its possession, thata high-risk AI system is not in conformity with the requirements set out in Section 2, it shall not make the high-risk AIsystem available on the market until the system has been brought into conformity with those requirements. Furthermore, where the high-risk AI system presents a risk within the meaning of Article 79(1), the distributor shall inform the provideror the importer of the system, as applicable, to that effect.3. Distributors shall ensure that, while a high-risk AI system is under their responsibility, storage or transportconditions, where applicable, do not jeopardise the compliance of the system with the requirements set out in Section 2.4. A distributor that considers or has reason to consider, on the basis of the information in its possession, a high-risk AIsystem which it has made available on the market not to be in conformity with the requirements set out in Section 2, shalltake the corrective actions necessary to bring that system into conformity with those requirements, to withdraw it or recallit, or shall ensure that the provider, the importer or any relevant operator, as appropriate, takes those corrective actions. Where the high-risk AI system presents a risk within the meaning of Article 79(1), the distributor shall immediately informthe provider or importer of the system and the authorities competent for the high-risk AI system concerned, giving details, in particular, of the non-compliance and of any corrective actions taken.5. Upon a reasoned request from a relevant competent authority, distributors of a high-risk AI system shall provide thatauthority with all the information and documentation regarding their actions pursuant to paragraphs 1 to 4 necessary todemonstrate the conformity of that system with the requirements set out in Section 2.6. Distributors shall cooperate with the relevant competent authorities in any action those authorities take in relation to Article 25Responsibilities along the AI value chain1. Any distributor, importer, deployer or other third-party shall be considered to be a provider of a high-risk AI systemfor the purposes of this Regulation and shall be subject to the obligations of the provider under Article 16, in any of thefollowing circumstances:(a) they put their name or trademark on a high-risk AI system already placed on the market or put into service, withoutprejudice to contractual arrangements stipulating that the obligations are otherwise allocated;(b) they make a substantial modification to a high-risk AI system that has already been placed on the market or has alreadybeen put into service in such a way that it remains a high-risk AI system pursuant to Article 6;(c) they modify the intended purpose of an AI system, including a general-purpose AI system, which has not been classifiedas high-risk and has already been placed on the market or put into service in such a way that the AI system concernedbecomes a high-risk AI system in accordance with Article 6.2. Where the circumstances referred to in paragraph 1 occur, the provider that initially placed the AI system on themarket or put it into service shall no longer be considered to be a provider of that specific AI system for the purposes ofthis Regulation. That initial provider shall closely cooperate with new providers and shall make available the necessaryinformation and provide the reasonably expected technical access and other assistance that are required for the fulfilment ofthe obligations set out in this Regulation, in particular regarding the compliance with the conformity assessment ofhigh-risk AI systems. This paragraph shall not apply in cases where the initial provider has clearly specified that its AIsystem is not to be changed into a high-risk AI system and therefore does not fall under the obligation to hand over thedocumentation.3. In the case of high-risk AI systems that are safety components of products covered by the Union harmonisationlegislation listed in Section A of Annex I, the product manufacturer shall be considered to be the provider of the high-risk AI system, and shall be subject to the obligations under Article 16 under either of the following circumstances:(a) the high-risk AI system is placed on the market together with the product under the name or trademark of the productmanufacturer;(b) the high-risk AI system is put into service under the name or trademark of the product manufacturer after the producthas been placed on the market.4. The provider of a high-risk AI system and the third party that supplies an AI system, tools, services, components, orprocesses that are used or integrated in a high-risk AI system shall, by written agreement, specify the necessary information, capabilities, technical access and other assistance based on the generally acknowledged state of the art, in order to enablethe provider of the high-risk AI system to fully comply with the obligations set out in this Regulation. This paragraph shallnot apply to third parties making accessible to the public tools, services, processes, or components, other thangeneral-purpose AI models, under a free and open-source licence. The AI Office may develop and recommend voluntary model terms for contracts between providers of high-risk AI systemsand third parties that supply tools, services, components or processes that are used for or integrated into high-risk AIsystems. When developing those voluntary model terms, the AI Office shall take into account possible contractualrequirements applicable in specific sectors or business cases. The voluntary model terms shall be published and be availablefree of charge in an easily usable electronic format.5. Paragraphs 2 and 3 are without prejudice to the need to observe and protect intellectual property rights, confidentialbusiness information and trade secrets in accordance with Union and national law. Article 26Obligations of deployers of high-risk AI systems2. Deployers shall assign human oversight to natural persons who have the necessary competence, training andauthority, as well as the necessary support.3. The obligations set out in paragraphs 1 and 2, are without prejudice to other deployer obligations under Union ornational law and to the deployer’s freedom to organise its own resources and activities for the purpose of implementing thehuman oversight measures indicated by the provider.4. Without prejudice to paragraphs 1 and 2, to the extent the deployer exercises control over the input data, thatdeployershall ensure that input data is relevant and sufficiently representative in view of the intended purpose of thehigh-risk AI system.5. Deployers shall monitor the operation of the high-risk AI system on the basis of the instructions for use and, whererelevant, inform providers in accordance with Article 72. Where deployers have reason to consider that the use of thehigh-risk AI system in accordance with the instructions may result in that AI system presenting a risk within the meaning of Article 79(1), they shall, without undue delay, inform the provider or distributor and the relevant market surveillanceauthority, and shall suspend the use of that system. Where deployers have identified a serious incident, they shall alsoimmediately inform first the provider, and then the importer or distributor and the relevant market surveillance authoritiesof that incident. If the deployer is not able to reach the provider, Article 73 shall apply mutatis mutandis. This obligationshall not cover sensitive operational data of deployers of AI systems which are law enforcement authorities. For deployers that are financial institutions subject to requirements regarding their internal governance, arrangements orprocesses under Union financial services law, the monitoring obligation set out in the first subparagraph shall be deemed tobe fulfilled by complying with the rules on internal governance arrangements, processes and mechanisms pursuant to therelevant financial service law.6. Deployers of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system to the extentsuch logs are under their control, for a period appropriate to the intended purpose of the high-risk AI system, of at least sixmonths, unless provided otherwise in applicable Union or national law, in particular in Union law on the protection ofpersonal data. Deployers that are financial institutions subject to requirements regarding their internal governance, arrangements orprocesses under Union financial services law shall maintain the logs as part of the documentation kept pursuant to therelevant Union financial service law.7. Before putting into service or using a high-risk AI system at the workplace, deployers who are employers shall informworkers’ representatives and the affected workers that they will be subject to the use of the high-risk AI system. Thisinformation shall be provided, where applicable, in accordance with the rules and procedures laid down in Union andnational law and practice on information of workers and their representatives.8. Deployers of high-risk AI systems that are public authorities, or Union institutions, bodies, offices or agencies shallcomply with the registration obligations referred to in Article 49. When such deployers find that the high-risk AI systemthat they envisage using has not been registered in the EU database referred to in Article 71, they shall not use that systemand shall inform the provider or the distributor.9. Where applicable, deployers of high-risk AI systems shall use the information provided under Article 13 of this Regulation to comply with their obligation to carry out a data protection impact assessment under Article 35 of Regulation(EU) 2016/679 or Article 27 of Directive (EU) 2016/680.10. Without prejudice to Directive (EU) 2016/680, in the framework of an investigation for the targeted search ofa person suspected or convicted of having committed a criminal offence, the deployer of a high-risk AI system forpost-remote biometric identification shall request an authorisation, ex ante, or without undue delay and no later than 48hours, by a judicial authority or an administrative authority whose decision is binding and subject to judicial review, for theuse of that system, except when it is used for the initial identification of a potential suspect based on objective and verifiablefacts directly linked to the offence. Each use shall be limited to what is strictly necessary for the investigation of a specificcriminal offence. If the authorisation requested pursuant to the first subparagraph is rejected, the use of the post-remote biometric In no case shall such high-risk AI system for post-remote biometric identification be used for law enforcement purposes inan untargeted way, without any link to a criminal offence, a criminal proceeding, a genuine and present or genuine andforeseeable threat of a criminal offence, or the search for a specific missing person. It shall be ensured that no decision thatproduces an adverse legal effect on a person may be taken by the law enforcement authorities based solely on the output ofsuch post-remote biometric identification systems. This paragraph is without prejudice to Article 9 of Regulation (EU) 2016/679 and Article 10 of Directive (EU) 2016/680for the processing of biometric data. Regardless of the purpose or deployer, each use of such high-risk AI systems shall be documented in the relevant police fileand shall be made available to the relevant market surveillance authority and the national data protection authority uponrequest, excluding the disclosure of sensitive operational data related to law enforcement. This subparagraph shall bewithout prejudice to the powers conferred by Directive (EU) 2016/680 on supervisory authorities. Deployers shall submit annual reports to the relevant market surveillance and national data protection authorities on theiruse of post-remote biometric identification systems, excluding the disclosure of sensitive operational data related to lawenforcement. The reports may be aggregated to cover more than one deployment. Member States may introduce, in accordance with Union law, more restrictive laws on the use of post-remote biometricidentification systems.11. Without prejudice to Article 50 of this Regulation, deployers of high-risk AI systems referred to in Annex III thatmake decisions or assist in making decisions related to natural persons shall inform the natural persons that they are subjectto the use of the high-risk AI system. For high-risk AI systems used for law enforcement purposes Article 13 of Directive(EU) 2016/680 shall apply.12. Deployers shall cooperate with the relevant competent authorities in any action those authorities take in relation tothe high-risk AI system in order to implement this Regulation. Article 27Fundamental rights impact assessment for high-risk AI systems1. Prior to deploying a high-risk AI system referred to in Article 6(2), with the exception of high-risk AI systemsintended to be used in the area listed in point 2 of Annex III, deployers that are bodies governed by public law, or are privateentities providing public services, and deployers of high-risk AI systems referred to in points 5 (b) and (c) of Annex III, shallperform an assessment of the impact on fundamental rights that the use of such system may produce. For that purpose, deployers shall perform an assessment consisting of:(a) a description of the deployer’s processes in which the high-risk AI system will be used in line with its intended purpose;(b) a description of the period of time within which, and the frequency with which, each high-risk AI system is intended tobe used;(c) the categories of natural persons and groups likely to be affected by its use in the specific context;(d) the specific risks of harm likely to have an impact on the categories of natural persons or groups of persons identifiedpursuant to point (c) of this paragraph, taking into account the information given by the provider pursuant to Article 13;(e) a description of the implementation of human oversight measures, according to the instructions for use;2. The obligation laid down in paragraph 1 applies to the first use of the high-risk AI system. The deployer may, insimilar cases, rely on previously conducted fundamental rights impact assessments or existing impact assessments carriedout by provider. If, during the use of the high-risk AI system, the deployer considers that any of the elements listed inparagraph 1 has changed or is no longer up to date, the deployershall take the necessary steps to update the information.3. Once the assessment referred to in paragraph 1 of this Article has been performed, the deployershall notify themarket surveillance authority of its results, submitting the filled-out template referred to in paragraph 5 of this Article aspart of the notification. In the case referred to in Article 46(1), deployers may be exempt from that obligation to notify.4. If any of the obligations laid down in this Article is already met through the data protection impact assessmentconducted pursuant to Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, the fundamentalrights impact assessment referred to in paragraph 1 of this Article shall complement that data protection impactassessment.5. The AI Office shall develop a template for a questionnaire, including through an automated tool, to facilitate deployersin complying with their obligations under this Article in a simplified manner. SECTION 4Notifying authorities and notified bodies Article 28Notifying authorities1. Each Member State shall designate or establish at least one notifying authority responsible for setting up and carryingout the necessary procedures for the assessment, designation and notification of conformity assessment bodies and for theirmonitoring. Those procedures shall be developed in cooperation between the notifying authorities of all Member States.2. Member States may decide that the assessment and monitoring referred to in paragraph 1 is to be carried out bya national accreditation body within the meaning of, and in accordance with, Regulation (EC) No 765/2008.3. Notifying authorities shall be established, organised and operated in such a way that no conflict of interest arises withconformity assessment bodies, and that the objectivity and impartiality of their activities are safeguarded.4. Notifying authorities shall be organised in such a way that decisions relating to the notification of conformityassessment bodies are taken by competent persons different from those who carried out the assessment of those bodies.5. Notifying authorities shall offer or provide neither any activities that conformity assessment bodies perform, nor anyconsultancy services on a commercial or competitive basis.6. Notifying authorities shall safeguard the confidentiality of the information that they obtain, in accordance with Article 78.7. Notifying authorities shall have an adequate number of competent personnel at their disposal for the properperformance of their tasks. Competent personnel shall have the necessary expertise, where applicable, for their function, infields such as information technologies, AI and law, including the supervision of fundamental rights. Article 29Application of a conformity assessment body for notification2. The application for notification shall be accompanied by a description of the conformity assessment activities, theconformity assessment module or modules and the types of AI systems for which the conformity assessment body claimsto be competent, as well as by an accreditation certificate, where one exists, issued by a national accreditation body attestingthat the conformity assessment body fulfils the requirements laid down in Article 31. Any valid document related to existing designations of the applicant notified body under any other Union harmonisationlegislation shall be added.3. Where the conformity assessment body concerned cannot provide an accreditation certificate, it shall provide thenotifying authority with all the documentary evidence necessary for the verification, recognition and regular monitoring ofits compliance with the requirements laid down in Article 31.4. For notified bodies which are designated under any other Union harmonisation legislation, all documents andcertificates linked to those designations may be used to support their designation procedure under this Regulation, asappropriate. The notified body shall update the documentation referred to in paragraphs 2 and 3 of this Article wheneverrelevant changes occur, in order to enable the authority responsible for notified bodies to monitor and verify continuouscompliance with all the requirements laid down in Article 31. Article 30Notification procedure1. Notifying authorities may notify only conformity assessment bodies which have satisfied the requirements laid downin Article 31.2. Notifying authorities shall notify the Commission and the other Member States, using the electronic notification tooldeveloped and managed by the Commission, of each conformity assessment body referred to in paragraph 1.3. The notification referred to in paragraph 2 of this Article shall include full details of the conformity assessmentactivities, the conformity assessment module or modules, the types of AI systems concerned, and the relevant attestation ofcompetence. Where a notification is not based on an accreditation certificate as referred to in Article 29(2), the notifyingauthority shall provide the Commission and the other Member States with documentary evidence which attests to thecompetence of the conformity assessment body and to the arrangements in place to ensure that that body will bemonitored regularly and will continue to satisfy the requirements laid down in Article 31.4. The conformity assessment body concerned may perform the activities of a notified body only where no objectionsare raised by the Commission or the other Member States within two weeks of a notification by a notifying authority whereit includes an accreditation certificate referred to in Article 29(2), or within two months of a notification by the notifyingauthority where it includes documentary evidence referred to in Article 29(3).5. Where objections are raised, the Commission shall, without delay, enter into consultations with the relevant Member States and the conformity assessment body. In view thereof, the Commission shall decide whether the authorisation isjustified. The Commission shall address its decision to the Member State concerned and to the relevant conformityassessment body. Article 31Requirements relating to notified bodies1. A notified body shall be established under the national law of a Member State and shall have legal personality.2. Notified bodies shall satisfy the organisational, quality management, resources and process requirements that arenecessary to fulfil their tasks, as well as suitable cybersecurity requirements.3. The organisational structure, allocation of responsibilities, reporting lines and operation of notified bodies shall4. Notified bodies shall be independent of the provider of a high-risk AI system in relation to which they performconformity assessment activities. Notified bodies shall also be independent of any other operator having an economicinterest in high-risk AI systems assessed, as well as of any competitors of the provider. This shall not preclude the use ofassessed high-risk AI systems that are necessary for the operations of the conformity assessment body, or the use of suchhigh-risk AI systems for personal purposes.5. Neither a conformity assessment body, its top-level management nor the personnel responsible for carrying out itsconformity assessment tasks shall be directly involved in the design, development, marketing or use of high-risk AI systems, nor shall they represent the parties engaged in those activities. They shall not engage in any activity that might conflict withtheir independence of judgement or integrity in relation to conformity assessment activities for which they are notified. This shall, in particular, apply to consultancy services.6. Notified bodies shall be organised and operated so as to safeguard the independence, objectivity and impartiality oftheir activities. Notified bodies shall document and implement a structure and procedures to safeguard impartiality and topromote and apply the principles of impartiality throughout their organisation, personnel and assessment activities.7. Notified bodies shall have documented procedures in place ensuring that their personnel, committees, subsidiaries, subcontractors and any associated body or personnel of external bodies maintain, in accordance with Article 78, theconfidentiality of the information which comes into their possession during the performance of conformity assessmentactivities, except when its disclosure is required by law. The staff of notified bodies shall be bound to observe professionalsecrecy with regard to all information obtained in carrying out their tasks under this Regulation, except in relation to thenotifying authorities of the Member State in which their activities are carried out.8. Notified bodies shall have procedures for the performance of activities which take due account of the size ofa provider, the sector in which it operates, its structure, and the degree of complexity of the AI system concerned.9. Notified bodies shall take out appropriate liability insurance for their conformity assessment activities, unless liabilityis assumed by the Member State in which they are established in accordance with national law or that Member State is itselfdirectly responsible for the conformity assessment.10. Notified bodies shall be capable of carrying out all their tasks under this Regulation with the highest degree ofprofessional integrity and the requisite competence in the specific field, whether those tasks are carried out by notifiedbodies themselves or on their behalf and under their responsibility.11. Notified bodies shall have sufficient internal competences to be able effectively to evaluate the tasks conducted byexternal parties on their behalf. The notified body shall have permanent availability of sufficient administrative, technical, legal and scientific personnel who possess experience and knowledge relating to the relevant types of AI systems, data anddata computing, and relating to the requirements set out in Section 2.12. Notified bodies shall participate in coordination activities as referred to in Article 38. They shall also take partdirectly, or be represented in, European standardisation organisations, or ensure that they are aware and up to date inrespect of relevant standards. Article 32Presumption of conformity with requirements relating to notified bodies Where a conformity assessment body demonstrates its conformity with the criteria laid down in the relevant harmonisedstandards or parts thereof, the references of which have been published in the Official Journal of the European Union, it shall Article 33Subsidiaries of notified bodies and subcontracting1. Where a notified body subcontracts specific tasks connected with the conformity assessment or has recourse toa subsidiary, it shall ensure that the subcontractor or the subsidiary meets the requirements laid down in Article 31, andshall inform the notifying authority accordingly.2. Notified bodies shall take full responsibility for the tasks performed by any subcontractors or subsidiaries.3. Activities may be subcontracted or carried out by a subsidiary only with the agreement of the provider. Notifiedbodies shall make a list of their subsidiaries publicly available.4. The relevant documents concerning the assessment of the qualifications of the subcontractor or the subsidiary andthe work carried out by them under this Regulation shall be kept at the disposal of the notifying authority for a period offive years from the termination date of the subcontracting. Article 34Operational obligations of notified bodies1. Notified bodies shall verify the conformity of high-risk AI systems in accordance with the conformity assessmentprocedures set out in Article 43.2. Notified bodies shall avoid unnecessary burdens for providers when performing their activities, and take due accountof the size of the provider, the sector in which it operates, its structure and the degree of complexity of the high-risk AIsystem concerned, in particular in view of minimising administrative burdens and compliance costs for micro- and smallenterprises within the meaning of Recommendation 2003/361/EC. The notified body shall, nevertheless, respect the degreeof rigour and the level of protection required for the compliance of the high-risk AI system with the requirements of this Regulation.3. Notified bodies shall make available and submit upon request all relevant documentation, including the providers’documentation, to the notifying authority referred to in Article 28 to allow that authority to conduct its assessment, designation, notification and monitoring activities, and to facilitate the assessment outlined in this Section. Article 35Identification numbers and lists of notified bodies1. The Commission shall assign a single identification number to each notified body, even where a body is notified undermore than one Union act.2. The Commission shall make publicly available the list of the bodies notified under this Regulation, including theiridentification numbers and the activities for which they have been notified. The Commission shall ensure that the list is keptup to date. Article 36Changes to notifications1. The notifying authority shall notify the Commission and the other Member States of any relevant changes to thenotification of a notified body via the electronic notification tool referred to in Article 30(2).2. The procedures laid down in Articles 29 and 30 shall apply to extensions of the scope of the notification.3. Where a notified body decides to cease its conformity assessment activities, it shall inform the notifying authority andthe providers concerned as soon as possible and, in the case of a planned cessation, at least one year before ceasing itsactivities. The certificates of the notified body may remain valid for a period of nine months after cessation of the notifiedbody’s activities, on condition that another notified body has confirmed in writing that it will assume responsibilities for thehigh-risk AI systems covered by those certificates. The latter notified body shall complete a full assessment of the high-risk AI systems affected by the end of that nine-month-period before issuing new certificates for those systems. Where thenotified body has ceased its activity, the notifying authority shall withdraw the designation.4. Where a notifying authority has sufficient reason to consider that a notified body no longer meets the requirementslaid down in Article 31, or that it is failing to fulfil its obligations, the notifying authority shall without delay investigate thematter with the utmost diligence. In that context, it shall inform the notified body concerned about the objections raisedand give it the possibility to make its views known. If the notifying authority comes to the conclusion that the notified bodyno longer meets the requirements laid down in Article 31 or that it is failing to fulfil its obligations, it shall restrict, suspendor withdraw the designation as appropriate, depending on the seriousness of the failure to meet those requirements or fulfilthose obligations. It shall immediately inform the Commission and the other Member States accordingly.5. Where its designation has been suspended, restricted, or fully or partially withdrawn, the notified body shall informthe providers concerned within 10 days.6. In the event of the restriction, suspension or withdrawal of a designation, the notifying authority shall takeappropriate steps to ensure that the files of the notified body concerned are kept, and to make them available to notifyingauthorities in other Member States and to market surveillance authorities at their request.7. In the event of the restriction, suspension or withdrawal of a designation, the notifying authority shall:(a) assess the impact on the certificates issued by the notified body;(b) submit a report on its findings to the Commission and the other Member States within three months of having notifiedthe changes to the designation;(c) require the notified body to suspend or withdraw, within a reasonable period of time determined by the authority, anycertificates which were unduly issued, in order to ensure the continuing conformity of high-risk AI systems on themarket;(d) inform the Commission and the Member States about certificates the suspension or withdrawal of which it has required;(e) provide the national competent authorities of the Member State in which the provider has its registered place ofbusiness with all relevant information about the certificates of which it has required the suspension or withdrawal; thatauthority shall take the appropriate measures, where necessary, to avoid a potential risk to health, safety or fundamentalrights.8. With the exception of certificates unduly issued, and where a designation has been suspended or restricted, thecertificates shall remain valid in one of the following circumstances:(a) the notifying authority has confirmed, within one month of the suspension or restriction, that there is no risk to health, safety or fundamental rights in relation to certificates affected by the suspension or restriction, and the notifyingauthority has outlined a timeline for actions to remedy the suspension or restriction; or(b) the notifying authority has confirmed that no certificates relevant to the suspension will be issued, amended or re-issuedduring the course of the suspension or restriction, and states whether the notified body has the capability of continuingto monitor and remain responsible for existing certificates issued for the period of the suspension or restriction; in theevent that the notifying authority determines that the notified body does not have the capability to support existingcertificates issued, the provider of the system covered by the certificate shall confirm in writing to the nationalcompetent authorities of the Member State in which it has its registered place of business, within three months of the9. With the exception of certificates unduly issued, and where a designation has been withdrawn, the certificates shallremain valid for a period of nine months under the following circumstances:(a) the national competent authority of the Member State in which the provider of the high-risk AI system covered by thecertificate has its registered place of business has confirmed that there is no risk to health, safety or fundamental rightsassociated with the high-risk AI systems concerned; and(b) another notified body has confirmed in writing that it will assume immediate responsibility for those AI systems andcompletes its assessment within 12 months of the withdrawal of the designation. In the circumstances referred to in the first subparagraph, the national competent authority of the Member State in whichthe provider of the system covered by the certificate has its place of business may extend the provisional validity of thecertificates for additional periods of three months, which shall not exceed 12 months in total. The national competent authority or the notified body assuming the functions of the notified body affected by the changeof designation shall immediately inform the Commission, the other Member States and the other notified bodies thereof. Article 37Challenge to the competence of notified bodies1. The Commission shall, where necessary, investigate all cases where there are reasons to doubt the competence ofa notified body or the continued fulfilment by a notified body of the requirements laid down in Article 31 and of itsapplicable responsibilities.2. The notifying authority shall provide the Commission, on request, with all relevant information relating to thenotification or the maintenance of the competence of the notified body concerned.3. The Commission shall ensure that all sensitive information obtained in the course of its investigations pursuant to this Article is treated confidentially in accordance with Article 78.4. Where the Commission ascertains that a notified body does not meet or no longer meets the requirements for itsnotification, it shall inform the notifying Member State accordingly and request it to take the necessary corrective measures, including the suspension or withdrawal of the notification if necessary. Where the Member State fails to take the necessarycorrective measures, the Commission may, by means of an implementing act, suspend, restrict or withdraw the designation. That implementing act shall be adopted in accordance with the examination procedure referred to in Article 98(2). Article 38Coordination of notified bodies1. The Commission shall ensure that, with regard to high-risk AI systems, appropriate coordination and cooperationbetween notified bodies active in the conformity assessment procedures pursuant to this Regulation are put in place andproperly operated in the form of a sectoral group of notified bodies.2. Each notifying authority shall ensure that the bodies notified by it participate in the work of a group referred to inparagraph 1, directly or through designated representatives. Article 39Conformity assessment bodies of third countries Conformity assessment bodies established under the law of a third country with which the Union has concluded anagreement may be authorised to carry out the activities of notified bodies under this Regulation, provided that they meetthe requirements laid down in Article 31 or they ensure an equivalent level of compliance. SECTION 5Standards, conformity assessment, certificates, registration Article 40Harmonised standards and standardisation deliverables1. High-risk AI systems or general-purpose AI models which are in conformity with harmonised standards or partsthereof the references of which have been published in the Official Journal of the European Union in accordance with Regulation (EU) No 1025/2012 shall be presumed to be in conformity with the requirements set out in Section 2 of this Chapter or, as applicable, with the obligations set out in of Chapter V, Sections 2 and 3, of this Regulation, to the extent thatthose standards cover those requirements or obligations.2. In accordance with Article 10 of Regulation (EU) No 1025/2012, the Commission shall issue, without undue delay, standardisation requests covering all requirements set out in Section 2 of this Chapter and, as applicable, standardisationrequests covering obligations set out in Chapter V, Sections 2 and 3, of this Regulation. The standardisation request shallalso ask for deliverables on reporting and documentation processes to improve AI systems’ resource performance, such asreducing the high-risk AI system’s consumption of energy and of other resources during its lifecycle, and on theenergy-efficient development of general-purpose AI models. When preparing a standardisation request, the Commissionshall consult the Board and relevant stakeholders, including the advisory forum. When issuing a standardisation request to European standardisation organisations, the Commission shall specify thatstandards have to be clear, consistent, including with the standards developed in the various sectors for products covered bythe existing Union harmonisation legislation listed in Annex I, and aiming to ensure that high-risk AI systems orgeneral-purpose AI models placed on the market or put into service in the Union meet the relevant requirements orobligations laid down in this Regulation. The Commission shall request the European standardisation organisations to provide evidence of their best efforts to fulfilthe objectives referred to in the first and the second subparagraph of this paragraph in accordance with Article 24 of Regulation (EU) No 1025/2012.3. The participants in the standardisation process shall seek to promote investment and innovation in AI, includingthrough increasing legal certainty, as well as the competitiveness and growth of the Union market, to contribute tostrengthening global cooperation on standardisation and taking into account existing international standards in the field of AI that are consistent with Union values, fundamental rights and interests, and to enhance multi-stakeholder governanceensuring a balanced representation of interests and the effective participation of all relevant stakeholders in accordance with Articles 5, 6, and 7 of Regulation (EU) No 1025/2012. Article 41Common specifications1. The Commission may adopt, implementing acts establishing common specifications for the requirements set out in Section 2 of this Chapter or, as applicable, for the obligations set out in Sections 2 and 3 of Chapter V where the followingconditions have been fulfilled:(a) the Commission has requested, pursuant to Article 10(1) of Regulation (EU) No 1025/2012, one or more Europeanstandardisation organisations to draft a harmonised standard for the requirements set out in Section 2 of this Chapter, or, as applicable, for the obligations set out in Sections 2 and 3 of Chapter V, and:(ii) the harmonised standards addressing that request are not delivered within the deadline set in accordance with Article 10(1) of Regulation (EU) No 1025/2012; or(iii) the relevant harmonised standards insufficiently address fundamental rights concerns; or(iv) the harmonised standards do not comply with the request; and(b) no reference to harmonised standards covering the requirements referred to in Section 2 of this Chapter or, asapplicable, the obligations referred to in Sections 2 and 3 of Chapter V has been published in the Official Journal of the European Union in accordance with Regulation (EU) No 1025/2012, and no such reference is expected to be publishedwithin a reasonable period. When drafting the common specifications, the Commission shall consult the advisory forum referred to in Article 67. The implementing acts referred to in the first subparagraph of this paragraph shall be adopted in accordance with theexamination procedure referred to in Article 98(2).2. Before preparing a draft implementing act, the Commission shall inform the committee referred to in Article 22 of Regulation (EU) No 1025/2012 that it considers the conditions laid down in paragraph 1 of this Article to be fulfilled.3. High-risk AI systems or general-purpose AI models which are in conformity with the common specifications referredto in paragraph 1, or parts of those specifications, shall be presumed to be in conformity with the requirements set out in Section 2 of this Chapter or, as applicable, to comply with the obligations referred to in Sections 2 and 3 of Chapter V, tothe extent those common specifications cover those requirements or those obligations.4. Where a harmonised standard is adopted by a European standardisation organisation and proposed to the Commission for the publication of its reference in the Official Journal of the European Union, the Commission shall assess theharmonised standard in accordance with Regulation (EU) No 1025/2012. When reference to a harmonised standard ispublished in the Official Journal of the European Union, the Commission shall repeal the implementing acts referred to inparagraph 1, or parts thereof which cover the same requirements set out in Section 2 of this Chapter or, as applicable, thesame obligations set out in Sections 2 and 3 of Chapter V.5. Where providers of high-risk AI systems or general-purpose AI models do not comply with the commonspecifications referred to in paragraph 1, they shall duly justify that they have adopted technical solutions that meet therequirements referred to in Section 2 of this Chapter or, as applicable, comply with the obligations set out in Sections 2 and3 of Chapter V to a level at least equivalent thereto.6. Where a Member State considers that a common specification does not entirely meet the requirements set out in Section 2 or, as applicable, comply with obligations set out in Sections 2 and 3 of Chapter V, it shall inform the Commission thereof with a detailed explanation. The Commission shall assess that information and, if appropriate, amendthe implementing act establishing the common specification concerned. Article 42Presumption of conformity with certain requirements1. High-risk AI systems that have been trained and tested on data reflecting the specific geographical, behavioural, contextual or functional setting within which they are intended to be used shall be presumed to comply with the relevantrequirements laid down in Article 10(4).2. High-risk AI systems that have been certified or for which a statement of conformity has been issued undera cybersecurity scheme pursuant to Regulation (EU) 2019/881 and the references of which have been published in the Official Journal of the European Union shall be presumed to comply with the cybersecurity requirements set out in Article 15Article 43Conformity assessment1. For high-risk AI systems listed in point 1 of Annex III, where, in demonstrating the compliance of a high-risk AIsystem with the requirements set out in Section 2, the provider has applied harmonised standards referred to in Article 40, or, where applicable, common specifications referred to in Article 41, the provider shall opt for one of the followingconformity assessment procedures based on:(a) the internal control referred to in Annex VI; or(b) the assessment of the quality management system and the assessment of the technical documentation, with theinvolvement of a notified body, referred to in Annex VII. In demonstrating the compliance of a high-risk AI system with the requirements set out in Section 2, the provider shallfollow the conformity assessment procedure set out in Annex VII where:(a) harmonised standards referred to in Article 40 do not exist, and common specifications referred to in Article 41 are notavailable;(b) the provider has not applied, or has applied only part of, the harmonised standard;(c) the common specifications referred to in point (a) exist, but the provider has not applied them;(d) one or more of the harmonised standards referred to in point (a) has been published with a restriction, and only on thepart of the standard that was restricted. For the purposes of the conformity assessment procedure referred to in Annex VII, the provider may choose any of thenotified bodies. However, where the high-risk AI system is intended to be put into service by law enforcement, immigrationor asylum authorities or by Union institutions, bodies, offices or agencies, the market surveillance authority referred to in Article 74(8) or (9), as applicable, shall act as a notified body.2. For high-risk AI systems referred to in points 2 to 8 of Annex III, providers shall follow the conformity assessmentprocedure based on internal control as referred to in Annex VI, which does not provide for the involvement of a notifiedbody.3. For high-risk AI systems covered by the Union harmonisation legislation listed in Section A of Annex I, the providershall follow the relevant conformity assessment procedure as required under those legal acts. The requirements set out in Section 2 of this Chapter shall apply to those high-risk AI systems and shall be part of that assessment. Points 4.3., 4.4., 4.5. and the fifth paragraph of point 4.6 of Annex VII shall also apply. For the purposes of that assessment, notified bodies which have been notified under those legal acts shall be entitled tocontrol the conformity of the high-risk AI systems with the requirements set out in Section 2, provided that the complianceof those notified bodies with requirements laid down in Article 31(4), (5), (10) and (11) has been assessed in the context ofthe notification procedure under those legal acts. Where a legal act listed in Section A of Annex I enables the product manufacturer to opt out from a third-party conformityassessment, provided that that manufacturer has applied all harmonised standards covering all the relevant requirements, that manufacturer may use that option only if it has also applied harmonised standards or, where applicable, commonspecifications referred to in Article 41, covering all requirements set out in Section 2 of this Chapter.4. High-risk AI systems that have already been subject to a conformity assessment procedure shall undergo a newconformity assessment procedure in the event of a substantial modification, regardless of whether the modified system isintended to be further distributed or continues to be used by the current deployer. For high-risk AI systems that continue to learn after being placed on the market or put into service, changes to the high-risk AI system and its performance that have been pre-determined by the provider at the moment of the initial conformityassessment and are part of the information contained in the technical documentation referred to in point 2(f) of Annex IV, shall not constitute a substantial modification.6. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend paragraphs 1and 2 of this Article in order to subject high-risk AI systems referred to in points 2 to 8 of Annex III to the conformityassessment procedure referred to in Annex VII or parts thereof. The Commission shall adopt such delegated acts taking intoaccount the effectiveness of the conformity assessment procedure based on internal control referred to in Annex VI inpreventing or minimising the risks to health and safety and protection of fundamental rights posed by such systems, as wellas the availability of adequate capacities and resources among notified bodies. Article 44Certificates1. Certificates issued by notified bodies in accordance with Annex VII shall be drawn-up in a language which can beeasily understood by the relevant authorities in the Member State in which the notified body is established.2. Certificates shall be valid for the period they indicate, which shall not exceed five years for AI systems covered by Annex I, and four years for AI systems covered by Annex III. At the request of the provider, the validity of a certificate maybe extended for further periods, each not exceeding five years for AI systems covered by Annex I, and four years for AIsystems covered by Annex III, based on a re-assessment in accordance with the applicable conformity assessmentprocedures. Any supplement to a certificate shall remain valid, provided that the certificate which it supplements is valid.3. Where a notified body finds that an AI system no longer meets the requirements set out in Section 2, it shall, takingaccount of the principle of proportionality, suspend or withdraw the certificate issued or impose restrictions on it, unlesscompliance with those requirements is ensured by appropriate corrective action taken by the provider of the system withinan appropriate deadline set by the notified body. The notified body shall give reasons for its decision. An appeal procedure against decisions of the notified bodies, including on conformity certificates issued, shall be available. Article 45Information obligations of notified bodies1. Notified bodies shall inform the notifying authority of the following:(a) any Union technical documentation assessment certificates, any supplements to those certificates, and any qualitymanagement system approvals issued in accordance with the requirements of Annex VII;(b) any refusal, restriction, suspension or withdrawal of a Union technical documentation assessment certificate or a qualitymanagement system approval issued in accordance with the requirements of Annex VII;(c) any circumstances affecting the scope of or conditions for notification;(d) any request for information which they have received from market surveillance authorities regarding conformityassessment activities;(e) on request, conformity assessment activities performed within the scope of their notification and any other activityperformed, including cross-border activities and subcontracting.2. Each notified body shall inform the other notified bodies of:(a) quality management system approvals which it has refused, suspended or withdrawn, and, upon request, of qualitysystem approvals which it has issued;3. Each notified body shall provide the other notified bodies carrying out similar conformity assessment activitiescovering the same types of AI systems with relevant information on issues relating to negative and, on request, positiveconformity assessment results.4. Notified bodies shall safeguard the confidentiality of the information that they obtain, in accordance with Article 78. Article 46Derogation from conformity assessment procedure1. By way of derogation from Article 43 and upon a duly justified request, any market surveillance authority mayauthorise the placing on the market or the putting into service of specific high-risk AI systems within the territory of the Member State concerned, for exceptional reasons of public security or the protection of life and health of persons, environmental protection or the protection of key industrial and infrastructural assets. That authorisation shall be fora limited period while the necessary conformity assessment procedures are being carried out, taking into account theexceptional reasons justifying the derogation. The completion of those procedures shall be undertaken without undue delay.2. In a duly justified situation of urgency for exceptional reasons of public security or in the case of specific, substantialand imminent threat to the life or physical safety of natural persons, law-enforcement authorities or civil protectionauthorities may put a specific high-risk AI system into service without the authorisation referred to in paragraph 1, provided that such authorisation is requested during or after the use without undue delay. If the authorisation referred to inparagraph 1 is refused, the use of the high-risk AI system shall be stopped with immediate effect and all the results andoutputs of such use shall be immediately discarded.3. The authorisation referred to in paragraph 1 shall be issued only if the market surveillance authority concludes thatthe high-risk AI system complies with the requirements of Section 2. The market surveillance authority shall inform the Commission and the other Member States of any authorisation issued pursuant to paragraphs 1 and 2. This obligation shallnot cover sensitive operational data in relation to the activities of law-enforcement authorities.4. Where, within 15 calendar days of receipt of the information referred to in paragraph 3, no objection has been raisedby either a Member State or the Commission in respect of an authorisation issued by a market surveillance authority ofa Member State in accordance with paragraph 1, that authorisation shall be deemed justified.5. Where, within 15 calendar days of receipt of the notification referred to in paragraph 3, objections are raised bya Member State against an authorisation issued by a market surveillance authority of another Member State, or where the Commission considers the authorisation to be contrary to Union law, or the conclusion of the Member States regarding thecompliance of the system as referred to in paragraph 3 to be unfounded, the Commission shall, without delay, enter intoconsultations with the relevant Member State. The operators concerned shall be consulted and have the possibility topresent their views. Having regard thereto, the Commission shall decide whether the authorisation is justified. The Commission shall address its decision to the Member State concerned and to the relevant operators.6. Where the Commission considers the authorisation unjustified, it shall be withdrawn by the market surveillanceauthority of the Member State concerned.7. For high-risk AI systems related to products covered by Union harmonisation legislation listed in Section A of Annex I, only the derogations from the conformity assessment established in that Union harmonisation legislation shallapply. Article 47EU declaration of conformity1. The provider shall draw up a written machine readable, physical or electronically signed EU declaration of conformityfor each high-risk AI system, and keep it at the disposal of the national competent authorities for 10 years after thehigh-risk AI system has been placed on the market or put into service. The EU declaration of conformity shall identify the2. The EU declaration of conformity shall state that the high-risk AI system concerned meets the requirements set out in Section 2. The EU declaration of conformity shall contain the information set out in Annex V, and shall be translated intoa language that can be easily understood by the national competent authorities of the Member States in which the high-risk AI system is placed on the market or made available.3. Where high-risk AI systems are subject to other Union harmonisation legislation which also requires an EUdeclaration of conformity, a single EU declaration of conformity shall be drawn up in respect of all Union law applicable tothe high-risk AI system. The declaration shall contain all the information required to identify the Union harmonisationlegislation to which the declaration relates.4. By drawing up the EU declaration of conformity, the provider shall assume responsibility for compliance with therequirements set out in Section 2. The provider shall keep the EU declaration of conformity up-to-date as appropriate.5. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend Annex V byupdating the content of the EU declaration of conformity set out in that Annex, in order to introduce elements that becomenecessary in light of technical progress. Article 48CE marking1. The CE marking shall be subject to the general principles set out in Article 30 of Regulation (EC) No 765/2008.2. For high-risk AI systems provided digitally, a digital CE marking shall be used, only if it can easily be accessed via theinterface from which that system is accessed or via an easily accessible machine-readable code or other electronic means.3. The CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems. Where that is not possible ornot warranted on account of the nature of the high-risk AI system, it shall be affixed to the packaging or to theaccompanying documentation, as appropriate.4. Where applicable, the CE marking shall be followed by the identification number of the notified body responsible forthe conformity assessment procedures set out in Article 43. The identification number of the notified body shall be affixedby the body itself or, under its instructions, by the provider or by the provider’s authorised representative. The identificationnumber shall also be indicated in any promotional material which mentions that the high-risk AI system fulfils therequirements for CE marking.5. Where high-risk AI systems are subject to other Union law which also provides for the affixing of the CE marking, the CE marking shall indicate that the high-risk AI system also fulfil the requirements of that other law. Article 49Registration1. Before placing on the market or putting into service a high-risk AI system listed in Annex III, with the exception ofhigh-risk AI systems referred to in point 2 of Annex III, the provider or, where applicable, the authorised representativeshall register themselves and their system in the EU database referred to in Article 71.2. Before placing on the market or putting into service an AI system for which the provider has concluded that it is nothigh-risk according to Article 6(3), that provider or, where applicable, the authorised representative shall registerthemselves and that system in the EU database referred to in Article 71.3. Before putting into service or using a high-risk AI system listed in Annex III, with the exception of high-risk AIsystems listed in point 2 of Annex III, deployers that are public authorities, Union institutions, bodies, offices or agencies or4. For high-risk AI systems referred to in points 1, 6 and 7 of Annex III, in the areas of law enforcement, migration, asylum and border control management, the registration referred to in paragraphs 1, 2 and 3 of this Article shall be ina secure non-public section of the EU database referred to in Article 71 and shall include only the following information, asapplicable, referred to in:(a) Section A, points 1 to 10, of Annex VIII, with the exception of points 6, 8 and 9;(b) Section B, points 1 to 5, and points 8 and 9 of Annex VIII;(c) Section C, points 1 to 3, of Annex VIII;(d) points 1, 2, 3 and 5, of Annex IX. Only the Commission and national authorities referred to in Article 74(8) shall have access to the respective restrictedsections of the EU database listed in the first subparagraph of this paragraph.5. High-risk AI systems referred to in point 2 of Annex III shall be registered at national level. CHAPTER IVTRANSPARENCY OBLIGATIONS FOR PROVIDERS AND DEPLOYERS OF CERTAIN AI SYSTEMSArticle 50Transparency obligations for providers and deployers of certain AI systems1. Providers shall ensure that AI systems intended to interact directly with natural persons are designed and developed insuch a way that the natural persons concerned are informed that they are interacting with an AI system, unless this isobvious from the point of view of a natural person who is reasonably well-informed, observant and circumspect, takinginto account the circumstances and the context of use. This obligation shall not apply to AI systems authorised by law todetect, prevent, investigate or prosecute criminal offences, subject to appropriate safeguards for the rights and freedoms ofthird parties, unless those systems are available for the public to report a criminal offence.2. Providers of AI systems, including general-purpose AI systems, generating synthetic audio, image, video or textcontent, shall ensure that the outputs of the AI system are marked in a machine-readable format and detectable asartificially generated or manipulated. Providers shall ensure their technical solutions are effective, interoperable, robust andreliable as far as this is technically feasible, taking into account the specificities and limitations of various types of content, the costs of implementation and the generally acknowledged state of the art, as may be reflected in relevant technicalstandards. This obligation shall not apply to the extent the AI systems perform an assistive function for standard editing ordo not substantially alter the input data provided by the deployer or the semantics thereof, or where authorised by law todetect, prevent, investigate or prosecute criminal offences.3. Deployers of an emotion recognition system or a biometric categorisation system shall inform the natural personsexposed thereto of the operation of the system, and shall process the personal data in accordance with Regulations (EU)2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, as applicable. This obligation shall not apply to AI systemsused for biometric categorisation and emotion recognition, which are permitted by law to detect, prevent or investigatecriminal offences, subject to appropriate safeguards for the rights and freedoms of third parties, and in accordance with Union law.4. Deployers of an AI system that generates or manipulates image, audio or video content constituting a deep fake, shalldisclose that the content has been artificially generated or manipulated. This obligation shall not apply where the use isauthorised by law to detect, prevent, investigate or prosecute criminal offence. Where the content forms part of an evidentlyartistic, creative, satirical, fictional or analogous work or programme, the transparency obligations set out in this paragraphare limited to disclosure of the existence of such generated or manipulated content in an appropriate manner that does nothamper the display or enjoyment of the work. Deployers of an AI system that generates or manipulates text which is published with the purpose of informing the publicon matters of public interest shall disclose that the text has been artificially generated or manipulated. This obligation shallnot apply where the use is authorised by law to detect, prevent, investigate or prosecute criminal offences or where the5. The information referred to in paragraphs 1 to 4 shall be provided to the natural persons concerned in a clear anddistinguishable manner at the latest at the time of the first interaction or exposure. The information shall conform to theapplicable accessibility requirements.6. Paragraphs 1 to 4 shall not affect the requirements and obligations set out in Chapter III, and shall be withoutprejudice to other transparency obligations laid down in Union or national law for deployers of AI systems.7. The AI Office shall encourage and facilitate the drawing up of codes of practice at Union level to facilitate the effectiveimplementation of the obligations regarding the detection and labelling of artificially generated or manipulated content. The Commission may adopt implementing acts to approve those codes of practice in accordance with the procedure laiddown in Article 56 (6). If it deems the code is not adequate, the Commission may adopt an implementing act specifyingcommon rules for the implementation of those obligations in accordance with the examination procedure laid down in Article 98(2). CHAPTER VGENERAL-PURPOSE AI MODELSSECTION 1Classification rules Article 51Classification of general-purpose AI models as general-purpose AI models with systemic risk1. A general-purpose AI model shall be classified as a general-purpose AI model with systemic risk if it meets any of thefollowing conditions:(a) it has high impact capabilities evaluated on the basis of appropriate technical tools and methodologies, includingindicators and benchmarks;(b) based on a decision of the Commission, ex officio or following a qualified alert from the scientific panel, it hascapabilities or an impact equivalent to those set out in point (a) having regard to the criteria set out in Annex XIII.2. A general-purpose AI model shall be presumed to have high impact capabilities pursuant to paragraph 1, point (a), when the cumulative amount of computation used for its training measured in floating point operations is greater than1025.3. The Commission shall adopt delegated acts in accordance with Article 97 to amend the thresholds listed inparagraphs 1 and 2 of this Article, as well as to supplement benchmarks and indicators in light of evolving technologicaldevelopments, such as algorithmic improvements or increased hardware efficiency, when necessary, for these thresholds toreflect the state of the art. Article 52Procedure1. Where a general-purpose AI model meets the condition referred to in Article 51(1), point (a), the relevant providershall notify the Commission without delay and in any event within two weeks after that requirement is met or it becomesknown that it will be met. That notification shall include the information necessary to demonstrate that the relevantrequirement has been met. If the Commission becomes aware of a general-purpose AI model presenting systemic risks ofwhich it has not been notified, it may decide to designate it as a model with systemic risk.2. The provider of a general-purpose AI model that meets the condition referred to in Article 51(1), point (a), maypresent, with its notification, sufficiently substantiated arguments to demonstrate that, exceptionally, although it meets that3. Where the Commission concludes that the arguments submitted pursuant to paragraph 2 are not sufficientlysubstantiated and the relevant provider was not able to demonstrate that the general-purpose AI model does not present, due to its specific characteristics, systemic risks, it shall reject those arguments, and the general-purpose AI model shall beconsidered to be a general-purpose AI model with systemic risk.4. The Commission may designate a general-purpose AI model as presenting systemic risks, ex officio or followinga qualified alert from the scientific panel pursuant to Article 90(1), point (a), on the basis of criteria set out in Annex XIII. The Commission is empowered to adopt delegated acts in accordance with Article 97 in order to amend Annex XIII byspecifying and updating the criteria set out in that Annex.5. Upon a reasoned request of a provider whose model has been designated as a general-purpose AI model with systemicrisk pursuant to paragraph 4, the Commission shall take the request into account and may decide to reassess whether thegeneral-purpose AI model can still be considered to present systemic risks on the basis of the criteria set out in Annex XIII. Such a request shall contain objective, detailed and new reasons that have arisen since the designation decision. Providersmay request reassessment at the earliest six months after the designation decision. Where the Commission, following itsreassessment, decides to maintain the designation as a general-purpose AI model with systemic risk, providers may requestreassessment at the earliest six months after that decision.6. The Commission shall ensure that a list of general-purpose AI models with systemic risk is published and shall keepthat list up to date, without prejudice to the need to observe and protect intellectual property rights and confidentialbusiness information or trade secrets in accordance with Union and national law. SECTION 2Obligations for providers of general-purpose AI models Article 53Obligations for providers of general-purpose AI models1. Providers of general-purpose AI models shall:(a) draw up and keep up-to-date the technical documentation of the model, including its training and testing process andthe results of its evaluation, which shall contain, at a minimum, the information set out in Annex XI for the purpose ofproviding it, upon request, to the AI Office and the national competent authorities;(b) draw up, keep up-to-date and make available information and documentation to providers of AI systems who intend tointegrate the general-purpose AI model into their AI systems. Without prejudice to the need to observe and protectintellectual property rights and confidential business information or trade secrets in accordance with Union andnational law, the information and documentation shall:(i) enable providers of AI systems to have a good understanding of the capabilities and limitations of thegeneral-purpose AI model and to comply with their obligations pursuant to this Regulation; and(ii) contain, at a minimum, the elements set out in Annex XII;(c) put in place a policy to comply with Union law on copyright and related rights, and in particular to identify and complywith, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790;2. The obligations set out in paragraph 1, points (a) and (b), shall not apply to providers of AI models that are releasedunder a free and open-source licence that allows for the access, usage, modification, and distribution of the model, andwhose parameters, including the weights, the information on the model architecture, and the information on model usage, are made publicly available. This exception shall not apply to general-purpose AI models with systemic risks.3. Providers of general-purpose AI models shall cooperate as necessary with the Commission and the nationalcompetent authorities in the exercise of their competences and powers pursuant to this Regulation.4. Providers of general-purpose AI models may rely on codes of practice within the meaning of Article 56 todemonstrate compliance with the obligations set out in paragraph 1 of this Article, until a harmonised standard ispublished. Compliance with European harmonised standards grants providers the presumption of conformity to the extentthat those standards cover those obligations. Providers of general-purpose AI models who do not adhere to an approvedcode of practice or do not comply with a European harmonised standard shall demonstrate alternative adequate means ofcompliance for assessment by the Commission.5. For the purpose of facilitating compliance with Annex XI, in particular points 2 (d) and (e) thereof, the Commission isempowered to adopt delegated acts in accordance with Article 97 to detail measurement and calculation methodologieswith a view to allowing for comparable and verifiable documentation.6. The Commission is empowered to adopt delegated acts in accordance with Article 97(2) to amend Annexes XI and XIIin light of evolving technological developments.7. Any information or documentation obtained pursuant to this Article, including trade secrets, shall be treated inaccordance with the confidentiality obligations set out in Article 78. Article 54Authorised representatives of providers of general-purpose AI models1. Prior to placing a general-purpose AI model on the Union market, providers established in third countries shall, bywritten mandate, appoint an authorised representative which is established in the Union.2. The provider shall enable its authorised representative to perform the tasks specified in the mandate received from theprovider.3. The authorised representative shall perform the tasks specified in the mandate received from the provider. It shallprovide a copy of the mandate to the AI Office upon request, in one of the official languages of the institutions of the Union. For the purposes of this Regulation, the mandate shall empower the authorised representative to carry out thefollowing tasks:(a) verify that the technical documentation specified in Annex XI has been drawn up and all obligations referred to in Article 53 and, where applicable, Article 55 have been fulfilled by the provider;(b) keep a copy of the technical documentation specified in Annex XI at the disposal of the AI Office and nationalcompetent authorities, for a period of 10 years after the general-purpose AI model has been placed on the market, andthe contact details of the provider that appointed the authorised representative;(c) provide the AI Office, upon a reasoned request, with all the information and documentation, including that referred toin point (b), necessary to demonstrate compliance with the obligations in this Chapter;(d) cooperate with the AI Office and competent authorities, upon a reasoned request, in any action they take in relation tothe general-purpose AI model, including when the model is integrated into AI systems placed on the market or put intoservice in the Union.5. The authorised representative shall terminate the mandate if it considers or has reason to consider the provider to beacting contrary to its obligations pursuant to this Regulation. In such a case, it shall also immediately inform the AI Officeabout the termination of the mandate and the reasons therefor.6. The obligation set out in this Article shall not apply to providers of general-purpose AI models that are released undera free and open-source licence that allows for the access, usage, modification, and distribution of the model, and whoseparameters, including the weights, the information on the model architecture, and the information on model usage, aremade publicly available, unless the general-purpose AI models present systemic risks. SECTION 3Obligations of providers of general-purpose AI models with systemic risk Article 55Obligations of providers of general-purpose AI models with systemic risk1. In addition to the obligations listed in Articles 53 and 54, providers of general-purpose AI models with systemic riskshall:(a) perform model evaluation in accordance with standardised protocols and tools reflecting the state of the art, includingconducting and documenting adversarial testing of the model with a view to identifying and mitigating systemic risks;(b) assess and mitigate possible systemic risks at Union level, including their sources, that may stem from the development, the placing on the market, or the use of general-purpose AI models with systemic risk;(c) keep track of, document, and report, without undue delay, to the AI Office and, as appropriate, to national competentauthorities, relevant information about serious incidents and possible corrective measures to address them;(d) ensure an adequate level of cybersecurity protection for the general-purpose AI model with systemic risk and thephysical infrastructure of the model.2. Providers of general-purpose AI models with systemic risk may rely on codes of practice within the meaning of Article 56 to demonstrate compliance with the obligations set out in paragraph 1 of this Article, until a harmonisedstandard is published. Compliance with European harmonised standards grants providers the presumption of conformity tothe extent that those standards cover those obligations. Providers of general-purpose AI models with systemic risks who donot adhere to an approved code of practice or do not comply with a European harmonised standard shall demonstratealternative adequate means of compliance for assessment by the Commission.3. Any information or documentation obtained pursuant to this Article, including trade secrets, shall be treated inaccordance with the confidentiality obligations set out in Article 78. SECTION 4Codes of practice Article 56Codes of practice1. The AI Office shall encourage and facilitate the drawing up of codes of practice at Union level in order to contributeto the proper application of this Regulation, taking into account international approaches.(a) the means to ensure that the information referred to in Article 53(1), points (a) and (b), is kept up to date in light ofmarket and technological developments;(b) the adequate level of detail for the summary about the content used for training;(c) the identification of the type and nature of the systemic risks at Union level, including their sources, where appropriate;(d) the measures, procedures and modalities for the assessment and management of the systemic risks at Union level, including the documentation thereof, which shall be proportionate to the risks, take into consideration their severityand probability and take into account the specific challenges of tackling those risks in light of the possible ways inwhich such risks may emerge and materialise along the AI value chain.3. The AI Office may invite all providers of general-purpose AI models, as well as relevant national competentauthorities, to participate in the drawing-up of codes of practice. Civil society organisations, industry, academia and otherrelevant stakeholders, such as downstream providers and independent experts, may support the process.4. The AI Office and the Board shall aim to ensure that the codes of practice clearly set out their specific objectives andcontain commitments or measures, including key performance indicators as appropriate, to ensure the achievement ofthose objectives, and that they take due account of the needs and interests of all interested parties, including affectedpersons, at Union level.5. The AI Office shall aim to ensure that participants to the codes of practice report regularly to the AI Office on theimplementation of the commitments and the measures taken and their outcomes, including as measured against the keyperformance indicators as appropriate. Key performance indicators and reporting commitments shall reflect differences insize and capacity between various participants.6. The AI Office and the Board shall regularly monitor and evaluate the achievement of the objectives of the codes ofpractice by the participants and their contribution to the proper application of this Regulation. The AI Office and the Boardshall assess whether the codes of practice cover the obligations provided for in Articles 53 and 55, and shall regularlymonitor and evaluate the achievement of their objectives. They shall publish their assessment of the adequacy of the codesof practice. The Commission may, by way of an implementing act, approve a code of practice and give it a general validity within the Union. That implementing act shall be adopted in accordance with the examination procedure referred to in Article 98(2).7. The AI Office may invite all providers of general-purpose AI models to adhere to the codes of practice. For providersof general-purpose AI models not presenting systemic risks this adherence may be limited to the obligations provided for in Article 53, unless they declare explicitly their interest to join the full code.8. The AI Office shall, as appropriate, also encourage and facilitate the review and adaptation of the codes of practice, inparticular in light of emerging standards. The AI Office shall assist in the assessment of available standards.9. Codes of practice shall be ready at the latest by 2 May 2025. The AI Office shall take the necessary steps, includinginviting providers pursuant to paragraph 7. If, by 2 August 2025, a code of practice cannot be finalised, or if the AI Office deems it is not adequate following itsassessment under paragraph 6 of this Article, the Commission may provide, by means of implementing acts, common rulesfor the implementation of the obligations provided for in Articles 53 and 55, including the issues set out in paragraph 2 of CHAPTER VIMEASURES IN SUPPORT OF INNOVATIONArticle 57AI regulatory sandboxes1. Member States shall ensure that their competent authorities establish at least one AI regulatory sandbox at nationallevel, which shall be operational by 2 August 2026. That sandbox may also be established jointly with the competentauthorities of other Member States. The Commission may provide technical support, advice and tools for the establishmentand operation of AI regulatory sandboxes. The obligation under the first subparagraph may also be fulfilled by participating in an existing sandbox in so far as thatparticipation provides an equivalent level of national coverage for the participating Member States.2. Additional AI regulatory sandboxes at regional or local level, or established jointly with the competent authorities ofother Member States may also be established.3. The European Data Protection Supervisor may also establish an AI regulatory sandbox for Union institutions, bodies, offices and agencies, and may exercise the roles and the tasks of national competent authorities in accordance with this Chapter.4. Member States shall ensure that the competent authorities referred to in paragraphs 1 and 2 allocate sufficientresources to comply with this Article effectively and in a timely manner. Where appropriate, national competent authoritiesshall cooperate with other relevant authorities, and may allow for the involvement of other actors within the AI ecosystem. This Article shall not affect other regulatory sandboxes established under Union or national law. Member States shall ensurean appropriate level of cooperation between the authorities supervising those other sandboxes and the national competentauthorities.5. AI regulatory sandboxes established under paragraph 1 shall provide for a controlled environment that fostersinnovation and facilitates the development, training, testing and validation of innovative AI systems for a limited timebefore their being placed on the market or put into service pursuant to a specific sandbox plan agreed between theproviders or prospective providers and the competent authority. Such sandboxes may include testing in real worldconditions supervised therein.6. Competent authorities shall provide, as appropriate, guidance, supervision and support within the AI regulatorysandbox with a view to identifying risks, in particular to fundamental rights, health and safety, testing, mitigation measures, and their effectiveness in relation to the obligations and requirements of this Regulation and, where relevant, other Unionand national law supervised within the sandbox.7. Competent authorities shall provide providers and prospective providers participating in the AI regulatory sandboxwith guidance on regulatory expectations and how to fulfil the requirements and obligations set out in this Regulation. Upon request of the provider or prospective provider of the AI system, the competent authority shall provide a writtenproof of the activities successfully carried out in the sandbox. The competent authority shall also provide an exit reportdetailing the activities carried out in the sandbox and the related results and learning outcomes. Providers may use suchdocumentation to demonstrate their compliance with this Regulation through the conformity assessment process orrelevant market surveillance activities. In this regard, the exit reports and the written proof provided by the nationalcompetent authority shall be taken positively into account by market surveillance authorities and notified bodies, witha view to accelerating conformity assessment procedures to a reasonable extent.8. Subject to the confidentiality provisions in Article 78, and with the agreement of the provider or prospective provider, the Commission and the Board shall be authorised to access the exit reports and shall take them into account, asappropriate, when exercising their tasks under this Regulation. If both the provider or prospective provider and the nationalcompetent authority explicitly agree, the exit report may be made publicly available through the single informationplatform referred to in this Article.9. The establishment of AI regulatory sandboxes shall aim to contribute to the following objectives:(b) supporting the sharing of best practices through cooperation with the authorities involved in the AI regulatorysandbox;(c) fostering innovation and competitiveness and facilitating the development of an AI ecosystem;(d) contributing to evidence-based regulatory learning;(e) facilitating and accelerating access to the Union market for AI systems, in particular when provided by SMEs, includingstart-ups.10. National competent authorities shall ensure that, to the extent the innovative AI systems involve the processing ofpersonal data or otherwise fall under the supervisory remit of other national authorities or competent authorities providingor supporting access to data, the national data protection authorities and those other national or competent authorities areassociated with the operation of the AI regulatory sandbox and involved in the supervision of those aspects to the extent oftheir respective tasks and powers.11. The AI regulatory sandboxes shall not affect the supervisory or corrective powers of the competent authoritiessupervising the sandboxes, including at regional or local level. Any significant risks to health and safety and fundamentalrights identified during the development and testing of such AI systems shall result in an adequate mitigation. Nationalcompetent authorities shall have the power to temporarily or permanently suspend the testing process, or the participationin the sandbox if no effective mitigation is possible, and shall inform the AI Office of such decision. National competentauthorities shall exercise their supervisory powers within the limits of the relevant law, using their discretionary powerswhen implementing legal provisions in respect of a specific AI regulatory sandbox project, with the objective of supportinginnovation in AI in the Union.12. Providers and prospective providers participating in the AI regulatory sandbox shall remain liable under applicable Union and national liability law for any damage inflicted on third parties as a result of the experimentation taking place inthe sandbox. However, provided that the prospective providers observe the specific plan and the terms and conditions fortheir participation and follow in good faith the guidance given by the national competent authority, no administrative finesshall be imposed by the authorities for infringements of this Regulation. Where other competent authorities responsible forother Union and national law were actively involved in the supervision of the AI system in the sandbox and providedguidance for compliance, no administrative fines shall be imposed regarding that law.13. The AI regulatory sandboxes shall be designed and implemented in such a way that, where relevant, they facilitatecross-border cooperation between national competent authorities.14. National competent authorities shall coordinate their activities and cooperate within the framework of the Board.15. National competent authorities shall inform the AI Office and the Board of the establishment of a sandbox, and mayask them for support and guidance. The AI Office shall make publicly available a list of planned and existing sandboxes andkeep it up to date in order to encourage more interaction in the AI regulatory sandboxes and cross-border cooperation.16. National competent authorities shall submit annual reports to the AI Office and to the Board, from one year afterthe establishment of the AI regulatory sandbox and every year thereafter until its termination, and a final report. Thosereports shall provide information on the progress and results of the implementation of those sandboxes, including bestpractices, incidents, lessons learnt and recommendations on their setup and, where relevant, on the application and possiblerevision of this Regulation, including its delegated and implementing acts, and on the application of other Union lawsupervised by the competent authorities within the sandbox. The national competent authorities shall make those annualreports or abstracts thereof available to the public, online. The Commission shall, where appropriate, take the annualreports into account when exercising its tasks under this Regulation.17. The Commission shall develop a single and dedicated interface containing all relevant information related to AIregulatory sandboxes to allow stakeholders to interact with AI regulatory sandboxes and to raise enquiries with competentauthorities, and to seek non-binding guidance on the conformity of innovative products, services, business models Article 58Detailed arrangements for, and functioning of, AI regulatory sandboxes1. In order to avoid fragmentation across the Union, the Commission shall adopt implementing acts specifying thedetailed arrangements for the establishment, development, implementation, operation and supervision of the AI regulatorysandboxes. The implementing acts shall include common principles on the following issues:(a) eligibility and selection criteria for participation in the AI regulatory sandbox;(b) procedures for the application, participation, monitoring, exiting from and termination of the AI regulatory sandbox, including the sandbox plan and the exit report;(c) the terms and conditions applicable to the participants. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article 98(2).2. The implementing acts referred to in paragraph 1 shall ensure:(a) that AI regulatory sandboxes are open to any applying provider or prospective provider of an AI system who fulfilseligibility and selection criteria, which shall be transparent and fair, and that national competent authorities informapplicants of their decision within three months of the application;(b) that AI regulatory sandboxes allow broad and equal access and keep up with demand for participation; providers andprospective providers may also submit applications in partnerships with deployers and other relevant third parties;(c) that the detailed arrangements for, and conditions concerning AI regulatory sandboxes support, to the best extentpossible, flexibility for national competent authorities to establish and operate their AI regulatory sandboxes;(d) that access to the AI regulatory sandboxes is free of charge for SMEs, including start-ups, without prejudice toexceptional costs that national competent authorities may recover in a fair and proportionate manner;(e) that they facilitate providers and prospective providers, by means of the learning outcomes of the AI regulatorysandboxes, in complying with conformity assessment obligations under this Regulation and the voluntary applicationof the codes of conduct referred to in Article 95;(f) that AI regulatory sandboxes facilitate the involvement of other relevant actors within the AI ecosystem, such as notifiedbodies and standardisation organisations, SMEs, including start-ups, enterprises, innovators, testing and experimenta-tion facilities, research and experimentation labs and European Digital Innovation Hubs, centres of excellence, individual researchers, in order to allow and facilitate cooperation with the public and private sectors;(g) that procedures, processes and administrative requirements for application, selection, participation and exiting the AIregulatory sandbox are simple, easily intelligible, and clearly communicated in order to facilitate the participation of SMEs, including start-ups, with limited legal and administrative capacities and are streamlined across the Union, in orderto avoid fragmentation and that participation in an AI regulatory sandbox established by a Member State, or by the European Data Protection Supervisor is mutually and uniformly recognised and carries the same legal effects across the Union;(h) that participation in the AI regulatory sandbox is limited to a period that is appropriate to the complexity and scale ofthe project and that may be extended by the national competent authority;(i) that AI regulatory sandboxes facilitate the development of tools and infrastructure for testing, benchmarking, assessingand explaining dimensions of AI systems relevant for regulatory learning, such as accuracy, robustness andcybersecurity, as well as measures to mitigate risks to fundamental rights and society at large.3. Prospective providers in the AI regulatory sandboxes, in particular SMEs and start-ups, shall be directed, whererelevant, to pre-deployment services such as guidance on the implementation of this Regulation, to other value-adding4. Where national competent authorities consider authorising testing in real world conditions supervised within theframework of an AI regulatory sandbox to be established under this Article, they shall specifically agree the terms andconditions of such testing and, in particular, the appropriate safeguards with the participants, with a view to protectingfundamental rights, health and safety. Where appropriate, they shall cooperate with other national competent authoritieswith a view to ensuring consistent practices across the Union. Article 59Further processing of personal data for developing certain AI systems in the public interest in the AI regulatorysandbox1. In the AI regulatory sandbox, personal data lawfully collected for other purposes may be processed solely for thepurpose of developing, training and testing certain AI systems in the sandbox when all of the following conditions are met:(a) AI systems shall be developed for safeguarding substantial public interest by a public authority or another natural orlegal person and in one or more of the following areas:(i) public safety and public health, including disease detection, diagnosis prevention, control and treatment andimprovement of health care systems;(ii) a high level of protection and improvement of the quality of the environment, protection of biodiversity, protectionagainst pollution, green transition measures, climate change mitigation and adaptation measures;(iii) energy sustainability;(iv) safety and resilience of transport systems and mobility, critical infrastructure and networks;(v) efficiency and quality of public administration and public services;(b) the data processed are necessary for complying with one or more of the requirements referred to in Chapter III, Section 2 where those requirements cannot effectively be fulfilled by processing anonymised, synthetic or othernon-personal data;(c) there are effective monitoring mechanisms to identify if any high risks to the rights and freedoms of the data subjects, asreferred to in Article 35 of Regulation (EU) 2016/679 and in Article 39 of Regulation (EU) 2018/1725, may ariseduring the sandbox experimentation, as well as response mechanisms to promptly mitigate those risks and, wherenecessary, stop the processing;(d) any personal data to be processed in the context of the sandbox are in a functionally separate, isolated and protecteddata processing environment under the control of the prospective provider and only authorised persons have access tothose data;(e) providers can further share the originally collected data only in accordance with Union data protection law; anypersonal data created in the sandbox cannot be shared outside the sandbox;(f) any processing of personal data in the context of the sandbox neither leads to measures or decisions affecting the datasubjects nor does it affect the application of their rights laid down in Union law on the protection of personal data;(g) any personal data processed in the context of the sandbox are protected by means of appropriate technical andorganisational measures and deleted once the participation in the sandbox has terminated or the personal data hasreached the end of its retention period;(h) the logs of the processing of personal data in the context of the sandbox are kept for the duration of the participation inthe sandbox, unless provided otherwise by Union or national law;(j) a short summary of the AI project developed in the sandbox, its objectives and expected results is published on thewebsite of the competent authorities; this obligation shall not cover sensitive operational data in relation to the activitiesof law enforcement, border control, immigration or asylum authorities.2. For the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution ofcriminal penalties, including safeguarding against and preventing threats to public security, under the control andresponsibility of law enforcement authorities, the processing of personal data in AI regulatory sandboxes shall be based ona specific Union or national law and subject to the same cumulative conditions as referred to in paragraph 1.3. Paragraph 1 is without prejudice to Union or national law which excludes processing of personal data for otherpurposes than those explicitly mentioned in that law, as well as to Union or national law laying down the basis for theprocessing of personal data which is necessary for the purpose of developing, testing or training of innovative AI systems orany other legal basis, in compliance with Union law on the protection of personal data. Article 60Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes1. Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes may be conducted byproviders or prospective providers of high-risk AI systems listed in Annex III, in accordance with this Article and thereal-world testing plan referred to in this Article, without prejudice to the prohibitions under Article 5. The Commission shall, by means of implementing acts, specify the detailed elements of the real-world testing plan. Thoseimplementing acts shall be adopted in accordance with the examination procedure referred to in Article 98(2). This paragraph shall be without prejudice to Union or national law on the testing in real world conditions of high-risk AIsystems related to products covered by Union harmonisation legislation listed in Annex I.2. Providers or prospective providers may conduct testing of high-risk AI systems referred to in Annex III in real worldconditions at any time before the placing on the market or the putting into service of the AI system on their own or inpartnership with one or more deployers or prospective deployers.3. The testing of high-risk AI systems in real world conditions under this Article shall be without prejudice to any ethicalreview that is required by Union or national law.4. Providers or prospective providers may conduct the testing in real world conditions only where all of the followingconditions are met:(a) the provider or prospective provider has drawn up a real-world testing plan and submitted it to the market surveillanceauthority in the Member State where the testing in real world conditions is to be conducted;(b) the market surveillance authority in the Member State where the testing in real world conditions is to be conducted hasapproved the testing in real world conditions and the real-world testing plan; where the market surveillance authorityhas not provided an answer within 30 days, the testing in real world conditions and the real-world testing plan shall beunderstood to have been approved; where national law does not provide for a tacit approval, the testing in real worldconditions shall remain subject to an authorisation;(c) the provider or prospective provider, with the exception of providers or prospective providers of high-risk AI systemsreferred to in points 1, 6 and 7 of Annex III in the areas of law enforcement, migration, asylum and border controlmanagement, and high-risk AI systems referred to in point 2 of Annex III has registered the testing in real worldconditions in accordance with Article 71(4) with a Union-wide unique single identification number and with theinformation specified in Annex IX; the provider or prospective provider of high-risk AI systems referred to in points 1,6 and 7 of Annex III in the areas of law enforcement, migration, asylum and border control management, has registeredthe testing in real-world conditions in the secure non-public section of the EU database according to Article 49(4), point(d), with a Union-wide unique single identification number and with the information specified therein; the provider or(d) the provider or prospective provider conducting the testing in real world conditions is established in the Union or hasappointed a legal representative who is established in the Union;(e) data collected and processed for the purpose of the testing in real world conditions shall be transferred to thirdcountries only provided that appropriate and applicable safeguards under Union law are implemented;(f) the testing in real world conditions does not last longer than necessary to achieve its objectives and in any case notlonger than six months, which may be extended for an additional period of six months, subject to prior notification bythe provider or prospective provider to the market surveillance authority, accompanied by an explanation of the needfor such an extension;(g) the subjects of the testing in real world conditions who are persons belonging to vulnerable groups due to their age ordisability, are appropriately protected;(h) where a provider or prospective provider organises the testing in real world conditions in cooperation with one or moredeployers or prospective deployers, the latter have been informed of all aspects of the testing that are relevant to theirdecision to participate, and given the relevant instructions for use of the AI system referred to in Article 13; theprovider or prospective provider and the deployer or prospective deployershall conclude an agreement specifying theirroles and responsibilities with a view to ensuring compliance with the provisions for testing in real world conditionsunder this Regulation and under other applicable Union and national law;(i) the subjects of the testing in real world conditions have given informed consent in accordance with Article 61, or in thecase of law enforcement, where the seeking of informed consent would prevent the AI system from being tested, thetesting itself and the outcome of the testing in the real world conditions shall not have any negative effect on thesubjects, and their personal data shall be deleted after the test is performed;(j) the testing in real world conditions is effectively overseen by the provider or prospective provider, as well as bydeployers or prospective deployers through persons who are suitably qualified in the relevant field and have thenecessary capacity, training and authority to perform their tasks;(k) the predictions, recommendations or decisions of the AI system can be effectively reversed and disregarded.5. Any subjects of the testing in real world conditions, or their legally designated representative, as appropriate, may, without any resulting detriment and without having to provide any justification, withdraw from the testing at any time byrevoking their informed consent and may request the immediate and permanent deletion of their personal data. Thewithdrawal of the informed consent shall not affect the activities already carried out.6. In accordance with Article 75, Member States shall confer on their market surveillance authorities the powers ofrequiring providers and prospective providers to provide information, of carrying out unannounced remote or on-siteinspections, and of performing checks on the conduct of the testing in real world conditions and the related high-risk AIsystems. Market surveillance authorities shall use those powers to ensure the safe development of testing in real worldconditions.7. Any serious incident identified in the course of the testing in real world conditions shall be reported to the nationalmarket surveillance authority in accordance with Article 73. The provider or prospective provider shall adopt immediatemitigation measures or, failing that, shall suspend the testing in real world conditions until such mitigation takes place, orotherwise terminate it. The provider or prospective provider shall establish a procedure for the prompt recall of the AIsystem upon such termination of the testing in real world conditions.8. Providers or prospective providers shall notify the national market surveillance authority in the Member State wherethe testing in real world conditions is to be conducted of the suspension or termination of the testing in real worldconditions and of the final outcomes. Article 61Informed consent to participate in testing in real world conditions outside AI regulatory sandboxes1. For the purpose of testing in real world conditions under Article 60, freely-given informed consent shall be obtainedfrom the subjects of testing prior to their participation in such testing and after their having been duly informed withconcise, clear, relevant, and understandable information regarding:(a) the nature and objectives of the testing in real world conditions and the possible inconvenience that may be linked totheir participation;(b) the conditions under which the testing in real world conditions is to be conducted, including the expected duration ofthe subject or subjects’ participation;(c) their rights, and the guarantees regarding their participation, in particular their right to refuse to participate in, and theright to withdraw from, testing in real world conditions at any time without any resulting detriment and without havingto provide any justification;(d) the arrangements for requesting the reversal or the disregarding of the predictions, recommendations or decisions ofthe AI system;(e) the Union-wide unique single identification number of the testing in real world conditions in accordance with Article60(4) point (c), and the contact details of the provider or its legal representative from whom further information can beobtained.2. The informed consent shall be dated and documented and a copy shall be given to the subjects of testing or their legalrepresentative. Article 62Measures for providers and deployers, in particular SMEs, including start-ups1. Member States shall undertake the following actions:(a) provide SMEs, including start-ups, having a registered office or a branch in the Union, with priority access to the AIregulatory sandboxes, to the extent that they fulfil the eligibility conditions and selection criteria; the priority accessshall not preclude other SMEs, including start-ups, other than those referred to in this paragraph from access to the AIregulatory sandbox, provided that they also fulfil the eligibility conditions and selection criteria;(b) organise specific awareness raising and training activities on the application of this Regulation tailored to the needs of SMEs including start-ups, deployers and, as appropriate, local public authorities;(c) utilise existing dedicated channels and where appropriate, establish new ones for communication with SMEs includingstart-ups, deployers, other innovators and, as appropriate, local public authorities to provide advice and respond toqueries about the implementation of this Regulation, including as regards participation in AI regulatory sandboxes;(d) facilitate the participation of SMEs and other relevant stakeholders in the standardisation development process.2. The specific interests and needs of the SME providers, including start-ups, shall be taken into account when setting thefees for conformity assessment under Article 43, reducing those fees proportionately to their size, market size and otherrelevant indicators.3. The AI Office shall undertake the following actions:(a) provide standardised templates for areas covered by this Regulation, as specified by the Board in its request;(c) organise appropriate communication campaigns to raise awareness about the obligations arising from this Regulation;(d) evaluate and promote the convergence of best practices in public procurement procedures in relation to AI systems. Article 63Derogations for specific operators1. Microenterprises within the meaning of Recommendation 2003/361/EC may comply with certain elements of thequality management system required by Article 17 of this Regulation in a simplified manner, provided that they do nothave partner enterprises or linked enterprises within the meaning of that Recommendation. For that purpose, the Commission shall develop guidelines on the elements of the quality management system which may be complied with ina simplified manner considering the needs of microenterprises, without affecting the level of protection or the need forcompliance with the requirements in respect of high-risk AI systems.2. Paragraph 1 of this Article shall not be interpreted as exempting those operators from fulfilling any otherrequirements or obligations laid down in this Regulation, including those established in Articles 9, 10, 11, 12, 13, 14, 15,72 and 73. CHAPTER VIIGOVERNANCESECTION 1Governance at Union level Article 64AI Office1. The Commission shall develop Union expertise and capabilities in the field of AI through the AI Office.2. Member States shall facilitate the tasks entrusted to the AI Office, as reflected in this Regulation. Article 65Establishment and structure of the European Artificial Intelligence Board1. A European Artificial Intelligence Board (the ‘Board’) is hereby established.2. The Board shall be composed of one representative per Member State. The European Data Protection Supervisor shallparticipate as observer. The AI Office shall also attend the Board’s meetings, without taking part in the votes. Other nationaland Union authorities, bodies or experts may be invited to the meetings by the Board on a case by case basis, where theissues discussed are of relevance for them.3. Each representative shall be designated by their Member State for a period of three years, renewable once.4. Member States shall ensure that their representatives on the Board:(a) have the relevant competences and powers in their Member State so as to contribute actively to the achievement of the Board’s tasks referred to in Article 66;(c) are empowered to facilitate consistency and coordination between national competent authorities in their Member Stateas regards the implementation of this Regulation, including through the collection of relevant data and information forthe purpose of fulfilling their tasks on the Board.5. The designated representatives of the Member States shall adopt the Board’s rules of procedure by a two-thirdsmajority. The rules of procedure shall, in particular, lay down procedures for the selection process, the duration of themandate of, and specifications of the tasks of, the Chair, detailed arrangements for voting, and the organisation of the Board’s activities and those of its sub-groups.6. The Board shall establish two standing sub-groups to provide a platform for cooperation and exchange among marketsurveillance authorities and notifying authorities about issues related to market surveillance and notified bodies respectively. The standing sub-group for market surveillance should act as the administrative cooperation group (ADCO) for this Regulation within the meaning of Article 30 of Regulation (EU) 2019/1020. The Board may establish other standing or temporary sub-groups as appropriate for the purpose of examining specificissues. Where appropriate, representatives of the advisory forum referred to in Article 67 may be invited to such sub-groupsor to specific meetings of those subgroups as observers.7. The Board shall be organised and operated so as to safeguard the objectivity and impartiality of its activities.8. The Board shall be chaired by one of the representatives of the Member States. The AI Office shall provide thesecretariat for the Board, convene the meetings upon request of the Chair, and prepare the agenda in accordance with thetasks of the Board pursuant to this Regulation and its rules of procedure. Article 66Tasks of the Board The Board shall advise and assist the Commission and the Member States in order to facilitate the consistent and effectiveapplication of this Regulation. To that end, the Board may in particular:(a) contribute to the coordination among national competent authorities responsible for the application of this Regulationand, in cooperation with and subject to the agreement of the market surveillance authorities concerned, support jointactivities of market surveillance authorities referred to in Article 74(11);(b) collect and share technical and regulatory expertise and best practices among Member States;(c) provide advice on the implementation of this Regulation, in particular as regards the enforcement of rules ongeneral-purpose AI models;(d) contribute to the harmonisation of administrative practices in the Member States, including in relation to thederogation from the conformity assessment procedures referred to in Article 46, the functioning of AI regulatorysandboxes, and testing in real world conditions referred to in Articles 57, 59 and 60;(e) at the request of the Commission or on its own initiative, issue recommendations and written opinions on any relevantmatters related to the implementation of this Regulation and to its consistent and effective application, including:(i) on the development and application of codes of conduct and codes of practice pursuant to this Regulation, as wellas of the Commission’s guidelines;(ii) the evaluation and review of this Regulation pursuant to Article 112, including as regards the serious incidentreports referred to in Article 73, and the functioning of the EU database referred to in Article 71, the preparationof the delegated or implementing acts, and as regards possible alignments of this Regulation with the Unionharmonisation legislation listed in Annex I;(iv) on the use of harmonised standards or common specifications referred to in Articles 40 and 41;(v) trends, such as European global competitiveness in AI, the uptake of AI in the Union, and the development ofdigital skills;(vi) trends on the evolving typology of AI value chains, in particular on the resulting implications in terms ofaccountability;(vii) on the potential need for amendment to Annex III in accordance with Article 7, and on the potential need forpossible revision of Article 5 pursuant to Article 112, taking into account relevant available evidence and thelatest developments in technology;(f) support the Commission in promoting AI literacy, public awareness and understanding of the benefits, risks, safeguards and rights and obligations in relation to the use of AI systems;(g) facilitate the development of common criteria and a shared understanding among market operators and competentauthorities of the relevant concepts provided for in this Regulation, including by contributing to the development ofbenchmarks;(h) cooperate, as appropriate, with other Union institutions, bodies, offices and agencies, as well as relevant Union expertgroups and networks, in particular in the fields of product safety, cybersecurity, competition, digital and media services, financial services, consumer protection, data and fundamental rights protection;(i) contribute to effective cooperation with the competent authorities of third countries and with internationalorganisations;(j) assist national competent authorities and the Commission in developing the organisational and technical expertiserequired for the implementation of this Regulation, including by contributing to the assessment of training needs forstaff of Member States involved in implementing this Regulation;(k) assist the AI Office in supporting national competent authorities in the establishment and development of AIregulatory sandboxes, and facilitate cooperation and information-sharing among AI regulatory sandboxes;(l) contribute to, and provide relevant advice on, the development of guidance documents;(m) advise the Commission in relation to international matters on AI;(n) provide opinions to the Commission on the qualified alerts regarding general-purpose AI models;(o) receive opinions by the Member States on qualified alerts regarding general-purpose AI models, and on nationalexperiences and practices on the monitoring and enforcement of AI systems, in particular systems integrating thegeneral-purpose AI models. Article 67Advisory forum1. An advisory forum shall be established to provide technical expertise and advise the Board and the Commission, andto contribute to their tasks under this Regulation.2. The membership of the advisory forum shall represent a balanced selection of stakeholders, including industry, start-ups, SMEs, civil society and academia. The membership of the advisory forum shall be balanced with regard tocommercial and non-commercial interests and, within the category of commercial interests, with regard to SMEs and otherundertakings.4. The term of office of the members of the advisory forum shall be two years, which may be extended by up to no morethan four years.5. The Fundamental Rights Agency, ENISA, the European Committee for Standardization (CEN), the European Committee for Electrotechnical Standardization (CENELEC), and the European Telecommunications Standards Institute(ETSI) shall be permanent members of the advisory forum.6. The advisory forum shall draw up its rules of procedure. It shall elect two co-chairs from among its members, inaccordance with criteria set out in paragraph 2. The term of office of the co-chairs shall be two years, renewable once.7. The advisory forum shall hold meetings at least twice a year. The advisory forum may invite experts and otherstakeholders to its meetings.8. The advisory forum may prepare opinions, recommendations and written contributions at the request of the Board orthe Commission.9. The advisory forum may establish standing or temporary sub-groups as appropriate for the purpose of examiningspecific questions related to the objectives of this Regulation.10. The advisory forum shall prepare an annual report on its activities. That report shall be made publicly available. Article 68Scientific panel of independent experts1. The Commission shall, by means of an implementing act, make provisions on the establishment of a scientific panelof independent experts (the ‘scientific panel’) intended to support the enforcement activities under this Regulation. Thatimplementing act shall be adopted in accordance with the examination procedure referred to in Article 98(2).2. The scientific panel shall consist of experts selected by the Commission on the basis of up-to-date scientific ortechnical expertise in the field of AI necessary for the tasks set out in paragraph 3, and shall be able to demonstrate meetingall of the following conditions:(a) having particular expertise and competence and scientific or technical expertise in the field of AI;(b) independence from any provider of AI systems or general-purpose AI models;(c) an ability to carry out activities diligently, accurately and objectively. The Commission, in consultation with the Board, shall determine the number of experts on the panel in accordance withthe required needs and shall ensure fair gender and geographical representation.3. The scientific panel shall advise and support the AI Office, in particular with regard to the following tasks:(a) supporting the implementation and enforcement of this Regulation as regards general-purpose AI models and systems, in particular by:(i) alerting the AI Office of possible systemic risks at Union level of general-purpose AI models, in accordance with Article 90;(ii) contributing to the development of tools and methodologies for evaluating capabilities of general-purpose AImodels and systems, including through benchmarks;(iii) providing advice on the classification of general-purpose AI models with systemic risk;(v) contributing to the development of tools and templates;(b) supporting the work of market surveillance authorities, at their request;(c) supporting cross-border market surveillance activities as referred to in Article 74(11), without prejudice to the powersof market surveillance authorities;(d) supporting the AI Office in carrying out its duties in the context of the Union safeguard procedure pursuant to Article 81.4. The experts on the scientific panel shall perform their tasks with impartiality and objectivity, and shall ensure theconfidentiality of information and data obtained in carrying out their tasks and activities. They shall neither seek nor takeinstructions from anyone when exercising their tasks under paragraph 3. Each expert shall draw up a declaration ofinterests, which shall be made publicly available. The AI Office shall establish systems and procedures to actively manageand prevent potential conflicts of interest.5. The implementing act referred to in paragraph 1 shall include provisions on the conditions, procedures and detailedarrangements for the scientific panel and its members to issue alerts, and to request the assistance of the AI Office for theperformance of the tasks of the scientific panel. Article 69Access to the pool of experts by the Member States1. Member States may call upon experts of the scientific panel to support their enforcement activities under this Regulation.2. The Member States may be required to pay fees for the advice and support provided by the experts. The structure andthe level of fees as well as the scale and structure of recoverable costs shall be set out in the implementing act referred to in Article 68(1), taking into account the objectives of the adequate implementation of this Regulation, cost-effectiveness andthe necessity of ensuring effective access to experts for all Member States.3. The Commission shall facilitate timely access to the experts by the Member States, as needed, and ensure that thecombination of support activities carried out by Union AI testing support pursuant to Article 84 and experts pursuant tothis Article is efficiently organised and provides the best possible added value. SECTION 2National competent authorities Article 70Designation of national competent authorities and single points of contact1. Each Member State shall establish or designate as national competent authorities at least one notifying authority andat least one market surveillance authority for the purposes of this Regulation. Those national competent authorities shallexercise their powers independently, impartially and without bias so as to safeguard the objectivity of their activities andtasks, and to ensure the application and implementation of this Regulation. The members of those authorities shall refrainfrom any action incompatible with their duties. Provided that those principles are observed, such activities and tasks may beperformed by one or more designated authorities, in accordance with the organisational needs of the Member State.2. Member States shall communicate to the Commission the identity of the notifying authorities and the marketsurveillance authorities and the tasks of those authorities, as well as any subsequent changes thereto. Member States shallmake publicly available information on how competent authorities and single points of contact can be contacted, throughelectronic communication means by 2 August 2025. Member States shall designate a market surveillance authority to act as3. Member States shall ensure that their national competent authorities are provided with adequate technical, financialand human resources, and with infrastructure to fulfil their tasks effectively under this Regulation. In particular, the nationalcompetent authorities shall have a sufficient number of personnel permanently available whose competences and expertiseshall include an in-depth understanding of AI technologies, data and data computing, personal data protection, cybersecurity, fundamental rights, health and safety risks and knowledge of existing standards and legal requirements. Member States shall assess and, if necessary, update competence and resource requirements referred to in this paragraph onan annual basis.4. National competent authorities shall take appropriate measures to ensure an adequate level of cybersecurity.5. When performing their tasks, the national competent authorities shall act in accordance with the confidentialityobligations set out in Article 78.6. By 2 August 2025, and once every two years thereafter, Member States shall report to the Commission on the statusof the financial and human resources of the national competent authorities, with an assessment of their adequacy. The Commission shall transmit that information to the Board for discussion and possible recommendations.7. The Commission shall facilitate the exchange of experience between national competent authorities.8. National competent authorities may provide guidance and advice on the implementation of this Regulation, inparticular to SMEs including start-ups, taking into account the guidance and advice of the Board and the Commission, asappropriate. Whenever national competent authorities intend to provide guidance and advice with regard to an AI systemin areas covered by other Union law, the national competent authorities under that Union law shall be consulted, asappropriate.9. Where Union institutions, bodies, offices or agencies fall within the scope of this Regulation, the European Data Protection Supervisor shall act as the competent authority for their supervision. CHAPTER VIIIEU DATABASE FOR HIGH-RISK AI SYSTEMSArticle 71EU database for high-risk AI systems listed in Annex III1. The Commission shall, in collaboration with the Member States, set up and maintain an EU database containinginformation referred to in paragraphs 2 and 3 of this Article concerning high-risk AI systems referred to in Article 6(2)which are registered in accordance with Articles 49 and 60 and AI systems that are not considered as high-risk pursuant to Article 6(3) and which are registered in accordance with Article 6(4) and Article 49. When setting the functionalspecifications of such database, the Commission shall consult the relevant experts, and when updating the functionalspecifications of such database, the Commission shall consult the Board.2. The data listed in Sections A and B of Annex VIII shall be entered into the EU database by the provider or, whereapplicable, by the authorised representative.3. The data listed in Section C of Annex VIII shall be entered into the EU database by the deployer who is, or who acts onbehalf of, a public authority, agency or body, in accordance with Article 49(3) and (4).4. With the exception of the section referred to in Article 49(4) and Article 60(4), point (c), the information contained inthe EU database registered in accordance with Article 49 shall be accessible and publicly available in a user-friendly manner. The information should be easily navigable and machine-readable. The information registered in accordance with Article 60shall be accessible only to market surveillance authorities and the Commission, unless the prospective provider or providerhas given consent for also making the information accessible the public.5. The EU database shall contain personal data only in so far as necessary for collecting and processing information in6. The Commission shall be the controller of the EU database. It shall make available to providers, prospective providersand deployers adequate technical and administrative support. The EU database shall comply with the applicable accessibilityrequirements. CHAPTER IXPOST-MARKET MONITORING, INFORMATION SHARING AND MARKET SURVEILLANCESECTION 1Post-market monitoring Article 72Post-market monitoring by providers and post-market monitoring plan for high-risk AI systems1. Providers shall establish and document a post-market monitoring system in a manner that is proportionate to thenature of the AI technologies and the risks of the high-risk AI system.2. The post-market monitoring system shall actively and systematically collect, document and analyse relevant datawhich may be provided by deployers or which may be collected through other sources on the performance of high-risk AIsystems throughout their lifetime, and which allow the provider to evaluate the continuous compliance of AI systems withthe requirements set out in Chapter III, Section 2. Where relevant, post-market monitoring shall include an analysis of theinteraction with other AI systems. This obligation shall not cover sensitive operational data of deployers which arelaw-enforcement authorities.3. The post-market monitoring system shall be based on a post-market monitoring plan. The post-market monitoringplan shall be part of the technical documentation referred to in Annex IV. The Commission shall adopt an implementing actlaying down detailed provisions establishing a template for the post-market monitoring plan and the list of elements to beincluded in the plan by 2 February 2026. That implementing act shall be adopted in accordance with the examinationprocedure referred to in Article 98(2).4. For high-risk AI systems covered by the Union harmonisation legislation listed in Section A of Annex I, wherea post-market monitoring system and plan are already established under that legislation, in order to ensure consistency, avoid duplications and minimise additional burdens, providers shall have a choice of integrating, as appropriate, thenecessary elements described in paragraphs 1, 2 and 3 using the template referred in paragraph 3 into systems and plansalready existing under that legislation, provided that it achieves an equivalent level of protection. The first subparagraph of this paragraph shall also apply to high-risk AI systems referred to in point 5 of Annex III placedon the market or put into service by financial institutions that are subject to requirements under Union financial serviceslaw regarding their internal governance, arrangements or processes. SECTION 2Sharing of information on serious incidents Article 73Reporting of serious incidents2. The report referred to in paragraph 1 shall be made immediately after the provider has established a causal linkbetween the AI system and the serious incident or the reasonable likelihood of such a link, and, in any event, not later than15 days after the provider or, where applicable, the deployer, becomes aware of the serious incident. The period for the reporting referred to in the first subparagraph shall take account of the severity of the serious incident.3. Notwithstanding paragraph 2 of this Article, in the event of a widespread infringement or a serious incident asdefined in Article 3, point (49)(b), the report referred to in paragraph 1 of this Article shall be provided immediately, andnot later than two days after the provider or, where applicable, the deployer becomes aware of that incident.4. Notwithstanding paragraph 2, in the event of the death of a person, the report shall be provided immediately after theprovider or the deployer has established, or as soon as it suspects, a causal relationship between the high-risk AI system andthe serious incident, but not later than 10 days after the date on which the provider or, where applicable, the deployerbecomes aware of the serious incident.5. Where necessary to ensure timely reporting, the provider or, where applicable, the deployer, may submit an initialreport that is incomplete, followed by a complete report.6. Following the reporting of a serious incident pursuant to paragraph 1, the provider shall, without delay, perform thenecessary investigations in relation to the serious incident and the AI system concerned. This shall include a risk assessmentof the incident, and corrective action. The provider shall cooperate with the competent authorities, and where relevant with the notified body concerned, duringthe investigations referred to in the first subparagraph, and shall not perform any investigation which involves altering the AI system concerned in a way which may affect any subsequent evaluation of the causes of the incident, prior to informingthe competent authorities of such action.7. Upon receiving a notification related to a serious incident referred to in Article 3, point (49)(c), the relevant marketsurveillance authority shall inform the national public authorities or bodies referred to in Article 77(1). The Commissionshall develop dedicated guidance to facilitate compliance with the obligations set out in paragraph 1 of this Article. Thatguidance shall be issued by 2 August 2025, and shall be assessed regularly.8. The market surveillance authority shall take appropriate measures, as provided for in Article 19 of Regulation (EU)2019/1020, within seven days from the date it received the notification referred to in paragraph 1 of this Article, and shallfollow the notification procedures as provided in that Regulation.9. For high-risk AI systems referred to in Annex III that are placed on the market or put into service by providers that aresubject to Union legislative instruments laying down reporting obligations equivalent to those set out in this Regulation, thenotification of serious incidents shall be limited to those referred to in Article 3, point (49)(c).10. For high-risk AI systems which are safety components of devices, or are themselves devices, covered by Regulations(EU) 2017/745 and (EU) 2017/746, the notification of serious incidents shall be limited to those referred to in Article 3, point (49)(c) of this Regulation, and shall be made to the national competent authority chosen for that purpose by the Member States where the incident occurred.11. National competent authorities shall immediately notify the Commission of any serious incident, whether or notthey have taken action on it, in accordance with Article 20 of Regulation (EU) 2019/1020. SECTION 3Enforcement Article 74Market surveillance and control of AI systems in the Union market(a) any reference to an economic operator under Regulation (EU) 2019/1020 shall be understood as including all operatorsidentified in Article 2(1) of this Regulation;(b) any reference to a product under Regulation (EU) 2019/1020 shall be understood as including all AI systems fallingwithin the scope of this Regulation.2. As part of their reporting obligations under Article 34(4) of Regulation (EU) 2019/1020, the market surveillanceauthorities shall report annually to the Commission and relevant national competition authorities any informationidentified in the course of market surveillance activities that may be of potential interest for the application of Union law oncompetition rules. They shall also annually report to the Commission about the use of prohibited practices that occurredduring that year and about the measures taken.3. For high-risk AI systems related to products covered by the Union harmonisation legislation listed in Section A of Annex I, the market surveillance authority for the purposes of this Regulation shall be the authority responsible for marketsurveillance activities designated under those legal acts. By derogation from the first subparagraph, and in appropriate circumstances, Member States may designate anotherrelevant authority to act as a market surveillance authority, provided they ensure coordination with the relevant sectoralmarket surveillance authorities responsible for the enforcement of the Union harmonisation legislation listed in Annex I.4. The procedures referred to in Articles 79 to 83 of this Regulation shall not apply to AI systems related to productscovered by the Union harmonisation legislation listed in section A of Annex I, where such legal acts already provide forprocedures ensuring an equivalent level of protection and having the same objective. In such cases, the relevant sectoralprocedures shall apply instead.5. Without prejudice to the powers of market surveillance authorities under Article 14 of Regulation (EU) 2019/1020, for the purpose of ensuring the effective enforcement of this Regulation, market surveillance authorities may exercise thepowers referred to in Article 14(4), points (d) and (j), of that Regulation remotely, as appropriate.6. For high-risk AI systems placed on the market, put into service, or used by financial institutions regulated by Unionfinancial services law, the market surveillance authority for the purposes of this Regulation shall be the relevant nationalauthority responsible for the financial supervision of those institutions under that legislation in so far as the placing on themarket, putting into service, or the use of the AI system is in direct connection with the provision of those financialservices.7. By way of derogation from paragraph 6, in appropriate circumstances, and provided that coordination is ensured, another relevant authority may be identified by the Member State as market surveillance authority for the purposes of this Regulation. National market surveillance authorities supervising regulated credit institutions regulated under Directive 2013/36/EU, which are participating in the Single Supervisory Mechanism established by Regulation (EU) No 1024/2013, should report, without delay, to the European Central Bank any information identified in the course of their market surveillance activitiesthat may be of potential interest for the prudential supervisory tasks of the European Central Bank specified in that Regulation.8. For high-risk AI systems listed in point 1 of Annex III to this Regulation, in so far as the systems are used for lawenforcement purposes, border management and justice and democracy, and for high-risk AI systems listed in points 6, 7and 8 of Annex III to this Regulation, Member States shall designate as market surveillance authorities for the purposes ofthis Regulation either the competent data protection supervisory authorities under Regulation (EU) 2016/679 or Directive(EU) 2016/680, or any other authority designated pursuant to the same conditions laid down in Articles 41 to 44 of Directive (EU) 2016/680. Market surveillance activities shall in no way affect the independence of judicial authorities, orotherwise interfere with their activities when acting in their judicial capacity.9. Where Union institutions, bodies, offices or agencies fall within the scope of this Regulation, the European Data Protection Supervisor shall act as their market surveillance authority, except in relation to the Court of Justice of the European Union acting in its judicial capacity.10. Member States shall facilitate coordination between market surveillance authorities designated under this Regulation11. Market surveillance authorities and the Commission shall be able to propose joint activities, including jointinvestigations, to be conducted by either market surveillance authorities or market surveillance authorities jointly with the Commission, that have the aim of promoting compliance, identifying non-compliance, raising awareness or providingguidance in relation to this Regulation with respect to specific categories of high-risk AI systems that are found to presenta serious risk across two or more Member States in accordance with Article 9 of Regulation (EU) 2019/1020. The AI Officeshall provide coordination support for joint investigations.12. Without prejudice to the powers provided for under Regulation (EU) 2019/1020, and where relevant and limited towhat is necessary to fulfil their tasks, the market surveillance authorities shall be granted full access by providers to thedocumentation as well as the training, validation and testing data sets used for the development of high-risk AI systems, including, where appropriate and subject to security safeguards, through application programming interfaces (API) or otherrelevant technical means and tools enabling remote access.13. Market surveillance authorities shall be granted access to the source code of the high-risk AI system upon a reasonedrequest and only when both of the following conditions are fulfilled:(a) access to source code is necessary to assess the conformity of a high-risk AI system with the requirements set out in Chapter III, Section 2; and(b) testing or auditing procedures and verifications based on the data and documentation provided by the provider havebeen exhausted or proved insufficient.14. Any information or documentation obtained by market surveillance authorities shall be treated in accordance withthe confidentiality obligations set out in Article 78. Article 75Mutual assistance, market surveillance and control of general-purpose AI systems1. Where an AI system is based on a general-purpose AI model, and the model and the system are developed by thesame provider, the AI Office shall have powers to monitor and supervise compliance of that AI system with obligationsunder this Regulation. To carry out its monitoring and supervision tasks, the AI Office shall have all the powers of a marketsurveillance authority provided for in this Section and Regulation (EU) 2019/1020.2. Where the relevant market surveillance authorities have sufficient reason to consider general-purpose AI systems thatcan be used directly by deployers for at least one purpose that is classified as high-risk pursuant to this Regulation to benon-compliant with the requirements laid down in this Regulation, they shall cooperate with the AI Office to carry outcompliance evaluations, and shall inform the Board and other market surveillance authorities accordingly.3. Where a market surveillance authority is unable to conclude its investigation of the high-risk AI system because of itsinability to access certain information related to the general-purpose AI model despite having made all appropriate effortsto obtain that information, it may submit a reasoned request to the AI Office, by which access to that information shall beenforced. In that case, the AI Office shall supply to the applicant authority without delay, and in any event within 30 days, any information that the AI Office considers to be relevant in order to establish whether a high-risk AI system isnon-compliant. Market surveillance authorities shall safeguard the confidentiality of the information that they obtain inaccordance with Article 78 of this Regulation. The procedure provided for in Chapter VI of Regulation (EU) 2019/1020shall apply mutatis mutandis. Article 76Supervision of testing in real world conditions by market surveillance authorities2. Where testing in real world conditions is conducted for AI systems that are supervised within an AI regulatorysandbox under Article 58, the market surveillance authorities shall verify the compliance with Article 60 as part of theirsupervisory role for the AI regulatory sandbox. Those authorities may, as appropriate, allow the testing in real worldconditions to be conducted by the provider or prospective provider, in derogation from the conditions set out in Article60(4), points (f) and (g).3. Where a market surveillance authority has been informed by the prospective provider, the provider or any third partyof a serious incident or has other grounds for considering that the conditions set out in Articles 60 and 61 are not met, itmay take either of the following decisions on its territory, as appropriate:(a) to suspend or terminate the testing in real world conditions;(b) to require the provider or prospective provider and the deployer or prospective deployer to modify any aspect of thetesting in real world conditions.4. Where a market surveillance authority has taken a decision referred to in paragraph 3 of this Article, or has issued anobjection within the meaning of Article 60(4), point (b), the decision or the objection shall indicate the grounds thereforand how the provider or prospective provider can challenge the decision or objection.5. Where applicable, where a market surveillance authority has taken a decision referred to in paragraph 3, it shallcommunicate the grounds therefor to the market surveillance authorities of other Member States in which the AI systemhas been tested in accordance with the testing plan. Article 77Powers of authorities protecting fundamental rights1. National public authorities or bodies which supervise or enforce the respect of obligations under Union lawprotecting fundamental rights, including the right to non-discrimination, in relation to the use of high-risk AI systemsreferred to in Annex III shall have the power to request and access any documentation created or maintained under this Regulation in accessible language and format when access to that documentation is necessary for effectively fulfilling theirmandates within the limits of their jurisdiction. The relevant public authority or body shall inform the market surveillanceauthority of the Member State concerned of any such request.2. By 2 November 2024, each Member State shall identify the public authorities or bodies referred to in paragraph 1 andmake a list of them publicly available. Member States shall notify the list to the Commission and to the other Member States, and shall keep the list up to date.3. Where the documentation referred to in paragraph 1 is insufficient to ascertain whether an infringement ofobligations under Union law protecting fundamental rights has occurred, the public authority or body referred to inparagraph 1 may make a reasoned request to the market surveillance authority, to organise testing of the high-risk AIsystem through technical means. The market surveillance authority shall organise the testing with the close involvement ofthe requesting public authority or body within a reasonable time following the request.4. Any information or documentation obtained by the national public authorities or bodies referred to in paragraph 1 ofthis Article pursuant to this Article shall be treated in accordance with the confidentiality obligations set out in Article 78. Article 78Confidentiality1. The Commission, market surveillance authorities and notified bodies and any other natural or legal person involved(a) the intellectual property rights and confidential business information or trade secrets of a natural or legal person, including source code, except in the cases referred to in Article 5 of Directive (EU) 2016/943 of the European Parliament and of the Council (57);(b) the effective implementation of this Regulation, in particular for the purposes of inspections, investigations or audits;(c) public and national security interests;(d) the conduct of criminal or administrative proceedings;(e) information classified pursuant to Union or national law.2. The authorities involved in the application of this Regulation pursuant to paragraph 1 shall request only data that isstrictly necessary for the assessment of the risk posed by AI systems and for the exercise of their powers in accordance withthis Regulation and with Regulation (EU) 2019/1020. They shall put in place adequate and effective cybersecurity measuresto protect the security and confidentiality of the information and data obtained, and shall delete the data collected as soonas it is no longer needed for the purpose for which it was obtained, in accordance with applicable Union or national law.3. Without prejudice to paragraphs 1 and 2, information exchanged on a confidential basis between the nationalcompetent authorities or between national competent authorities and the Commission shall not be disclosed without priorconsultation of the originating national competent authority and the deployer when high-risk AI systems referred to inpoint 1, 6 or 7 of Annex III are used by law enforcement, border control, immigration or asylum authorities and when suchdisclosure would jeopardise public and national security interests. This exchange of information shall not cover sensitiveoperational data in relation to the activities of law enforcement, border control, immigration or asylum authorities. When the law enforcement, immigration or asylum authorities are providers of high-risk AI systems referred to in point 1,6 or 7 of Annex III, the technical documentation referred to in Annex IV shall remain within the premises of thoseauthorities. Those authorities shall ensure that the market surveillance authorities referred to in Article 74(8) and (9), asapplicable, can, upon request, immediately access the documentation or obtain a copy thereof. Only staff of the marketsurveillance authority holding the appropriate level of security clearance shall be allowed to access that documentation orany copy thereof.4. Paragraphs 1, 2 and 3 shall not affect the rights or obligations of the Commission, Member States and their relevantauthorities, as well as those of notified bodies, with regard to the exchange of information and the dissemination ofwarnings, including in the context of cross-border cooperation, nor shall they affect the obligations of the parties concernedto provide information under criminal law of the Member States.5. The Commission and Member States may exchange, where necessary and in accordance with relevant provisions ofinternational and trade agreements, confidential information with regulatory authorities of third countries with which theyhave concluded bilateral or multilateral confidentiality arrangements guaranteeing an adequate level of confidentiality. Article 79Procedure at national level for dealing with AI systems presenting a risk1. AI systems presenting a risk shall be understood as a ‘product presenting a risk’ as defined in Article 3, point 19 of Regulation (EU) 2019/1020, in so far as they present risks to the health or safety, or to fundamental rights, of persons.2. Where the market surveillance authority of a Member State has sufficient reason to consider an AI system to presenta risk as referred to in paragraph 1 of this Article, it shall carry out an evaluation of the AI system concerned in respect ofits compliance with all the requirements and obligations laid down in this Regulation. Particular attention shall be given to AI systems presenting a risk to vulnerable groups. Where risks to fundamental rights are identified, the market surveillanceauthority shall also inform and fully cooperate with the relevant national public authorities or bodies referred to in Article77(1). The relevant operators shall cooperate as necessary with the market surveillance authority and with the othernational public authorities or bodies referred to in Article 77(1). Where, in the course of that evaluation, the market surveillance authority or, where applicable the market surveillanceauthority in cooperation with the national public authority referred to in Article 77(1), finds that the AI system does notcomply with the requirements and obligations laid down in this Regulation, it shall without undue delay require the relevantoperator to take all appropriate corrective actions to bring the AI system into compliance, to withdraw the AI system fromthe market, or to recall it within a period the market surveillance authority may prescribe, and in any event within theshorter of 15 working days, or as provided for in the relevant Union harmonisation legislation. The market surveillance authority shall inform the relevant notified body accordingly. Article 18 of Regulation (EU)2019/1020 shall apply to the measures referred to in the second subparagraph of this paragraph.3. Where the market surveillance authority considers that the non-compliance is not restricted to its national territory, itshall inform the Commission and the other Member States without undue delay of the results of the evaluation and of theactions which it has required the operator to take.4. The operator shall ensure that all appropriate corrective action is taken in respect of all the AI systems concerned thatit has made available on the Union market.5. Where the operator of an AI system does not take adequate corrective action within the period referred to inparagraph 2, the market surveillance authority shall take all appropriate provisional measures to prohibit or restrict the AIsystem’s being made available on its national market or put into service, to withdraw the product or the standalone AIsystem from that market or to recall it. That authority shall without undue delay notify the Commission and the other Member States of those measures.6. The notification referred to in paragraph 5 shall include all available details, in particular the information necessaryfor the identification of the non-compliant AI system, the origin of the AI system and the supply chain, the nature of thenon-compliance alleged and the risk involved, the nature and duration of the national measures taken and the argumentsput forward by the relevant operator. In particular, the market surveillance authorities shall indicate whether thenon-compliance is due to one or more of the following:(a) non-compliance with the prohibition of the AI practices referred to in Article 5;(b) a failure of a high-risk AI system to meet requirements set out in Chapter III, Section 2;(c) shortcomings in the harmonised standards or common specifications referred to in Articles 40 and 41 conferringa presumption of conformity;(d) non-compliance with Article 50.7. The market surveillance authorities other than the market surveillance authority of the Member State initiating theprocedure shall, without undue delay, inform the Commission and the other Member States of any measures adopted and ofany additional information at their disposal relating to the non-compliance of the AI system concerned, and, in the event ofdisagreement with the notified national measure, of their objections.8. Where, within three months of receipt of the notification referred to in paragraph 5 of this Article, no objection hasbeen raised by either a market surveillance authority of a Member State or by the Commission in respect of a provisionalmeasure taken by a market surveillance authority of another Member State, that measure shall be deemed justified. Thisshall be without prejudice to the procedural rights of the concerned operator in accordance with Article 18 of Regulation(EU) 2019/1020. The three-month period referred to in this paragraph shall be reduced to 30 days in the event ofnon-compliance with the prohibition of the AI practices referred to in Article 5 of this Regulation.9. The market surveillance authorities shall ensure that appropriate restrictive measures are taken in respect of theproduct or the AI system concerned, such as withdrawal of the product or the AI system from their market, without unduedelay. Article 80Procedure for dealing with AI systems classified by the provider as non-high-risk in application of Annex III1. Where a market surveillance authority has sufficient reason to consider that an AI system classified by the provider asnon-high-risk pursuant to Article 6(3) is indeed high-risk, the market surveillance authority shall carry out an evaluation of2. Where, in the course of that evaluation, the market surveillance authority finds that the AI system concerned ishigh-risk, it shall without undue delay require the relevant provider to take all necessary actions to bring the AI system intocompliance with the requirements and obligations laid down in this Regulation, as well as take appropriate corrective actionwithin a period the market surveillance authority may prescribe.3. Where the market surveillance authority considers that the use of the AI system concerned is not restricted to itsnational territory, it shall inform the Commission and the other Member States without undue delay of the results of theevaluation and of the actions which it has required the provider to take.4. The provider shall ensure that all necessary action is taken to bring the AI system into compliance with therequirements and obligations laid down in this Regulation. Where the provider of an AI system concerned does not bringthe AI system into compliance with those requirements and obligations within the period referred to in paragraph 2 of this Article, the provider shall be subject to fines in accordance with Article 99.5. The provider shall ensure that all appropriate corrective action is taken in respect of all the AI systems concerned thatit has made available on the Union market.6. Where the provider of the AI system concerned does not take adequate corrective action within the period referred toin paragraph 2 of this Article, Article 79(5) to (9) shall apply.7. Where, in the course of the evaluation pursuant to paragraph 1 of this Article, the market surveillance authorityestablishes that the AI system was misclassified by the provider as non-high-risk in order to circumvent the application ofrequirements in Chapter III, Section 2, the provider shall be subject to fines in accordance with Article 99.8. In exercising their power to monitor the application of this Article, and in accordance with Article 11 of Regulation(EU) 2019/1020, market surveillance authorities may perform appropriate checks, taking into account in particularinformation stored in the EU database referred to in Article 71 of this Regulation. Article 81Union safeguard procedure1. Where, within three months of receipt of the notification referred to in Article 79(5), or within 30 days in the case ofnon-compliance with the prohibition of the AI practices referred to in Article 5, objections are raised by the marketsurveillance authority of a Member State to a measure taken by another market surveillance authority, or where the Commission considers the measure to be contrary to Union law, the Commission shall without undue delay enter intoconsultation with the market surveillance authority of the relevant Member State and the operator or operators, and shallevaluate the national measure. On the basis of the results of that evaluation, the Commission shall, within six months, orwithin 60 days in the case of non-compliance with the prohibition of the AI practices referred to in Article 5, starting fromthe notification referred to in Article 79(5), decide whether the national measure is justified and shall notify its decision tothe market surveillance authority of the Member State concerned. The Commission shall also inform all other marketsurveillance authorities of its decision.2. Where the Commission considers the measure taken by the relevant Member State to be justified, all Member Statesshall ensure that they take appropriate restrictive measures in respect of the AI system concerned, such as requiring thewithdrawal of the AI system from their market without undue delay, and shall inform the Commission accordingly. Wherethe Commission considers the national measure to be unjustified, the Member State concerned shall withdraw the measureand shall inform the Commission accordingly.3. Where the national measure is considered justified and the non-compliance of the AI system is attributed toshortcomings in the harmonised standards or common specifications referred to in Articles 40 and 41 of this Regulation, the Commission shall apply the procedure provided for in Article 11 of Regulation (EU) No 1025/2012. Article 82Compliant AI systems which present a risk1. Where, having performed an evaluation under Article 79, after consulting the relevant national public authorityreferred to in Article 77(1), the market surveillance authority of a Member State finds that although a high-risk AI systemcomplies with this Regulation, it nevertheless presents a risk to the health or safety of persons, to fundamental rights, or toother aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure2. The provider or other relevant operator shall ensure that corrective action is taken in respect of all the AI systemsconcerned that it has made available on the Union market within the timeline prescribed by the market surveillanceauthority of the Member State referred to in paragraph 1.3. The Member States shall immediately inform the Commission and the other Member States of a finding underparagraph 1. That information shall include all available details, in particular the data necessary for the identification of the AI system concerned, the origin and the supply chain of the AI system, the nature of the risk involved and the nature andduration of the national measures taken.4. The Commission shall without undue delay enter into consultation with the Member States concerned and therelevant operators, and shall evaluate the national measures taken. On the basis of the results of that evaluation, the Commission shall decide whether the measure is justified and, where necessary, propose other appropriate measures.5. The Commission shall immediately communicate its decision to the Member States concerned and to the relevantoperators. It shall also inform the other Member States. Article 83Formal non-compliance1. Where the market surveillance authority of a Member State makes one of the following findings, it shall require therelevant provider to put an end to the non-compliance concerned, within a period it may prescribe:(a) the CE marking has been affixed in violation of Article 48;(b) the CE marking has not been affixed;(c) the EU declaration of conformity referred to in Article 47 has not been drawn up;(d) the EU declaration of conformity referred to in Article 47 has not been drawn up correctly;(e) the registration in the EU database referred to in Article 71 has not been carried out;(f) where applicable, no authorised representative has been appointed;(g) technical documentation is not available.2. Where the non-compliance referred to in paragraph 1 persists, the market surveillance authority of the Member Stateconcerned shall take appropriate and proportionate measures to restrict or prohibit the high-risk AI system being madeavailable on the market or to ensure that it is recalled or withdrawn from the market without delay. Article 84Union AI testing support structures1. The Commission shall designate one or more Union AI testing support structures to perform the tasks listed under Article 21(6) of Regulation (EU) 2019/1020 in the area of AI. SECTION 4Remedies Article 85Right to lodge a complaint with a market surveillance authority Without prejudice to other administrative or judicial remedies, any natural or legal person having grounds to consider thatthere has been an infringement of the provisions of this Regulation may submit complaints to the relevant marketsurveillance authority. In accordance with Regulation (EU) 2019/1020, such complaints shall be taken into account for the purpose of conductingmarket surveillance activities, and shall be handled in line with the dedicated procedures established therefor by the marketsurveillance authorities. Article 86Right to explanation of individual decision-making1. Any affected person subject to a decision which is taken by the deployer on the basis of the output from a high-risk AIsystem listed in Annex III, with the exception of systems listed under point 2 thereof, and which produces legal effects orsimilarly significantly affects that person in a way that they consider to have an adverse impact on their health, safety orfundamental rights shall have the right to obtain from the deployer clear and meaningful explanations of the role of the AIsystem in the decision-making procedure and the main elements of the decision taken.2. Paragraph 1 shall not apply to the use of AI systems for which exceptions from, or restrictions to, the obligationunder that paragraph follow from Union or national law in compliance with Union law.3. This Article shall apply only to the extent that the right referred to in paragraph 1 is not otherwise provided for under Union law. Article 87Reporting of infringements and protection of reporting persons Directive (EU) 2019/1937 shall apply to the reporting of infringements of this Regulation and the protection of personsreporting such infringements. SECTION 5Supervision, investigation, enforcement and monitoring in respect of providers of general-purpose AI models Article 88Enforcement of the obligations of providers of general-purpose AI models1. The Commission shall have exclusive powers to supervise and enforce Chapter V, taking into account the proceduralguarantees under Article 94. The Commission shall entrust the implementation of these tasks to the AI Office, withoutprejudice to the powers of organisation of the Commission and the division of competences between Member States andthe Union based on the Treaties.2. Without prejudice to Article 75(3), market surveillance authorities may request the Commission to exercise the Article 89Monitoring actions1. For the purpose of carrying out the tasks assigned to it under this Section, the AI Office may take the necessaryactions to monitor the effective implementation and compliance with this Regulation by providers of general-purpose AImodels, including their adherence to approved codes of practice.2. Downstream providers shall have the right to lodge a complaint alleging an infringement of this Regulation. A complaint shall be duly reasoned and indicate at least:(a) the point of contact of the provider of the general-purpose AI model concerned;(b) a description of the relevant facts, the provisions of this Regulation concerned, and the reason why the downstreamprovider considers that the provider of the general-purpose AI model concerned infringed this Regulation;(c) any other information that the downstream provider that sent the request considers relevant, including, whereappropriate, information gathered on its own initiative. Article 90Alerts of systemic risks by the scientific panel1. The scientific panel may provide a qualified alert to the AI Office where it has reason to suspect that:(a) a general-purpose AI model poses concrete identifiable risk at Union level; or(b) a general-purpose AI model meets the conditions referred to in Article 51.2. Upon such qualified alert, the Commission, through the AI Office and after having informed the Board, may exercisethe powers laid down in this Section for the purpose of assessing the matter. The AI Office shall inform the Board of anymeasure according to Articles 91 to 94.3. A qualified alert shall be duly reasoned and indicate at least:(a) the point of contact of the provider of the general-purpose AI model with systemic risk concerned;(b) a description of the relevant facts and the reasons for the alert by the scientific panel;(c) any other information that the scientific panel considers to be relevant, including, where appropriate, informationgathered on its own initiative. Article 91Power to request documentation and information1. The Commission may request the provider of the general-purpose AI model concerned to provide the documentationdrawn up by the provider in accordance with Articles 53 and 55, or any additional information that is necessary for thepurpose of assessing compliance of the provider with this Regulation.2. Before sending the request for information, the AI Office may initiate a structured dialogue with the provider of thegeneral-purpose AI model.3. Upon a duly substantiated request from the scientific panel, the Commission may issue a request for information to4. The request for information shall state the legal basis and the purpose of the request, specify what information isrequired, set a period within which the information is to be provided, and indicate the fines provided for in Article 101 forsupplying incorrect, incomplete or misleading information.5. The provider of the general-purpose AI model concerned, or its representative shall supply the information requested. In the case of legal persons, companies or firms, or where the provider has no legal personality, the persons authorised torepresent them by law or by their statutes, shall supply the information requested on behalf of the provider of thegeneral-purpose AI model concerned. Lawyers duly authorised to act may supply information on behalf of their clients. Theclients shall nevertheless remain fully responsible if the information supplied is incomplete, incorrect or misleading. Article 92Power to conduct evaluations1. The AI Office, after consulting the Board, may conduct evaluations of the general-purpose AI model concerned:(a) to assess compliance of the provider with obligations under this Regulation, where the information gathered pursuantto Article 91 is insufficient; or(b) to investigate systemic risks at Union level of general-purpose AI models with systemic risk, in particular followinga qualified alert from the scientific panel in accordance with Article 90(1), point (a).2. The Commission may decide to appoint independent experts to carry out evaluations on its behalf, including from thescientific panel established pursuant to Article 68. Independent experts appointed for this task shall meet the criteriaoutlined in Article 68(2).3. For the purposes of paragraph 1, the Commission may request access to the general-purpose AI model concernedthrough APIs or further appropriate technical means and tools, including source code.4. The request for access shall state the legal basis, the purpose and reasons of the request and set the period withinwhich the access is to be provided, and the fines provided for in Article 101 for failure to provide access.5. The providers of the general-purpose AI model concerned or its representative shall supply the information requested. In the case of legal persons, companies or firms, or where the provider has no legal personality, the persons authorised torepresent them by law or by their statutes, shall provide the access requested on behalf of the provider of thegeneral-purpose AI model concerned.6. The Commission shall adopt implementing acts setting out the detailed arrangements and the conditions for theevaluations, including the detailed arrangements for involving independent experts, and the procedure for the selectionthereof. Those implementing acts shall be adopted in accordance with the examination procedure referred to in Article98(2).7. Prior to requesting access to the general-purpose AI model concerned, the AI Office may initiate a structured dialoguewith the provider of the general-purpose AI model to gather more information on the internal testing of the model, internalsafeguards for preventing systemic risks, and other internal procedures and measures the provider has taken to mitigatesuch risks. Article 93Power to request measures1. Where necessary and appropriate, the Commission may request providers to:(b) implement mitigation measures, where the evaluation carried out in accordance with Article 92 has given rise to seriousand substantiated concern of a systemic risk at Union level;(c) restrict the making available on the market, withdraw or recall the model.2. Before a measure is requested, the AI Office may initiate a structured dialogue with the provider of thegeneral-purpose AI model.3. If, during the structured dialogue referred to in paragraph 2, the provider of the general-purpose AI model withsystemic risk offers commitments to implement mitigation measures to address a systemic risk at Union level, the Commission may, by decision, make those commitments binding and declare that there are no further grounds for action. Article 94Procedural rights of economic operators of the general-purpose AI model Article 18 of Regulation (EU) 2019/1020 shall apply mutatis mutandis to the providers of the general-purpose AI model, without prejudice to more specific procedural rights provided for in this Regulation. CHAPTER XCODES OF CONDUCT AND GUIDELINESArticle 95Codes of conduct for voluntary application of specific requirements1. The AI Office and the Member States shall encourage and facilitate the drawing up of codes of conduct, includingrelated governance mechanisms, intended to foster the voluntary application to AI systems, other than high-risk AI systems, of some or all of the requirements set out in Chapter III, Section 2 taking into account the available technical solutions andindustry best practices allowing for the application of such requirements.2. The AI Office and the Member States shall facilitate the drawing up of codes of conduct concerning the voluntaryapplication, including by deployers, of specific requirements to all AI systems, on the basis of clear objectives and keyperformance indicators to measure the achievement of those objectives, including elements such as, but not limited to:(a) applicable elements provided for in Union ethical guidelines for trustworthy AI;(b) assessing and minimising the impact of AI systems on environmental sustainability, including as regards energy-efficientprogramming and techniques for the efficient design, training and use of AI;(c) promoting AI literacy, in particular that of persons dealing with the development, operation and use of AI;(d) facilitating an inclusive and diverse design of AI systems, including through the establishment of inclusive and diversedevelopment teams and the promotion of stakeholders’ participation in that process;(e) assessing and preventing the negative impact of AI systems on vulnerable persons or groups of vulnerable persons, including as regards accessibility for persons with a disability, as well as on gender equality.3. Codes of conduct may be drawn up by individual providers or deployers of AI systems or by organisationsrepresenting them or by both, including with the involvement of any interested stakeholders and their representativeorganisations, including civil society organisations and academia. Codes of conduct may cover one or more AI systemstaking into account the similarity of the intended purpose of the relevant systems. Article 96Guidelines from the Commission on the implementation of this Regulation1. The Commission shall develop guidelines on the practical implementation of this Regulation, and in particular on:(a) the application of the requirements and obligations referred to in Articles 8 to 15 and in Article 25;(b) the prohibited practices referred to in Article 5;(c) the practical implementation of the provisions related to substantial modification;(d) the practical implementation of transparency obligations laid down in Article 50;(e) detailed information on the relationship of this Regulation with the Union harmonisation legislation listed in Annex I, as well as with other relevant Union law, including as regards consistency in their enforcement;(f) the application of the definition of an AI system as set out in Article 3, point (1). When issuing such guidelines, the Commission shall pay particular attention to the needs of SMEs including start-ups, oflocal public authorities and of the sectors most likely to be affected by this Regulation. The guidelines referred to in the first subparagraph of this paragraph shall take due account of the generally acknowledgedstate of the art on AI, as well as of relevant harmonised standards and common specifications that are referred to in Articles 40 and 41, or of those harmonised standards or technical specifications that are set out pursuant to Unionharmonisation law.2. At the request of the Member States or the AI Office, or on its own initiative, the Commission shall update guidelinespreviously adopted when deemed necessary. CHAPTER XIDELEGATION OF POWER AND COMMITTEE PROCEDUREArticle 97Exercise of the delegation1. The power to adopt delegated acts is conferred on the Commission subject to the conditions laid down in this Article.2. The power to adopt delegated acts referred to in Article 6(6) and (7), Article 7(1) and (3), Article 11(3), Article 43(5)and (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) shall be conferred on the Commission fora period of five years from 1 August 2024. The Commission shall draw up a report in respect of the delegation of powernot later than nine months before the end of the five-year period. The delegation of power shall be tacitly extended forperiods of an identical duration, unless the European Parliament or the Council opposes such extension not later than threemonths before the end of each period.3. The delegation of power referred to in Article 6(6) and (7), Article 7(1) and (3), Article 11(3), Article 43(5) and (6), Article 47(5), Article 51(3), Article 52(4) and Article 53(5) and (6) may be revoked at any time by the European Parliamentor by the Council. A decision of revocation shall put an end to the delegation of power specified in that decision. It shalltake effect the day following that of its publication in the Official Journal of the European Union or at a later date specifiedtherein. It shall not affect the validity of any delegated acts already in force.5. As soon as it adopts a delegated act, the Commission shall notify it simultaneously to the European Parliament and tothe Council.6. Any delegated act adopted pursuant to Article 6(6) or (7), Article 7(1) or (3), Article 11(3), Article 43(5) or (6), Article 47(5), Article 51(3), Article 52(4) or Article 53(5) or (6) shall enter into force only if no objection has beenexpressed by either the European Parliament or the Council within a period of three months of notification of that act tothe European Parliament and the Council or if, before the expiry of that period, the European Parliament and the Councilhave both informed the Commission that they will not object. That period shall be extended by three months at theinitiative of the European Parliament or of the Council. Article 98Committee procedure1. The Commission shall be assisted by a committee. That committee shall be a committee within the meaning of Regulation (EU) No 182/2011.2. Where reference is made to this paragraph, Article 5 of Regulation (EU) No 182/2011 shall apply. CHAPTER XIIPENALTIESArticle 99Penalties1. In accordance with the terms and conditions laid down in this Regulation, Member States shall lay down the rules onpenalties and other enforcement measures, which may also include warnings and non-monetary measures, applicable toinfringements of this Regulation by operators, and shall take all measures necessary to ensure that they are properly andeffectively implemented, thereby taking into account the guidelines issued by the Commission pursuant to Article 96. Thepenalties provided for shall be effective, proportionate and dissuasive. They shall take into account the interests of SMEs, including start-ups, and their economic viability.2. The Member States shall, without delay and at the latest by the date of entry into application, notify the Commissionof the rules on penalties and of other enforcement measures referred to in paragraph 1, and shall notify it, without delay, ofany subsequent amendment to them.3. Non-compliance with the prohibition of the AI practices referred to in Article 5 shall be subject to administrativefines of up to EUR 35 000 000 or, if the offender is an undertaking, up to 7 % of its total worldwide annual turnover for thepreceding financial year, whichever is higher.4. Non-compliance with any of the following provisions related to operators or notified bodies, other than those laiddown in Articles 5, shall be subject to administrative fines of up to EUR 15 000 000 or, if the offender is an undertaking, upto 3 % of its total worldwide annual turnover for the preceding financial year, whichever is higher:(a) obligations of providers pursuant to Article 16;(b) obligations of authorised representatives pursuant to Article 22;(c) obligations of importers pursuant to Article 23;(d) obligations of distributors pursuant to Article 24;(e) obligations of deployers pursuant to Article 26;(f) requirements and obligations of notified bodies pursuant to Article 31, Article 33(1), (3) and (4) or Article 34;5. The supply of incorrect, incomplete or misleading information to notified bodies or national competent authorities inreply to a request shall be subject to administrative fines of up to EUR 7 500 000 or, if the offender is an undertaking, up to1 % of its total worldwide annual turnover for the preceding financial year, whichever is higher.6. In the case of SMEs, including start-ups, each fine referred to in this Article shall be up to the percentages or amountreferred to in paragraphs 3, 4 and 5, whichever thereof is lower.7. When deciding whether to impose an administrative fine and when deciding on the amount of the administrative finein each individual case, all relevant circumstances of the specific situation shall be taken into account and, as appropriate, regard shall be given to the following:(a) the nature, gravity and duration of the infringement and of its consequences, taking into account the purpose of the AIsystem, as well as, where appropriate, the number of affected persons and the level of damage suffered by them;(b) whether administrative fines have already been applied by other market surveillance authorities to the same operator forthe same infringement;(c) whether administrative fines have already been applied by other authorities to the same operator for infringements ofother Union or national law, when such infringements result from the same activity or omission constituting a relevantinfringement of this Regulation;(d) the size, the annual turnover and market share of the operator committing the infringement;(e) any other aggravating or mitigating factor applicable to the circumstances of the case, such as financial benefits gained, or losses avoided, directly or indirectly, from the infringement;(f) the degree of cooperation with the national competent authorities, in order to remedy the infringement and mitigate thepossible adverse effects of the infringement;(g) the degree of responsibility of the operator taking into account the technical and organisational measures implementedby it;(h) the manner in which the infringement became known to the national competent authorities, in particular whether, andif so to what extent, the operator notified the infringement;(i) the intentional or negligent character of the infringement;(j) any action taken by the operator to mitigate the harm suffered by the affected persons.8. Each Member State shall lay down rules on to what extent administrative fines may be imposed on public authoritiesand bodies established in that Member State.9. Depending on the legal system of the Member States, the rules on administrative fines may be applied in sucha manner that the fines are imposed by competent national courts or by other bodies, as applicable in those Member States. The application of such rules in those Member States shall have an equivalent effect.10. The exercise of powers under this Article shall be subject to appropriate procedural safeguards in accordance with Union and national law, including effective judicial remedies and due process.11. Member States shall, on an annual basis, report to the Commission about the administrative fines they have issuedduring that year, in accordance with this Article, and about any related litigation or judicial proceedings. Article 100Administrative fines on Union institutions, bodies, offices and agencies1. The European Data Protection Supervisor may impose administrative fines on Union institutions, bodies, offices andagencies falling within the scope of this Regulation. When deciding whether to impose an administrative fine and when(a) the nature, gravity and duration of the infringement and of its consequences, taking into account the purpose of the AIsystem concerned, as well as, where appropriate, the number of affected persons and the level of damage suffered bythem;(b) the degree of responsibility of the Union institution, body, office or agency, taking into account technical andorganisational measures implemented by them;(c) any action taken by the Union institution, body, office or agency to mitigate the damage suffered by affected persons;(d) the degree of cooperation with the European Data Protection Supervisor in order to remedy the infringement andmitigate the possible adverse effects of the infringement, including compliance with any of the measures previouslyordered by the European Data Protection Supervisor against the Union institution, body, office or agency concernedwith regard to the same subject matter;(e) any similar previous infringements by the Union institution, body, office or agency;(f) the manner in which the infringement became known to the European Data Protection Supervisor, in particularwhether, and if so to what extent, the Union institution, body, office or agency notified the infringement;(g) the annual budget of the Union institution, body, office or agency.2. Non-compliance with the prohibition of the AI practices referred to in Article 5 shall be subject to administrativefines of up to EUR 1 500 000.3. The non-compliance of the AI system with any requirements or obligations under this Regulation, other than thoselaid down in Article 5, shall be subject to administrative fines of up to EUR 750 000.4. Before taking decisions pursuant to this Article, the European Data Protection Supervisor shall give the Unioninstitution, body, office or agency which is the subject of the proceedings conducted by the European Data Protection Supervisor the opportunity of being heard on the matter regarding the possible infringement. The European Data Protection Supervisor shall base his or her decisions only on elements and circumstances on which the parties concernedhave been able to comment. Complainants, if any, shall be associated closely with the proceedings.5. The rights of defence of the parties concerned shall be fully respected in the proceedings. They shall be entitled tohave access to the European Data Protection Supervisor’s file, subject to the legitimate interest of individuals orundertakings in the protection of their personal data or business secrets.6. Funds collected by imposition of fines in this Article shall contribute to the general budget of the Union. The finesshall not affect the effective operation of the Union institution, body, office or agency fined.7. The European Data Protection Supervisor shall, on an annual basis, notify the Commission of the administrative finesit has imposed pursuant to this Article and of any litigation or judicial proceedings it has initiated. Article 101Fines for providers of general-purpose AI models1. The Commission may impose on providers of general-purpose AI models fines not exceeding 3 % of their annual totalworldwide turnover in the preceding financial year or EUR 15 000 000, whichever is higher., when the Commission findsthat the provider intentionally or negligently:(a) infringed the relevant provisions of this Regulation;(b) failed to comply with a request for a document or for information pursuant to Article 91, or supplied incorrect, incomplete or misleading information;(d) failed to make available to the Commission access to the general-purpose AI model or general-purpose AI model withsystemic risk with a view to conducting an evaluation pursuant to Article 92. In fixing the amount of the fine or periodic penalty payment, regard shall be had to the nature, gravity and duration of theinfringement, taking due account of the principles of proportionality and appropriateness. The Commission shall also intoaccount commitments made in accordance with Article 93(3) or made in relevant codes of practice in accordance with Article 56.2. Before adopting the decision pursuant to paragraph 1, the Commission shall communicate its preliminary findings tothe provider of the general-purpose AI model and give it an opportunity to be heard.3. Fines imposed in accordance with this Article shall be effective, proportionate and dissuasive.4. Information on fines imposed under this Article shall also be communicated to the Board as appropriate.5. The Court of Justice of the European Union shall have unlimited jurisdiction to review decisions of the Commissionfixing a fine under this Article. It may cancel, reduce or increase the fine imposed.6. The Commission shall adopt implementing acts containing detailed arrangements and procedural safeguards forproceedings in view of the possible adoption of decisions pursuant to paragraph 1 of this Article. Those implementing actsshall be adopted in accordance with the examination procedure referred to in Article 98(2). CHAPTER XIIIFINAL PROVISIONSArticle 102Amendment to Regulation (EC) No 300/2008In Article 4(3) of Regulation (EC) No 300/2008, the following subparagraph is added:‘When adopting detailed measures related to technical specifications and procedures for approval and use of securityequipment concerning Artificial Intelligence systems within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), the requirements set out in Chapter III, Section 2, of that Regulation shall be taken intoaccount.(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonisedrules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: europa. eu/eli/reg/2024/1689/oj).’. Article 103Amendment to Regulation (EU) No 167/2013In Article 17(5) of Regulation (EU) No 167/2013, the following subparagraph is added:‘When adopting delegated acts pursuant to the first subparagraph concerning artificial intelligence systems which are safetycomponents within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), therequirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonisedrules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: europa. eu/eli/reg/Article 104Amendment to Regulation (EU) No 168/2013In Article 22(5) of Regulation (EU) No 168/2013, the following subparagraph is added:‘When adopting delegated acts pursuant to the first subparagraph concerning Artificial Intelligence systems which are safetycomponents within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), therequirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonisedrules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: europa. eu/eli/reg/2024/1689/oj).’. Article 105Amendment to Directive 2014/90/EUIn Article 8 of Directive 2014/90/EU, the following paragraph is added:‘5. For Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 ofthe European Parliament and of the Council (*), when carrying out its activities pursuant to paragraph 1 and when adoptingtechnical specifications and testing standards in accordance with paragraphs 2 and 3, the Commission shall take intoaccount the requirements set out in Chapter III, Section 2, of that Regulation.(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonisedrules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: europa. eu/eli/reg/2024/1689/oj).’. Article 106Amendment to Directive (EU) 2016/797In Article 5 of Directive (EU) 2016/797, the following paragraph is added:‘12. When adopting delegated acts pursuant to paragraph 1 and implementing acts pursuant to paragraph 11concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689of the European Parliament and of the Council (*), the requirements set out in Chapter III, Section 2, of that Regulation shallbe taken into account.(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonisedrules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: europa. eu/eli/reg/Article 107Amendment to Regulation (EU) 2018/858In Article 5 of Regulation (EU) 2018/858 the following paragraph is added:‘4. When adopting delegated acts pursuant to paragraph 3 concerning Artificial Intelligence systems which are safetycomponents within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), therequirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonisedrules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: europa. eu/eli/reg/2024/1689/oj).’. Article 108Amendments to Regulation (EU) 2018/1139Regulation (EU) 2018/1139 is amended as follows:(1) in Article 17, the following paragraph is added:‘3. Without prejudice to paragraph 2, when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), the requirements set out in Chapter III, Section 2, of that Regulation shall betaken into account.(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying downharmonised rules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU)No 168/2013, (EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797and (EU) 2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: europa. eu/eli/reg/2024/1689/oj).’;(2) in Article 19, the following paragraph is added:‘4. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems whichare safety components within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.’;(3) in Article 43, the following paragraph is added:‘4. When adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which aresafety components within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.’;(4) in Article 47, the following paragraph is added:‘3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems whichare safety components within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.’;(5) in Article 57, the following subparagraph is added:‘When adopting those implementing acts concerning Artificial Intelligence systems which are safety components within(6) in Article 58, the following paragraph is added:‘3. When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems whichare safety components within the meaning of Regulation (EU) 2024/1689, the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.’. Article 109Amendment to Regulation (EU) 2019/2144In Article 11 of Regulation (EU) 2019/2144, the following paragraph is added:‘3. When adopting the implementing acts pursuant to paragraph 2, concerning artificial intelligence systems which aresafety components within the meaning of Regulation (EU) 2024/1689 of the European Parliament and of the Council (*), the requirements set out in Chapter III, Section 2, of that Regulation shall be taken into account.(*) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonisedrules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: europa. eu/eli/reg/2024/1689/oj).’. Article 110Amendment to Directive (EU) 2020/1828In Annex I to Directive (EU) 2020/1828 of the European Parliament and of the Council (58), the following point is added:‘(68) Regulation (EU) 2024/1689 of the European Parliament and of the Council of 13 June 2024 laying down harmonisedrules on artificial intelligence and amending Regulations (EC) No 300/2008, (EU) No 167/2013, (EU) No 168/2013,(EU) 2018/858, (EU) 2018/1139 and (EU) 2019/2144 and Directives 2014/90/EU, (EU) 2016/797 and (EU)2020/1828 (Artificial Intelligence Act) (OJ L, 2024/1689, 12.7.2024, ELI: europa. eu/eli/reg/2024/1689/oj).’. Article 111AI systems already placed on the market or put into service and general-purpose AI models already placed on themarked1. Without prejudice to the application of Article 5 as referred to in Article 113(3), point (a), AI systems which arecomponents of the large-scale IT systems established by the legal acts listed in Annex X that have been placed on the marketor put into service before 2 August 2027 shall be brought into compliance with this Regulation by 31 December 2030. The requirements laid down in this Regulation shall be taken into account in the evaluation of each large-scale IT systemestablished by the legal acts listed in Annex X to be undertaken as provided for in those legal acts and where those legal actsare replaced or amended.2. Without prejudice to the application of Article 5 as referred to in Article 113(3), point (a), this Regulation shall applyto operators of high-risk AI systems, other than the systems referred to in paragraph 1 of this Article, that have been placedon the market or put into service before 2 August 2026, only if, as from that date, those systems are subject to significantchanges in their designs. In any case, the providers and deployers of high-risk AI systems intended to be used by publicauthorities shall take the necessary steps to comply with the requirements and obligations of this Regulation by 2 August2030.3. Providers of general-purpose AI models that have been placed on the market before 2 August 2025 shall take thenecessary steps in order to comply with the obligations laid down in this Regulation by 2 August 2027. Article 112Evaluation and review1. The Commission shall assess the need for amendment of the list set out in Annex III and of the list of prohibited AIpractices laid down in Article 5, once a year following the entry into force of this Regulation, and until the end of the periodof the delegation of power laid down in Article 97. The Commission shall submit the findings of that assessment to the European Parliament and the Council.2. By 2 August 2028 and every four years thereafter, the Commission shall evaluate and report to the European Parliament and to the Council on the following:(a) the need for amendments extending existing area headings or adding new area headings in Annex III;(b) amendments to the list of AI systems requiring additional transparency measures in Article 50;(c) amendments enhancing the effectiveness of the supervision and governance system.3. By 2 August 2029 and every four years thereafter, the Commission shall submit a report on the evaluation and reviewof this Regulation to the European Parliament and to the Council. The report shall include an assessment with regard to thestructure of enforcement and the possible need for a Union agency to resolve any identified shortcomings. On the basis ofthe findings, that report shall, where appropriate, be accompanied by a proposal for amendment of this Regulation. Thereports shall be made public.4. The reports referred to in paragraph 2 shall pay specific attention to the following:(a) the status of the financial, technical and human resources of the national competent authorities in order to effectivelyperform the tasks assigned to them under this Regulation;(b) the state of penalties, in particular administrative fines as referred to in Article 99(1), applied by Member States forinfringements of this Regulation;(c) adopted harmonised standards and common specifications developed to support this Regulation;(d) the number of undertakings that enter the market after the entry into application of this Regulation, and how many ofthem are SMEs.5. By 2 August 2028, the Commission shall evaluate the functioning of the AI Office, whether the AI Office has beengiven sufficient powers and competences to fulfil its tasks, and whether it would be relevant and needed for the properimplementation and enforcement of this Regulation to upgrade the AI Office and its enforcement competences and toincrease its resources. The Commission shall submit a report on its evaluation to the European Parliament and to the Council.6. By 2 August 2028 and every four years thereafter, the Commission shall submit a report on the review of the progresson the development of standardisation deliverables on the energy-efficient development of general-purpose AI models, andasses the need for further measures or actions, including binding measures or actions. The report shall be submitted to the European Parliament and to the Council, and it shall be made public.7. By 2 August 2028 and every three years thereafter, the Commission shall evaluate the impact and effectiveness ofvoluntary codes of conduct to foster the application of the requirements set out in Chapter III, Section 2 for AI systemsother than high-risk AI systems and possibly other additional requirements for AI systems other than high-risk AI systems, including as regards environmental sustainability.8. For the purposes of paragraphs 1 to 7, the Board, the Member States and national competent authorities shall providethe Commission with information upon its request and without undue delay.10. The Commission shall, if necessary, submit appropriate proposals to amend this Regulation, in particular taking intoaccount developments in technology, the effect of AI systems on health and safety, and on fundamental rights, and in lightof the state of progress in the information society.11. To guide the evaluations and reviews referred to in paragraphs 1 to 7 of this Article, the AI Office shall undertake todevelop an objective and participative methodology for the evaluation of risk levels based on the criteria outlined in therelevant Articles and the inclusion of new systems in:(a) the list set out in Annex III, including the extension of existing area headings or the addition of new area headings inthat Annex;(b) the list of prohibited practices set out in Article 5; and(c) the list of AI systems requiring additional transparency measures pursuant to Article 50.12. Any amendment to this Regulation pursuant to paragraph 10, or relevant delegated or implementing acts, whichconcerns sectoral Union harmonisation legislation listed in Section B of Annex I shall take into account the regulatoryspecificities of each sector, and the existing governance, conformity assessment and enforcement mechanisms andauthorities established therein.13. By 2 August 2031, the Commission shall carry out an assessment of the enforcement of this Regulation and shallreport on it to the European Parliament, the Council and the European Economic and Social Committee, taking intoaccount the first years of application of this Regulation. On the basis of the findings, that report shall, where appropriate, beaccompanied by a proposal for amendment of this Regulation with regard to the structure of enforcement and the need fora Union agency to resolve any identified shortcomings. Article 113Entry into force and application This Regulation shall enter into force on the twentieth day following that of its publication in the Official Journal of the European Union. It shall apply from 2 August 2026. However:(a) Chapters I and II shall apply from 2 February 2025;(b) Chapter III Section 4, Chapter V, Chapter VII and Chapter XII and Article 78 shall apply from 2 August 2025, with theexception of Article 101;(c) Article 6(1) and the corresponding obligations in this Regulation shall apply from 2 August 2027. This Regulation shall be binding in its entirety and directly applicable in all Member States. Done at Brussels, 13 June 2024. For the European Parliament For the Council The President The President R. METSOLA M. MICHELANNEX IList of Union harmonisation legislation Section A. List of Union harmonisation legislation based on the New Legislative Framework1. Directive 2006/42/EC of the European Parliament and of the Council of 17 May 2006 on machinery, and amending Directive 95/16/EC (OJ L 157, 9.6.2006, p. 24);2. Directive 2009/48/EC of the European Parliament and of the Council of 18 June 2009 on the safety of toys (OJL 170, 30.6.2009, p. 1);3. Directive 2013/53/EU of the European Parliament and of the Council of 20 November 2013 on recreational craftand personal watercraft and repealing Directive 94/25/EC (OJ L 354, 28.12.2013, p. 90);4. Directive 2014/33/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation ofthe laws of the Member States relating to lifts and safety components for lifts (OJ L 96, 29.3.2014, p. 251);5. Directive 2014/34/EU of the European Parliament and of the Council of 26 February 2014 on the harmonisation ofthe laws of the Member States relating to equipment and protective systems intended for use in potentially explosiveatmospheres (OJ L 96, 29.3.2014, p. 309);6. Directive 2014/53/EU of the European Parliament and of the Council of 16 April 2014 on the harmonisation of thelaws of the Member States relating to the making available on the market of radio equipment and repealing Directive1999/5/EC (OJ L 153, 22.5.2014, p. 62);7. Directive 2014/68/EU of the European Parliament and of the Council of 15 May 2014 on the harmonisation of thelaws of the Member States relating to the making available on the market of pressure equipment (OJ L 189,27.6.2014, p. 164);8. Regulation (EU) 2016/424 of the European Parliament and of the Council of 9 March 2016 on cablewayinstallations and repealing Directive 2000/9/EC (OJ L 81, 31.3.2016, p. 1);9. Regulation (EU) 2016/425 of the European Parliament and of the Council of 9 March 2016 on personal protectiveequipment and repealing Council Directive 89/686/EEC (OJ L 81, 31.3.2016, p. 51);10. Regulation (EU) 2016/426 of the European Parliament and of the Council of 9 March 2016 on appliances burninggaseous fuels and repealing Directive 2009/142/EC (OJ L 81, 31.3.2016, p. 99);11. Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices, amending Directive 2001/83/EC, Regulation (EC) No 178/2002 and Regulation (EC) No 1223/2009 and repealing Council Directives 90/385/EEC and 93/42/EEC (OJ L 117, 5.5.2017, p. 1);12. Regulation (EU) 2017/746 of the European Parliament and of the Council of 5 April 2017 on in vitro diagnosticmedical devices and repealing Directive 98/79/EC and Commission Decision 2010/227/EU (OJ L 117, 5.5.2017, p. 176). Section B. List of other Union harmonisation legislation13. Regulation (EC) No 300/2008 of the European Parliament and of the Council of 11 March 2008 on common rulesin the field of civil aviation security and repealing Regulation (EC) No 2320/2002 (OJ L 97, 9.4.2008, p. 72);14. Regulation (EU) No 168/2013 of the European Parliament and of the Council of 15 January 2013 on the approvaland market surveillance of two- or three-wheel vehicles and quadricycles (OJ L 60, 2.3.2013, p. 52);16. Directive 2014/90/EU of the European Parliament and of the Council of 23 July 2014 on marine equipment andrepealing Council Directive 96/98/EC (OJ L 257, 28.8.2014, p. 146);17. Directive (EU) 2016/797 of the European Parliament and of the Council of 11 May 2016 on the interoperability ofthe rail system within the European Union (OJ L 138, 26.5.2016, p. 44);18. Regulation (EU) 2018/858 of the European Parliament and of the Council of 30 May 2018 on the approval andmarket surveillance of motor vehicles and their trailers, and of systems, components and separate technical unitsintended for such vehicles, amending Regulations (EC) No 715/2007 and (EC) No 595/2009 and repealing Directive2007/46/EC (OJ L 151, 14.6.2018, p. 1);19. Regulation (EU) 2019/2144 of the European Parliament and of the Council of 27 November 2019 on type-approvalrequirements for motor vehicles and their trailers, and systems, components and separate technical units intendedfor such vehicles, as regards their general safety and the protection of vehicle occupants and vulnerable road users, amending Regulation (EU) 2018/858 of the European Parliament and of the Council and repealing Regulations (EC)No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 of the European Parliament and of the Council and Commission Regulations (EC) No 631/2009, (EU) No 406/2010, (EU) No 672/2010, (EU) No 1003/2010,(EU) No 1005/2010, (EU) No 1008/2010, (EU) No 1009/2010, (EU) No 19/2011, (EU) No 109/2011, (EU)No 458/2011, (EU) No 65/2012, (EU) No 130/2012, (EU) No 347/2012, (EU) No 351/2012, (EU) No 1230/2012and (EU) 2015/166 (OJ L 325, 16.12.2019, p. 1);20. Regulation (EU) 2018/1139 of the European Parliament and of the Council of 4 July 2018 on common rules in thefield of civil aviation and establishing a European Union Aviation Safety Agency, and amending Regulations (EC)No 2111/2005, (EC) No 1008/2008, (EU) No 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and2014/53/EU of the European Parliament and of the Council, and repealing Regulations (EC) No 552/2004 and (EC)No 216/2008 of the European Parliament and of the Council and Council Regulation (EEC) No 3922/91 (OJ L 212,22.8.2018, p. 1), in so far as the design, production and placing on the market of aircrafts referred to in Article 2(1), points (a) and (b) thereof, where it concerns unmanned aircraft and their engines, propellers, parts and equipment tocontrol them remotely, are concerned. ANNEX IIList of criminal offences referred to in Article 5(1), first subparagraph, point (h)(iii)Criminal offences referred to in Article 5(1), first subparagraph, point (h)(iii):— terrorism,— trafficking in human beings,— sexual exploitation of children, and child pornography,— illicit trafficking in narcotic drugs or psychotropic substances,— illicit trafficking in weapons, munitions or explosives,— murder, grievous bodily injury,— illicit trade in human organs or tissue,— illicit trafficking in nuclear or radioactive materials,— kidnapping, illegal restraint or hostage-taking,— crimes within the jurisdiction of the International Criminal Court,— unlawful seizure of aircraft or ships,— rape,— environmental crime,— organised or armed robbery,— sabotage,— participation in a criminal organisation involved in one or more of the offences listed above. ANNEX IIIHigh-risk AI systems referred to in Article 6(2)High-risk AI systems pursuant to Article 6(2) are the AI systems listed in any of the following areas:1. Biometrics, in so far as their use is permitted under relevant Union or national law:(a) remote biometric identification systems. This shall not include AI systems intended to be used for biometric verification the sole purpose of which is toconfirm that a specific natural person is the person he or she claims to be;(b) AI systems intended to be used for biometric categorisation, according to sensitive or protected attributes orcharacteristics based on the inference of those attributes or characteristics;(c) AI systems intended to be used for emotion recognition.2. Critical infrastructure: AI systems intended to be used as safety components in the management and operation ofcritical digital infrastructure, road traffic, or in the supply of water, gas, heating or electricity.3. Education and vocational training:(a) AI systems intended to be used to determine access or admission or to assign natural persons to educational andvocational training institutions at all levels;(b) AI systems intended to be used to evaluate learning outcomes, including when those outcomes are used to steerthe learning process of natural persons in educational and vocational training institutions at all levels;(c) AI systems intended to be used for the purpose of assessing the appropriate level of education that an individualwill receive or will be able to access, in the context of or within educational and vocational training institutionsat all levels;(d) AI systems intended to be used for monitoring and detecting prohibited behaviour of students during tests in thecontext of or within educational and vocational training institutions at all levels.4. Employment, workers’ management and access to self-employment:(a) AI systems intended to be used for the recruitment or selection of natural persons, in particular to place targetedjob advertisements, to analyse and filter job applications, and to evaluate candidates;(b) AI systems intended to be used to make decisions affecting terms of work-related relationships, the promotion ortermination of work-related contractual relationships, to allocate tasks based on individual behaviour or personaltraits or characteristics or to monitor and evaluate the performance and behaviour of persons in suchrelationships.5. Access to and enjoyment of essential private services and essential public services and benefits:(a) AI systems intended to be used by public authorities or on behalf of public authorities to evaluate the eligibilityof natural persons for essential public assistance benefits and services, including healthcare services, as well as togrant, reduce, revoke, or reclaim such benefits and services;(b) AI systems intended to be used to evaluate the creditworthiness of natural persons or establish their credit score, with the exception of AI systems used for the purpose of detecting financial fraud;(d) AI systems intended to evaluate and classify emergency calls by natural persons or to be used to dispatch, or toestablish priority in the dispatching of, emergency first response services, including by police, firefighters andmedical aid, as well as of emergency healthcare patient triage systems.6. Law enforcement, in so far as their use is permitted under relevant Union or national law:(a) AI systems intended to be used by or on behalf of law enforcement authorities, or by Union institutions, bodies, offices or agencies in support of law enforcement authorities or on their behalf to assess the risk of a naturalperson becoming the victim of criminal offences;(b) AI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies, offices or agencies in support of law enforcement authorities as polygraphs or similar tools;(c) AI systems intended to be used by or on behalf of law enforcement authorities, or by Union institutions, bodies, offices or agencies, in support of law enforcement authorities to evaluate the reliability of evidence in the courseof the investigation or prosecution of criminal offences;(d) AI systems intended to be used by law enforcement authorities or on their behalf or by Union institutions, bodies, offices or agencies in support of law enforcement authorities for assessing the risk of a natural personoffending or re-offending not solely on the basis of the profiling of natural persons as referred to in Article 3(4)of Directive (EU) 2016/680, or to assess personality traits and characteristics or past criminal behaviour ofnatural persons or groups;(e) AI systems intended to be used by or on behalf of law enforcement authorities or by Union institutions, bodies, offices or agencies in support of law enforcement authorities for the profiling of natural persons as referred to in Article 3(4) of Directive (EU) 2016/680 in the course of the detection, investigation or prosecution of criminaloffences.7. Migration, asylum and border control management, in so far as their use is permitted under relevant Union ornational law:(a) AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies, offices or agencies as polygraphs or similar tools;(b) AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies, offices or agencies to assess a risk, including a security risk, a risk of irregular migration, or a health risk, posedby a natural person who intends to enter or who has entered into the territory of a Member State;(c) AI systems intended to be used by or on behalf of competent public authorities or by Union institutions, bodies, offices or agencies to assist competent public authorities for the examination of applications for asylum, visa orresidence permits and for associated complaints with regard to the eligibility of the natural persons applying fora status, including related assessments of the reliability of evidence;(d) AI systems intended to be used by or on behalf of competent public authorities, or by Union institutions, bodies, offices or agencies, in the context of migration, asylum or border control management, for the purpose ofdetecting, recognising or identifying natural persons, with the exception of the verification of travel documents.8. Administration of justice and democratic processes:(a) AI systems intended to be used by a judicial authority or on their behalf to assist a judicial authority in(b) AI systems intended to be used for influencing the outcome of an election or referendum or the votingbehaviour of natural persons in the exercise of their vote in elections or referenda. This does not include AIsystems to the output of which natural persons are not directly exposed, such as tools used to organise, optimiseor structure political campaigns from an administrative or logistical point of view. ANNEX IVTechnical documentation referred to in Article 11(1)The technical documentation referred to in Article 11(1) shall contain at least the following information, as applicable tothe relevant AI system:1. A general description of the AI system including:(a) its intended purpose, the name of the provider and the version of the system reflecting its relation to previousversions;(b) how the AI system interacts with, or can be used to interact with, hardware or software, including with other AIsystems, that are not part of the AI system itself, where applicable;(c) the versions of relevant software or firmware, and any requirements related to version updates;(d) the description of all the forms in which the AI system is placed on the market or put into service, such assoftware packages embedded into hardware, downloads, or APIs;(e) the description of the hardware on which the AI system is intended to run;(f) where the AI system is a component of products, photographs or illustrations showing external features, themarking and internal layout of those products;(g) a basic description of the user-interface provided to the deployer;(h) instructions for use for the deployer, and a basic description of the user-interface provided to the deployer, whereapplicable;2. A detailed description of the elements of the AI system and of the process for its development, including:(a) the methods and steps performed for the development of the AI system, including, where relevant, recourse topre-trained systems or tools provided by third parties and how those were used, integrated or modified by theprovider;(b) the design specifications of the system, namely the general logic of the AI system and of the algorithms; the keydesign choices including the rationale and assumptions made, including with regard to persons or groups ofpersons in respect of who, the system is intended to be used; the main classification choices; what the system isdesigned to optimise for, and the relevance of the different parameters; the description of the expected outputand output quality of the system; the decisions about any possible trade-off made regarding the technicalsolutions adopted to comply with the requirements set out in Chapter III, Section 2;(c) the description of the system architecture explaining how software components build on or feed into each otherand integrate into the overall processing; the computational resources used to develop, train, test and validate the AI system;(d) where relevant, the data requirements in terms of datasheets describing the training methodologies andtechniques and the training data sets used, including a general description of these data sets, information abouttheir provenance, scope and main characteristics; how the data was obtained and selected; labelling procedures(e. g. for supervised learning), data cleaning methodologies (e. g. outliers detection);(e) assessment of the human oversight measures needed in accordance with Article 14, including an assessment ofthe technical measures needed to facilitate the interpretation of the outputs of AI systems by the deployers, inaccordance with Article 13(3), point (d);(f) where applicable, a detailed description of pre-determined changes to the AI system and its performance, together with all the relevant information related to the technical solutions adopted to ensure continuouscompliance of the AI system with the relevant requirements set out in Chapter III, Section 2;(g) the validation and testing procedures used, including information about the validation and testing data used andtheir main characteristics; metrics used to measure accuracy, robustness and compliance with other relevantrequirements set out in Chapter III, Section 2, as well as potentially discriminatory impacts; test logs and all test(h) cybersecurity measures put in place;3. Detailed information about the monitoring, functioning and control of the AI system, in particular with regard to: its capabilities and limitations in performance, including the degrees of accuracy for specific persons or groups ofpersons on which the system is intended to be used and the overall expected level of accuracy in relation to itsintended purpose; the foreseeable unintended outcomes and sources of risks to health and safety, fundamental rightsand discrimination in view of the intended purpose of the AI system; the human oversight measures needed inaccordance with Article 14, including the technical measures put in place to facilitate the interpretation of theoutputs of AI systems by the deployers; specifications on input data, as appropriate;4. A description of the appropriateness of the performance metrics for the specific AI system;5. A detailed description of the risk management system in accordance with Article 9;6. A description of relevant changes made by the provider to the system through its lifecycle;7. A list of the harmonised standards applied in full or in part the references of which have been published in the Official Journal of the European Union; where no such harmonised standards have been applied, a detailed descriptionof the solutions adopted to meet the requirements set out in Chapter III, Section 2, including a list of other relevantstandards and technical specifications applied;8. A copy of the EU declaration of conformity referred to in Article 47;9. A detailed description of the system in place to evaluate the AI system performance in the post-market phase inaccordance with Article 72, including the post-market monitoring plan referred to in Article 72(3). ANNEX VEU declaration of conformity The EU declaration of conformity referred to in Article 47, shall contain all of the following information:1. AI system name and type and any additional unambiguous reference allowing the identification and traceability ofthe AI system;2. The name and address of the provider or, where applicable, of their authorised representative;3. A statement that the EU declaration of conformity referred to in Article 47 is issued under the sole responsibility ofthe provider;4. A statement that the AI system is in conformity with this Regulation and, if applicable, with any other relevant Union law that provides for the issuing of the EU declaration of conformity referred to in Article 47;5. Where an AI system involves the processing of personal data, a statement that that AI system complies with Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680;6. References to any relevant harmonised standards used or any other common specification in relation to whichconformity is declared;7. Where applicable, the name and identification number of the notified body, a description of the conformityassessment procedure performed, and identification of the certificate issued;8. The place and date of issue of the declaration, the name and function of the person who signed it, as well as anindication for, or on behalf of whom, that person signed, a signature. ANNEX VIConformity assessment procedure based on internal control1. The conformity assessment procedure based on internal control is the conformity assessment procedure based onpoints 2, 3 and 4.2. The provider verifies that the established quality management system is in compliance with the requirements of Article 17.3. The provider examines the information contained in the technical documentation in order to assess the complianceof the AI system with the relevant essential requirements set out in Chapter III, Section 2.4. The provider also verifies that the design and development process of the AI system and its post-market monitoringas referred to in Article 72 is consistent with the technical documentation. ANNEX VIIConformity based on an assessment of the quality management system and an assessment of thetechnical documentation1. Introduction Conformity based on an assessment of the quality management system and an assessment of the technicaldocumentation is the conformity assessment procedure based on points 2 to 5.2. Overview The approved quality management system for the design, development and testing of AI systems pursuant to Article 17 shall be examined in accordance with point 3 and shall be subject to surveillance as specified in point 5. The technical documentation of the AI system shall be examined in accordance with point 4.3. Quality management system3.1. The application of the provider shall include:(a) the name and address of the provider and, if the application is lodged by an authorised representative, also theirname and address;(b) the list of AI systems covered under the same quality management system;(c) the technical documentation for each AI system covered under the same quality management system;(d) the documentation concerning the quality management system which shall cover all the aspects listed under Article 17;(e) a description of the procedures in place to ensure that the quality management system remains adequate andeffective;(f) a written declaration that the same application has not been lodged with any other notified body.3.2. The quality management system shall be assessed by the notified body, which shall determine whether it satisfies therequirements referred to in Article 17. The decision shall be notified to the provider or its authorised representative. The notification shall contain the conclusions of the assessment of the quality management system and the reasonedassessment decision.3.3. The quality management system as approved shall continue to be implemented and maintained by the provider sothat it remains adequate and efficient.3.4. Any intended change to the approved quality management system or the list of AI systems covered by the latter shallbe brought to the attention of the notified body by the provider. The proposed changes shall be examined by the notified body, which shall decide whether the modified qualitymanagement system continues to satisfy the requirements referred to in point 3.2 or whether a reassessment isnecessary. The notified body shall notify the provider of its decision. The notification shall contain the conclusions of theexamination of the changes and the reasoned assessment decision.4. Control of the technical documentation.4.1. In addition to the application referred to in point 3, an application with a notified body of their choice shall belodged by the provider for the assessment of the technical documentation relating to the AI system which theprovider intends to place on the market or put into service and which is covered by the quality management systemreferred to under point 3.4.2. The application shall include:(a) the name and address of the provider;(b) a written declaration that the same application has not been lodged with any other notified body;4.3. The technical documentation shall be examined by the notified body. Where relevant, and limited to what isnecessary to fulfil its tasks, the notified body shall be granted full access to the training, validation, and testing datasets used, including, where appropriate and subject to security safeguards, through API or other relevant technicalmeans and tools enabling remote access.4.4. In examining the technical documentation, the notified body may require that the provider supply further evidenceor carry out further tests so as to enable a proper assessment of the conformity of the AI system with therequirements set out in Chapter III, Section 2. Where the notified body is not satisfied with the tests carried out bythe provider, the notified body shall itself directly carry out adequate tests, as appropriate.4.5. Where necessary to assess the conformity of the high-risk AI system with the requirements set out in Chapter III, Section 2, after all other reasonable means to verify conformity have been exhausted and have proven to beinsufficient, and upon a reasoned request, the notified body shall also be granted access to the training and trainedmodels of the AI system, including its relevant parameters. Such access shall be subject to existing Union law on theprotection of intellectual property and trade secrets.4.6. The decision of the notified body shall be notified to the provider or its authorised representative. The notificationshall contain the conclusions of the assessment of the technical documentation and the reasoned assessmentdecision. Where the AI system is in conformity with the requirements set out in Chapter III, Section 2, the notified body shallissue a Union technical documentation assessment certificate. The certificate shall indicate the name and address ofthe provider, the conclusions of the examination, the conditions (if any) for its validity and the data necessary for theidentification of the AI system. The certificate and its annexes shall contain all relevant information to allow the conformity of the AI system to beevaluated, and to allow for control of the AI system while in use, where applicable. Where the AI system is not in conformity with the requirements set out in Chapter III, Section 2, the notified bodyshall refuse to issue a Union technical documentation assessment certificate and shall inform the applicantaccordingly, giving detailed reasons for its refusal. Where the AI system does not meet the requirement relating to the data used to train it, re-training of the AI systemwill be needed prior to the application for a new conformity assessment. In this case, the reasoned assessmentdecision of the notified body refusing to issue the Union technical documentation assessment certificate shallcontain specific considerations on the quality data used to train the AI system, in particular on the reasons fornon-compliance.4.7. Any change to the AI system that could affect the compliance of the AI system with the requirements or its intendedpurpose shall be assessed by the notified body which issued the Union technical documentation assessmentcertificate. The provider shall inform such notified body of its intention to introduce any of the abovementionedchanges, or if it otherwise becomes aware of the occurrence of such changes. The intended changes shall be assessedby the notified body, which shall decide whether those changes require a new conformity assessment in accordancewith Article 43(4) or whether they could be addressed by means of a supplement to the Union technicaldocumentation assessment certificate. In the latter case, the notified body shall assess the changes, notify theprovider of its decision and, where the changes are approved, issue to the provider a supplement to the Uniontechnical documentation assessment certificate.5. Surveillance of the approved quality management system.5.1. The purpose of the surveillance carried out by the notified body referred to in Point 3 is to make sure that theprovider duly complies with the terms and conditions of the approved quality management system.5.2. For assessment purposes, the provider shall allow the notified body to access the premises where the design, development, testing of the AI systems is taking place. The provider shall further share with the notified body allnecessary information.5.3. The notified body shall carry out periodic audits to make sure that the provider maintains and applies the qualitymanagement system and shall provide the provider with an audit report. In the context of those audits, the notifiedbody may carry out additional tests of the AI systems for which a Union technical documentation assessmentcertificate was issued. ANNEX VIIIInformation to be submitted upon the registration of high-risk AI systems in accordance with Article 49Section A — Information to be submitted by providers of high-risk AI systems in accordance with Article 49(1)The following information shall be provided and thereafter kept up to date with regard to high-risk AI systems to beregistered in accordance with Article 49(1):1. The name, address and contact details of the provider;2. Where submission of information is carried out by another person on behalf of the provider, the name, address andcontact details of that person;3. The name, address and contact details of the authorised representative, where applicable;4. The AI system trade name and any additional unambiguous reference allowing the identification and traceability ofthe AI system;5. A description of the intended purpose of the AI system and of the components and functions supported throughthis AI system;6. A basic and concise description of the information used by the system (data, inputs) and its operating logic;7. The status of the AI system (on the market, or in service; no longer placed on the market/in service, recalled);8. The type, number and expiry date of the certificate issued by the notified body and the name or identificationnumber of that notified body, where applicable;9. A scanned copy of the certificate referred to in point 8, where applicable;10. Any Member States in which the AI system has been placed on the market, put into service or made available in the Union;11. A copy of the EU declaration of conformity referred to in Article 47;12. Electronic instructions for use; this information shall not be provided for high-risk AI systems in the areas of lawenforcement or migration, asylum and border control management referred to in Annex III, points 1, 6 and 7;13. A URL for additional information (optional). Section B — Information to be submitted by providers of high-risk AI systems in accordance with Article 49(2)The following information shall be provided and thereafter kept up to date with regard to AI systems to be registered inaccordance with Article 49(2):1. The name, address and contact details of the provider;2. Where submission of information is carried out by another person on behalf of the provider, the name, address andcontact details of that person;3. The name, address and contact details of the authorised representative, where applicable;4. The AI system trade name and any additional unambiguous reference allowing the identification and traceability ofthe AI system;5. A description of the intended purpose of the AI system;6. The condition or conditions under Article 6(3)based on which the AI system is considered to be not-high-risk;7. A short summary of the grounds on which the AI system is considered to be not-high-risk in application of theprocedure under Article 6(3);8. The status of the AI system (on the market, or in service; no longer placed on the market/in service, recalled); Section C — Information to be submitted by deployers of high-risk AI systems in accordance with Article 49(3)The following information shall be provided and thereafter kept up to date with regard to high-risk AI systems to beregistered in accordance with Article 49(3):1. The name, address and contact details of the deployer;2. The name, address and contact details of the person submitting information on behalf of the deployer;3. The URL of the entry of the AI system in the EU database by its provider;4. A summary of the findings of the fundamental rights impact assessment conducted in accordance with Article 27;5. A summary of the data protection impact assessment carried out in accordance with Article 35 of Regulation (EU)2016/679 or Article 27 of Directive (EU) 2016/680 as specified in Article 26(8) of this Regulation, whereapplicable. ANNEX IXInformation to be submitted upon the registration of high-risk AI systems listed in Annex III inrelation to testing in real world conditions in accordance with Article 60The following information shall be provided and thereafter kept up to date with regard to testing in real world conditions tobe registered in accordance with Article 60:1. A Union-wide unique single identification number of the testing in real world conditions;2. The name and contact details of the provider or prospective provider and of the deployers involved in the testing inreal world conditions;3. A brief description of the AI system, its intended purpose, and other information necessary for the identification ofthe system;4. A summary of the main characteristics of the plan for testing in real world conditions;5. Information on the suspension or termination of the testing in real world conditions. ANNEX XUnion legislative acts on large-scale IT systems in the area of Freedom, Security and Justice1. Schengen Information System(a) Regulation (EU) 2018/1860 of the European Parliament and of the Council of 28 November 2018 on the use ofthe Schengen Information System for the return of illegally staying third-country nationals (OJ L 312,7.12.2018, p. 1).(b) Regulation (EU) 2018/1861 of the European Parliament and of the Council of 28 November 2018 on theestablishment, operation and use of the Schengen Information System (SIS) in the field of border checks, andamending the Convention implementing the Schengen Agreement, and amending and repealing Regulation (EC)No 1987/2006 (OJ L 312, 7.12.2018, p. 14).(c) Regulation (EU) 2018/1862 of the European Parliament and of the Council of 28 November 2018 on theestablishment, operation and use of the Schengen Information System (SIS) in the field of police cooperation andjudicial cooperation in criminal matters, amending and repealing Council Decision 2007/533/JHA, andrepealing Regulation (EC) No 1986/2006 of the European Parliament and of the Council and Commission Decision 2010/261/EU (OJ L 312, 7.12.2018, p. 56).2. Visa Information System(a) Regulation (EU) 2021/1133 of the European Parliament and of the Council of 7 July 2021 amending Regulations (EU) No 603/2013, (EU) 2016/794, (EU) 2018/1862, (EU) 2019/816 and (EU) 2019/818 as regardsthe establishment of the conditions for accessing other EU information systems for the purposes of the Visa Information System (OJ L 248, 13.7.2021, p. 1).(b) Regulation (EU) 2021/1134 of the European Parliament and of the Council of 7 July 2021 amending Regulations (EC) No 767/2008, (EC) No 810/2009, (EU) 2016/399, (EU) 2017/2226, (EU) 2018/1240, (EU)2018/1860, (EU) 2018/1861, (EU) 2019/817 and (EU) 2019/1896 of the European Parliament and of the Council and repealing Council Decisions 2004/512/EC and 2008/633/JHA, for the purpose of reforming the Visa Information System (OJ L 248, 13.7.2021, p. 11).3. Eurodac Regulation (EU) 2024/1358 of the European Parliament and of the Council of 14 May 2024 on the establishment of‘Eurodac’ for the comparison of biometric data in order to effectively apply Regulations (EU) 2024/1315 and (EU)2024/1350 of the European Parliament and of the Council and Council Directive 2001/55/EC and to identifyillegally staying third-country nationals and stateless persons and on requests for the comparison with Eurodac databy Member States’ law enforcement authorities and Europol for law enforcement purposes, amending Regulations(EU) 2018/1240 and (EU) 2019/818 of the European Parliament and of the Council and repealing Regulation (EU)No 603/2013 of the European Parliament and of the Council (OJ L, 2024/1358, 22.5.2024, ELI: europa. eu/eli/reg/2024/1358/oj).4. Entry/Exit System Regulation (EU) 2017/2226 of the European Parliament and of the Council of 30 November 2017 establishing an Entry/Exit System (EES) to register entry and exit data and refusal of entry data of third-country nationals crossingthe external borders of the Member States and determining the conditions for access to the EES for law enforcementpurposes, and amending the Convention implementing the Schengen Agreement and Regulations (EC)No 767/2008 and (EU) No 1077/2011 (OJ L 327, 9.12.2017, p. 20).5. European Travel Information and Authorisation System(a) Regulation (EU) 2018/1240 of the European Parliament and of the Council of 12 September 2018 establishinga European Travel Information and Authorisation System (ETIAS) and amending Regulations (EU)No 1077/2011, (EU) No 515/2014, (EU) 2016/399, (EU) 2016/1624 and (EU) 2017/2226 (OJ L 236,19.9.2018, p. 1).(b) Regulation (EU) 2018/1241 of the European Parliament and of the Council of 12 September 2018 amending6. European Criminal Records Information System on third-country nationals and stateless persons Regulation (EU) 2019/816 of the European Parliament and of the Council of 17 April 2019 establishinga centralised system for the identification of Member States holding conviction information on third-countrynationals and stateless persons (ECRIS-TCN) to supplement the European Criminal Records Information System andamending Regulation (EU) 2018/1726 (OJ L 135, 22.5.2019, p. 1).7. Interoperability(a) Regulation (EU) 2019/817 of the European Parliament and of the Council of 20 May 2019 on establishinga framework for interoperability between EU information systems in the field of borders and visa and amending Regulations (EC) No 767/2008, (EU) 2016/399, (EU) 2017/2226, (EU) 2018/1240, (EU) 2018/1726 and (EU)2018/1861 of the European Parliament and of the Council and Council Decisions 2004/512/EC and2008/633/JHA (OJ L 135, 22.5.2019, p. 27).(b) Regulation (EU) 2019/818 of the European Parliament and of the Council of 20 May 2019 on establishinga framework for interoperability between EU information systems in the field of police and judicial cooperation, asylum and migration and amending Regulations (EU) 2018/1726, (EU) 2018/1862 and (EU) 2019/816 (OJL 135, 22.5.2019, p. 85). ANNEX XITechnical documentation referred to in Article 53(1), point (a) — technical documentation forproviders of general-purpose AI models Section 1Information to be provided by all providers of general-purpose AI models The technical documentation referred to in Article 53(1), point (a) shall contain at least the following information asappropriate to the size and risk profile of the model:1. A general description of the general-purpose AI model including:(a) the tasks that the model is intended to perform and the type and nature of AI systems in which it can beintegrated;(b) the acceptable use policies applicable;(c) the date of release and methods of distribution;(d) the architecture and number of parameters;(e) the modality (e. g. text, image) and format of inputs and outputs;(f) the licence.2. A detailed description of the elements of the model referred to in point 1, and relevant information of the processfor the development, including the following elements:(a) the technical means (e. g. instructions of use, infrastructure, tools) required for the general-purpose AI model tobe integrated in AI systems;(b) the design specifications of the model and training process, including training methodologies and techniques, the key design choices including the rationale and assumptions made; what the model is designed to optimise forand the relevance of the different parameters, as applicable;(c) information on the data used for training, testing and validation, where applicable, including the type andprovenance of data and curation methodologies (e. g. cleaning, filtering, etc.), the number of data points, theirscope and main characteristics; how the data was obtained and selected as well as all other measures to detect theunsuitability of data sources and methods to detect identifiable biases, where applicable;(d) the computational resources used to train the model (e. g. number of floating point operations), training time, and other relevant details related to the training;(e) known or estimated energy consumption of the model. With regard to point (e), where the energy consumption of the model is unknown, the energy consumption may bebased on information about computational resources used. Section 2Additional information to be provided by providers of general-purpose AI models with systemic risk1. A detailed description of the evaluation strategies, including evaluation results, on the basis of available publicevaluation protocols and tools or otherwise of other evaluation methodologies. Evaluation strategies shall includeevaluation criteria, metrics and the methodology on the identification of limitations.3. Where applicable, a detailed description of the system architecture explaining how software components build orfeed into each other and integrate into the overall processing. ANNEX XIITransparency information referred to in Article 53(1), point (b) — technical documentation forproviders of general-purpose AI models to downstream providers that integrate the model into their AI system The information referred to in Article 53(1), point (b) shall contain at least the following:1. A general description of the general-purpose AI model including:(a) the tasks that the model is intended to perform and the type and nature of AI systems into which it can beintegrated;(b) the acceptable use policies applicable;(c) the date of release and methods of distribution;(d) how the model interacts, or can be used to interact, with hardware or software that is not part of the modelitself, where applicable;(e) the versions of relevant software related to the use of the general-purpose AI model, where applicable;(f) the architecture and number of parameters;(g) the modality (e. g. text, image) and format of inputs and outputs;(h) the licence for the model.2. A description of the elements of the model and of the process for its development, including:(a) the technical means (e. g. instructions for use, infrastructure, tools) required for the general-purpose AI model tobe integrated into AI systems;(b) the modality (e. g. text, image, etc.) and format of the inputs and outputs and their maximum size (e. g. contextwindow length, etc.);(c) information on the data used for training, testing and validation, where applicable, including the type andprovenance of data and curation methodologies. ANNEX XIIICriteria for the designation of general-purpose AI models with systemic risk referred to in Article 51For the purpose of determining that a general-purpose AI model has capabilities or an impact equivalent to those set out in Article 51(1), point (a), the Commission shall take into account the following criteria:(a) the number of parameters of the model;(b) the quality or size of the data set, for example measured through tokens;(c) the amount of computation used for training the model, measured in floating point operations or indicated bya combination of other variables such as estimated cost of training, estimated time required for the training, orestimated energy consumption for the training;(d) the input and output modalities of the model, such as text to text (large language models), text to image, multi-modality, and the state of the art thresholds for determining high-impact capabilities for each modality, andthe specific type of inputs and outputs (e. g. biological sequences);(e) the benchmarks and evaluations of capabilities of the model, including considering the number of tasks withoutadditional training, adaptability to learn new, distinct tasks, its level of autonomy and scalability, the tools it hasaccess to;(f) whether it has a high impact on the internal market due to its reach, which shall be presumed when it has beenmade available to at least 10 000 registered business users established in the Union;(g) the number of registered end-users.

--- DOCUMENT: EU_GPAI_Code.pdf ---

Introductory note by the Chair and Vice-Chair of the Transparency Chapter. The Transparency Chapter of the Code of Practice describes three Measures which Signatories committo implementing to comply with their transparency obligations under Article 53(1), points (a) and (b), and the corresponding Annexes XI and XII of the AI Act. To facilitate compliance and fulfilment of the commitments contained in Measure 1.1, we includea user-friendly Model Documentation Form which allows Signatories to easily compile the informationrequired by the aforementioned provisions of the AI Act in a single place. The Model Documentation Form indicates for each item whether the information is intended fordownstream providers, the AI Office or national competent authorities. Information intended for the AI Office or national competent authorities is only to be made available following a request from the AI Office, either ex officio or based on a request to the AI Office from national competent authorities. Such requests will state the legal basis and purpose of the request and will concern only items fromthe Form that are strictly necessary for the AI Office to fulfil its tasks under the AI Act at the time ofthe request, or for national competent authorities to exercise their supervisory tasks under the AI Actat the time of the request, in particular to assess compliance of providers high-risk AI systems built ongeneral-purpose AI models where the provider of the system is different from the provider of themodel. In accordance with Article 78 AI Act, the recipients of any of the information contained in the Model Documentation Form are obliged to respect the confidentiality of the information obtained, in particular intellectual property rights and confidential business information or trade secrets, andto put in place adequate and effective cybersecurity measures to protect the security andconfidentiality of the information obtained. Objectives The overarching objective of this Code of Practice (“Code”) is to improve the functioning of theinternal market, to promote the uptake of human-centric and trustworthy artificial intelligence (“AI”), while ensuring a high level of protection of health, safety, and fundamental rights enshrined in the Charter, including democracy, the rule of law, and environmental protection, against harmful effectsof AI in the Union, and to support innovation pursuant to Article 1(1) AI Act. To achieve this overarching objective, the specific objectives of this Code are: A. To serve as a guiding document for demonstrating compliance with the obligations providedfor in Articles 53 and 55 AI Act, while recognising that adherence to the Code does notconstitute conclusive evidence of compliance with these obligations under the AI Act. B. To ensure providers of general-purpose AI models comply with their obligations under the AIAct and to enable the AI Office to assess compliance of providers of general-purpose AImodels who choose to rely on the Code to demonstrate compliance with their obligationsunder the AI Act. Recitals Whereas:(a) The Signatories recognise the particular role and responsibility of providers of general-purpose AI models along the AI value chain, as the models they provide may form the basisfor a range of downstream AI systems, often provided by downstream providers that need agood understanding of the models and their capabilities, both to enable the integration ofsuch models into their products and to fulfil their obligations under the AI Act (see recital 101AI Act).(b) The Signatories recognise that in the case of a fine-tuning or other modification of a general-purpose AI model, where the natural or legal person, public authority, agency or other bodythat modifies the model becomes the provider of the modified model subject to theobligations for providers of general purpose AI models, their Commitments under the Transparency Chapter of the Code should be limited to that modification or fine-tuning, tocomply with the principle of proportionality (see recital 109 AI Act). In this context, Signatories should take into account relevant guidelines by the European Commission.(c) The Signatories recognise that, without exceeding the Commitments under the Transparency Chapter of this Code, when providing information to the AI Office or to downstream providersthey may need to take into account market and technological developments, so that theinformation continues to serve its purpose of allowing the AI Office and national competentauthorities to fulfil their tasks under the AI Act, and downstream providers to integrate the Signatories’ models into AI systems and to comply with their obligations under the AI Act (see Article 56(2), point (a), AI Act). This Chapter of the Code focuses on the documentation obligations from Article 53(1), points (a) and(b), AI Act that are applicable to all providers of general-purpose AI models (without prejudice to theexception laid down in Article 53(2) AI Act), namely those concerning Annex XI, Section 1, and Annex XII AI Act. The documentation obligations concerning Annex XI, Section 2, AI Act, applicable only toproviders of general-purpose AI models with systemic risk are covered by Measure 10.1 of the Safetyand Security Chapter of this Code. Commitment 1 Documentation LEGAL TEXT: Articles 53(1)(a), 53(1)(b), 53(2), 53(7), and Annexes XI and XII AI Act In order to fulfil the obligations in Article 53(1), points (a) and (b), AI Act, Signatories commit todrawing up and keeping up-to-date model documentation in accordance with Measure 1.1, providingrelevant information to providers of AI systems who intend to integrate the general-purpose AI modelinto their AI systems (‘downstream providers’ hereafter), and to the AI Office upon request (possiblyon behalf of national competent authorities upon request to the AI Office when this is strictlynecessary for the exercise of their supervisory tasks under the AI Act, in particular to assess thecompliance of a high-risk AI system built on a general-purpose AI model where the provider of thesystem is different from the provider of the model1), in accordance with Measure 1.2, and ensuringquality, security, and integrity of the documented information in accordance with Measure 1.3. Inaccordance with Article 53(2) AI Act, these Measures do not apply to providers of general-purpose AImodels released under a free and open-source license that satisfy the conditions specified in thatprovision, unless the model is a general-purpose AI model with systemic risk. Measure 1.1 Drawing up and keeping up-to-date model documentation Signatories, when placing a general-purpose AI model on the market, will have documented at leastall the information referred to in the Model Documentation Form below (hereafter this informationis referred to as the ‘Model Documentation’). Signatories may choose to complete the Model Documentation Form provided in the Appendix to comply with this commitment. Signatories will update the Model Documentation to reflect relevant changes in the informationcontained in the Model Documentation, including in relation to updated versions of the same model, while keeping previous versions of the Model Documentation for a period ending 10 years after themodel has been placed on the market. Measure 1.2 Providing relevant information Signatories, when placing a general-purpose AI model on the market, will publicly disclose via theirwebsite, or via other appropriate means if they do not have a website, contact information for the AIOffice and downstream providers to request access to the relevant information contained in the Model Documentation, or other necessary information. Signatories will provide, upon a request from the AI Office pursuant to Articles 91 or 75(3) AI Act forone or more elements of the Model Documentation, or any additional information, that are necessaryfor the AI Office to fulfil its tasks under the AI Act or for national competent authorities to exercisetheir supervisory tasks under the AI Act, in particular to assess compliance of high-risk AI systems builton general-purpose AI models where the provider of the system is different from the provider of themodel,2 the requested information in its most up-to-date form, within the period specified in the AIOffice’s request in accordance with Article 91(4) AI Act.1 See Article 75(1) and (3) AI Act and Article 88(2) AI Act.2 See Article 75(1) and (3) and Article 88(2) AI Act Signatories will provide to downstream providers the information contained in the most up-to-date Model Documentation that is intended for downstream providers, subject to the confidentialitysafeguards and conditions provided for under Articles 53(7) and 78 AI Act. Furthermore, withoutprejudice to the need to observe and protect intellectual property rights and confidential businessinformation or trade secrets in accordance with Union and national law, Signatories will provideadditional information upon a request from downstream providers insofar as such information isnecessary to enable them to have a good understanding of the capabilities and limitations of thegeneral-purpose AI model relevant for its integration into the downstream providers’ AI system andto enable those downstream providers to comply with their obligations pursuant to the AI Act. Signatories will provide such information within a reasonable timeframe, and no later than 14 days ofreceiving the request save for exceptional circumstances. Signatories are encouraged to consider whether the documented information can be disclosed, inwhole or in part, to the public to promote public transparency. Some of this information may also berequired in a summarised form as part of the training content summary that providers must makepublicly available under Article 53(1), point (d), AI Act, according to a template to be provided by the AI Office. Measure 1.3 Ensuring quality, integrity, and security of information Signatories will ensure that the documented information is controlled for quality and integrity, retained as evidence of compliance with obligations in the AI Act, and protected from unintendedalterations. In the context of drawing-up, updating, and controlling the quality and security of theinformation and records, Signatories are encouraged to follow the established protocols and technicalstandards. Model Documentation Form Below is a static version of the Model Documentation Form. In this version, the input fields cannot befilled in. A fillable version of this form is separately available. Objectives The overarching objective of this Code of Practice (“Code”) is to improve the functioning of theinternal market, to promote the uptake of human-centric and trustworthy artificial intelligence (“AI”), while ensuring a high level of protection of health, safety, and fundamental rights enshrined in the Charter, including democracy, the rule of law, and environmental protection, against harmful effectsof AI in the Union, and to support innovation pursuant to Article 1(1) AI Act. To achieve this overarching objective, the specific objectives of this Code are: A. To serve as a guiding document for demonstrating compliance with the obligations providedfor in Articles 53 and 55 AI Act, while recognising that adherence to the Code does notconstitute conclusive evidence of compliance with these obligations under the AI Act. B. To ensure providers of general-purpose AI models comply with their obligations under the AIAct and to enable the AI Office to assess compliance of providers of general-purpose AImodels who choose to rely on the Code to demonstrate compliance with their obligationsunder the AI Act. Recitals Whereas:(a) This Chapter aims to contribute to the proper application of the obligation laid down in Article53(1), point (c), of the AI Act pursuant to which providers that place general-purpose AImodels on the Union market must put in place a policy to comply with Union law on copyrightand related rights, and in particular to identify and comply with, including through state-of-the-art technologies, a reservation of rights expressed by rightsholders pursuant to Article 4(3) of Directive (EU) 2019/790. While Signatories will implement the Measures setout in this Chapter in order to demonstrate compliance with Article 53(1), point (c), of the AIAct, adherence to the Code does not constitute compliance with Union law on copyright andrelated rights.(b) This Chapter in no way affects the application and enforcement of Union law on copyrightand related right which is for the courts of Member States and ultimately the Court of Justiceof the European Union to interpret.(c) The Signatories hereby acknowledge that Union law on copyright and related rights:(i) is provided for in directives addressed to Member States and that for presentpurposes Directive 2001/29/EC, Directive (EU) 2019/790 and Directive 2004/48/ECare the most relevant;(ii) provides for exclusive rights that are preventive in nature and thus is based on priorconsent save where an exception or limitation applies;(iii) provides for an exception or limitation for text and data mining in Article 4(1) of Directive (EU) 2019/790 which shall apply on conditions of lawful access and that theuse of works and other subject matter referred to in that paragraph has not beenexpressly reserved by their rightsholders in an appropriate manner pursuant to Article 4(3) of Directive (EU) 2019/790.(d) The commitments in this Chapter that require proportionate measures should becommensurate and proportionate to the size of providers, taking due account of the interestsof SMEs, including startups.(e) This Chapter does not affect agreements between the Signatories and rightsholdersauthorising the use of works and other protected subject matter.(f) The commitments in this Chapter to demonstrate compliance with the obligation under Article 53(1), point (c), of the AI Act are complementary to the obligation of providers under Article 53(1), point (d), of the AI Act to draw up and make publicly available sufficientlydetailed summaries about the content used by the Signatories for the training of theirgeneral-purpose AI models, according to a template to be provided by the AI Office. Commitment 1 Copyright policy LEGAL TEXT: Article 53(1)(c) AI Act(1) In order to demonstrate compliance with their obligation pursuant to Article 53(1), point (c)of the AI Act to put in place a policy to comply with Union law on copyright and related rights, and in particular to identify and comply with, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790, Signatories commit to drawing up, keeping up-to-date and implementing such a copyrightpolicy. The Measures below do not affect compliance with Union law on copyright and relatedrights. They set out commitments by which the Signatories can demonstrate compliance withthe obligation to put in place a copyright policy for their general-purpose AI models they placeon the Union market.(2) In addition, the Signatories remain responsible for verifying that the Measures included intheir copyright policy as outlined below comply with Member States’ implementation of Union law on copyright and related rights, in particular but not only Article 4(3) of Directive(EU) 2019/790, before carrying out any copyright-relevant act in the territory of the relevant Member State as failure to do so may give rise to liability under Union law on copyright andrelated rights. Measure 1.1 Draw up, keep up-to-date and implement a copyright policy(1) Signatories will draw up, keep up-to-date and implement a policy to comply with Union lawon copyright and related rights for all general-purpose AI models they place on the Unionmarket. Signatories commit to describe that policy in a single document incorporating the Measures set out in this Chapter. Signatories will assign responsibilities within theirorganisation for the implementation and overseeing of this policy.(2) Signatories are encouraged to make publicly available and keep up-to-date a summary oftheir copyright policy. Measure 1.2 Reproduce and extract only lawfully accessible copyright-protected contentwhen crawling the World Wide Web(1) In order to help ensure that Signatories only reproduce and extract lawfully accessible worksand other protected subject matter if they use web-crawlers or have such web-crawlers usedon their behalf to scrape or otherwise compile data for the purpose of text and data miningas defined in Article 2(2) of Directive (EU) 2019/790 and the training of their general-purpose AI models, Signatories commit: a) not to circumvent effective technological measures as defined in Article 6(3) of Directive 2001/29/EC that are designed to prevent or restrict unauthorised acts inrespect of works and other protected subject matter, in particular by respecting anytechnological denial or restriction of access imposed by subscription models orpaywalls, andb) to exclude from their web-crawling websites that make available to the public contentand which are, at the time of web-crawling, recognised as persistently and repeatedlyinfringing copyright and related rights on a commercial scale by courts or publicauthorities in the European Union and the European Economic Area. For the purposeof compliance with this measure, a dynamic list of hyperlinks to lists of these websitesissued by the relevant bodies in the European Union and the European Economic Areawill be made publicly available on an EU website. Measure 1.3 Identify and comply with rights reservations when crawling the World Wide Web(1) In order to help ensure that Signatories will identify and comply with, including through state-of-the-art technologies, machine-readable reservations of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790 if they use web-crawlers or have such web-crawlersused on their behalf to scrape or otherwise compile data for the purpose of text and datamining as defined in Article 2(2) of Directive (EU) 2019/790 and the training of their general-purpose AI models, Signatories commit: a) to employ web-crawlers that read and follow instructions expressed in accordancewith the Robot Exclusion Protocol (robots. txt), as specified in the Internet Engineering Task Force (IETF) Request for Comments No. 9309, and any subsequent version of this Protocol for which the IETF demonstrates that it is technically feasible andimplementable by AI providers and content providers, including rightsholders, andb) to identify and comply with other appropriate machine-readable protocols to expressrights reservations pursuant to Article 4(3) of Directive (EU) 2019/790, for examplethrough asset-based or location-based metadata, that have either have been adoptedby international or European standardisation organisations, or are state-of-the-art, including technically implementable, and widely adopted by rightsholders, considering different cultural sectors, and generally agreed through an inclusiveprocess based on bona fide discussions to be facilitated at EU level with theinvolvement of rightsholders, AI providers and other relevant stakeholders as a moreimmediate solution, while anticipating the development of standards.(2) This commitment does not affect the right of rightsholders to expressly reserve the use ofworks and other protected subject matter for the purposes of text and data mining pursuantto Article 4(3) of Directive (EU) 2019/790 in any appropriate manner, such as machine-readable means in the case of content made publicly available online or by other means. Furthermore, this commitment does not affect the application of Union law on copyright andrelated rights to protected content scraped or crawled from the internet by third parties andused by Signatories for the purpose of text and data mining and the training of their general-purpose AI models, in particular with regard to rights reservations expressed pursuant to Article 4(3) of Directive (EU) 2019/790.(3) Signatories are encouraged to support the processes referred to in the first paragraph, points(a) and (b), of this Measure and engage on a voluntary basis in bona fide discussions withrightsholders and other relevant stakeholders, with the aim to develop appropriate machine-readable standards and protocols to express a rights reservation pursuant to Article 4(3) of Directive (EU) 2019/790.(4) Signatories commit to take appropriate measures to enable affected rightsholders to obtaininformation about the web crawlers employed, their robots. txt features and other measuresthat a Signatory adopts to identify and comply with rights reservations expressed pursuantto Article 4(3) of Directive (EU) 2019/790 at the time of crawling by making public suchinformation and by providing a means for affected rightsholders to be automatically notifiedwhen such information is updated (such as by syndicating a web feed) without prejudice tothe right of information provided for in Article 8 of Directive 2004/48/EC.(5) Signatories that also provide an online search engine as defined in Article 3, point (j), of Regulation (EU) 2022/2065 or control such a provider are encouraged to take appropriatemeasures to ensure that their compliance with a rights reservation in the context of text anddata mining activities and the training of general-purpose AI models referred to in the firstparagraph of this Measure does not directly lead to adverse effects on the indexing of thecontent, domain(s) and/or URL(s), for which a rights reservation has been expressed, in theirsearch engine. Measure 1.4 Mitigate the risk of copyright-infringing outputs(1) In order to mitigate the risk that a downstream AI system, into which a general-purpose AImodel is integrated, generates output that may infringe rights in works or other subjectmatter protected by Union law on copyright or related rights, Signatories commit: a) to implement appropriate and proportionate technical safeguards to prevent theirmodels from generating outputs that reproduce training content protected by Unionlaw on copyright and related rights in an infringing manner, andb) to prohibit copyright-infringing uses of a model in their acceptable use policy, termsand conditions, or other equivalent documents, or in case of general-purpose AImodels released under free and open source licenses to alert users to the prohibitionof copyright infringing uses of the model in the documentation accompanying themodel without prejudice to the free and open source nature of the license.(2) This Measure applies irrespective of whether a Signatory vertically integrates the model intoits own AI system(s) or whether the model is provided to another entity based on contractualrelations. Measure 1.5 Designate a point of contact and enable the lodging of complaints(1) Signatories commit to designate a point of contact for electronic communication withaffected rightsholders and provide easily accessible information about it.(2) Signatories commit to put a mechanism in place to allow affected rightsholders and theirauthorised representatives, including collective management organisations, to submit, byelectronic means, sufficiently precise and adequately substantiated complaints concerningthe non-compliance of Signatories with their commitments pursuant to this Chapter andprovide easily accessible information about it. Signatories will act on complaints in a diligent, non-arbitrary manner and within a reasonable time, unless a complaint is manifestlyunfounded or the Signatory has already responded to an identical complaint by the samerightsholder. This commitment does not affect the measures, remedies and sanctionsavailable to enforce copyright and related rights under Union and national law. Objectives The overarching objective of this Code of Practice (“Code”) is to improve the functioning of theinternal market, to promote the uptake of human-centric and trustworthy artificial intelligence (“AI”), while ensuring a high level of protection of health, safety, and fundamental rights enshrined in the Charter, including democracy, the rule of law, and environmental protection, against harmful effectsof AI in the Union, and to support innovation pursuant to Article 1(1) AI Act. To achieve this overarching objective, the specific objectives of this Code are: A. To serve as a guiding document for demonstrating compliance with the obligations providedfor in Articles 53 and 55 AI Act, while recognising that adherence to the Code does notconstitute conclusive evidence of compliance with these obligations under the AI Act. B. To ensure providers of general-purpose AI models comply with their obligations under the AIAct and to enable the AI Office to assess compliance of providers of general-purpose AImodels who choose to rely on the Code to demonstrate compliance with their obligationsunder the AI Act. Recitals Whereas:(a) Principle of Appropriate Lifecycle Management. The Signatories recognise that providers ofgeneral-purpose AI models with systemic risk should continuously assess and mitigatesystemic risks, taking appropriate measures along the entire model lifecycle (including duringdevelopment that occurs before and after a model has been placed on the market), cooperating with and taking into account relevant actors along the AI value chain (such asstakeholders likely to be affected by the model), and ensuring their systemic riskmanagement is made future-proof by regular updates in response to improving and emergingmodel capabilities (see recitals 114 and 115 AI Act). Accordingly, the Signatories recognisethat implementing appropriate measures will often require Signatories to adopt at least thestate of the art, unless systemic risk can be conclusively ruled out with a less advancedprocess, measure, methodology, method, or technique. Systemic risk assessment is a multi-step process and model evaluations, referring to a range of methods used in assessingsystemic risks of models, are integral along the entire model lifecycle. When systemic riskmitigations are implemented, the Signatories recognise the importance of continuouslyassessing their effectiveness.(b) Principle of Contextual Risk Assessment and Mitigation. The Signatories recognise that this Safety and Security Chapter (“Chapter”) is only relevant for providers of general-purpose AImodels with systemic risk and not AI systems. However, the Signatories also recognise thatthe assessment and mitigation of systemic risks should include, as reasonably foreseeable, the system architecture, other software into which the model may be integrated, and thecomputing resources available at inference time because of their importance to the model’seffects, for example by affecting the effectiveness of safety and security mitigations.(c) Principle of Proportionality to Systemic Risks. The Signatories recognise that the assessmentand mitigation of systemic risks should be proportionate to the risks (Article 56(2), point (d)AI Act). Therefore, the degree of scrutiny in systemic risk assessment and mitigation, inparticular the level of detail in documentation and reporting, should be proportionate to thesystemic risks at the relevant points along the entire model lifecycle. The Signatoriesrecognise that while systemic risk assessment and mitigation is iterative and continuous, theyneed not duplicate assessments that are still appropriate to the systemic risks stemming fromthe model.(d) Principle of Integration with Existing Laws. The Signatories recognise that this Chapter formspart of, and is complemented by, other Union laws. The Signatories further recognise thatconfidentiality (including commercial confidentiality) obligations are preserved to the extentrequired by Union law, and information sent to the European AI Office (“AI Office”) inadherence to this Chapter will be treated pursuant to Article 78 AI Act. Additionally, the Signatories recognise that information about future developments and future businessactivities that they submit to the AI Office will be understood as subject to change. The Signatories further recognise that the Measure under this Chapter to promote a healthy riskculture (Measure 8.3) is without prejudice to any obligations arising from Directive (EU)2019/1937 on the protection of whistleblowers and implementing laws of Member States inconjunction with Article 87 AI Act. The Signatories also recognise that they may be able torely on international standards to the extent they cover the provisions of this Chapter.(e) Principle of Cooperation. The Signatories recognise that systemic risk assessment andmitigation merit significant investment of time and resources. They recognise the advantagesof collaborative efficiency, e. g. by sharing model evaluations methods and/or infrastructure. The Signatories further recognise the importance of cooperation with licensees, downstreammodifiers, and downstream providers in systemic risk assessment and mitigation, and ofengaging expert or lay representatives of civil society, academia, and other relevantstakeholders in understanding the model effects. The Signatories recognise that suchcooperation may involve entering into agreements to share information relevant to systemicrisk assessment and mitigation, while ensuring proportionate protection of sensitiveinformation and compliance with applicable Union law. The Signatories further recognise theimportance of cooperating with the AI Office (Article 53(3) AI Act) to foster collaborationbetween providers of general-purpose AI models with systemic risk, researchers, andregulatory bodies to address emerging challenges and opportunities in the AI landscape.(f) Principle of Innovation in AI Safety and Security. The Signatories recognise that determiningthe most effective methods for understanding and ensuring the safety and security ofgeneral-purpose AI models with systemic risk remains an evolving challenge. The Signatoriesrecognise that this Chapter should encourage providers of general-purpose AI models withsystemic risk to advance the state of the art in AI safety and security and related processesand measures. The Signatories recognise that advancing the state of the art also includesdeveloping targeted methods that specifically address risks while maintaining beneficialcapabilities (e. g. mitigating biosecurity risks without unduly reducing beneficial biomedicalcapabilities), acknowledging that such precision demands greater technical effort andinnovation than less targeted methods. The Signatories further recognise that if providers ofgeneral-purpose AI models with systemic risk can demonstrate equal or superior safety orsecurity outcomes through alternative means that achieve greater efficiency, suchinnovations should be recognised as advancing the state of the art in AI safety and securityand meriting consideration for wider adoption.(g) Precautionary Principle. The Signatories recognise the important role of the Precautionary Principle, particularly for systemic risks for which the lack or quality of scientific data does notyet permit a complete assessment. Accordingly, the Signatories recognise that theextrapolation of current adoption rates and research and development trajectories of modelsshould be taken into account for the identification of systemic risks.(h) Small and medium enterprises (“SMEs”) and small mid-cap enterprises (“SMCs”). Toaccount for differences between providers of general-purpose AI models with systemic riskregarding their size and capacity, simplified ways of compliance for SMEs and SMCs, includingstartups, should be possible as proportionate. For example, SMEs and SMCs may beexempted from some reporting commitments (Article 56(5) AI Act). Signatories that are SMEsor SMCs and are exempted from reporting commitments recognise that they maynonetheless voluntarily adhere to them.(i) Interpretation. The Signatories recognise that all Commitments and Measures shall beinterpreted in light of the objective to assess and mitigate systemic risks. The Signatoriesfurther recognise that given the rapid pace of AI development, purposive interpretationfocused on systemic risk assessment and mitigation is particularly important to ensure this Chapter remains effective, relevant, and future-proof. Additionally, any term appearing inthis Chapter that is defined in the Glossary for this Chapter has the meaning set forth in that Glossary. The Signatories recognise that Appendix 1 should be interpreted, in instances ofdoubt, in good faith in light of: (1) the probability and severity of harm pursuant to thedefinition of ‘risk’ in Article 3(2) AI Act; and (2) the definition of ‘systemic risk’ in Article 3(65)AI Act. The Signatories recognise that this Chapter is to be interpreted in conjunction and inaccordance with any AI Office guidance on the AI Act.(j) Serious Incident Reporting. The Signatories recognise that the reporting of a serious incidentis not an admission of wrongdoing. Further, they recognise that relevant information aboutserious incidents cannot be kept track of, documented, and reported at the model level onlyin retrospect after a serious incident has occurred. The information that could directly orindirectly lead up to such an event is often dispersed and may be lost, overwritten, orfragmented by the time Signatories become aware of a serious incident. This justifies theestablishment of processes and measures to keep track of and document relevantinformation before serious incidents occur. Commitment 1 Safety and Security Framework LEGAL TEXT: Articles 55(1) and 56(5), and recitals 110, 114, and 115 AI Act Signatories commit to adopting a state-of-the-art Safety and Security Framework (“Framework”). Thepurpose of the Framework is to outline the systemic risk management processes and measures that Signatories implement to ensure the systemic risks stemming from their models are acceptable. Signatories commit to a Framework adoption process that involves three steps:(1) creating the Framework (as specified in Measure 1.1);(2) implementing the Framework (as specified in Measure 1.2); and(3) updating the Framework (as specified in Measure 1.3). Further, Signatories commit to notifying the AI Office of their Framework (as specified in Measure1.4). Figure 1. Process for creating, implementing, and updating Frameworks. The text of the Commitments and Measures takes precedence. Measure 1.1 Creating the Framework Signatories will create a state-of-the-art Framework, taking into account the models they aredeveloping, making available on the market, and/or using. The Framework will contain a high-level description of implemented and planned processes andmeasures for systemic risk assessment and mitigation to adhere to this Chapter. In addition, the Framework will contain:(1) a description and justification of the trigger points and their usage, at which the Signatorieswill conduct additional lighter-touch model evaluations along the entire model lifecycle, asspecified in Measure 1.2, second paragraph, point (1)(a);(2) for the Signatories’ determination of whether systemic risk is considered acceptable, asspecified in Commitment 4:(a) a description and justification of the systemic risk acceptance criteria, including thesystemic risk tiers, and their usage as specified in Measure 4.1;(b) a high-level description of what safety and security mitigations Signatories wouldneed to implement once each systemic risk tier is reached;(c) for each systemic risk that Signatories defined systemic risk tiers for as specified in Measure 4.1, estimates of timelines when Signatories reasonably foresee that theywill have a model that exceeds the highest systemic risk tier already reached by anyof their existing models. Such estimates: (i) may consist of time ranges or probabilitydistributions; and (ii) may take into account aggregate forecasts, surveys, and otherestimates produced with other providers. Further, such estimates will be supportedby justifications, including underlying assumptions and uncertainties; and(d) a description of whether and, if so, by what process input from external actors, including governments, influences proceeding with the development, makingavailable on the market, and/or use of the Signatories’ models as specified in Measure 4.2, other than as the result of independent external evaluations;(3) a description of how systemic risk responsibility is allocated for the processes by whichsystemic risk is assessed and mitigated as specified in Commitment 8; and(4) a description of the process by which Signatories will update the Framework, including howthey will determine that an updated Framework is confirmed, as specified in Measure 1.3. Signatories will have confirmed the Framework no later than four weeks after having notified the Commission pursuant to Article 52(1) AI Act and no later than two weeks before placing the modelon the market. Measure 1.2 Implementing the Framework Signatories will implement the processes and measures outlined in their Framework as specified inthe following paragraphs. Along the entire model lifecycle, Signatories will continuously:(1) assess the systemic risks stemming from the model by:(a) conducting lighter-touch model evaluations that need not adhere to Appendix 3 (e. g. automated evaluations) at appropriate trigger points defined in terms of, e. g. time, training compute, development stages, user access, inference compute, and/oraffordances;(b) conducting post-market monitoring after placing the model on the market, asspecified in Measure 3.5;(c) taking into account relevant information about serious incidents (pursuant to Commitment 9); and(d) increasing the breadth and/or depth of assessment or conducting a full systemic riskassessment and mitigation process that is specified in the following paragraph, basedon the results of points (a), (b), and (c); and(2) implement systemic risk mitigations taking into account the results of point (1), includingaddressing serious incidents as appropriate. Figure 2. Illustrative timeline of systemic risk assessment and mitigation along the model lifecycle. The text of the Commitments and Measures takes precedence. In addition, Signatories will implement a full systemic risk assessment and mitigation process thatinvolves four steps, without needing to duplicate parts of the model’s previous systemic riskassessments that are still appropriate:(1) identifying the systemic risks stemming from the model as specified in Commitment 2;(2) analysing each identified systemic risk as specified in Commitment 3;(3) determining whether the systemic risks stemming from the model are acceptable as specifiedin Measure 4.1; and(4) if the systemic risks stemming from the model are not determined to be acceptable, implementing safety and/or security mitigations as specified in Commitments 5 and 6, andre-assessing the systemic risks stemming from the model starting from point (1), as specifiedin Measure 4.2. Signatories will conduct such a full systemic risk assessment and mitigation process at least beforeplacing the model on the market and whenever the conditions specified in Measure 7.6, first andthird paragraph, are met. Signatories will report their implemented measures and processes to the AI Office as specified in Commitment 7. Figure 3. Full systemic risk assessment and mitigation process. The text of the Commitments and Measures takes precedence. Measure 1.3 Updating the Framework Signatories will update the Framework as appropriate, including without undue delay after a Framework assessment (specified in the following paragraphs), to ensure the information in Measure1.1 is kept up-to-date and the Framework is at least state-of-the-art. For any update of the Framework, Signatories will include a changelog, describing how and why the Framework has beenupdated, along with a version number and the date of change. Signatories will conduct an appropriate Framework assessment, if they have reasonable grounds tobelieve that the adequacy of their Framework and/or their adherence thereto has been or will bematerially undermined, or every 12 months starting from their placing of the model on the market, whichever is sooner. Examples of such grounds are:(1) how the Signatories develop models will change materially, which can be reasonably foreseento lead to the systemic risks stemming from at least one of their models not being acceptable;(2) serious incidents and/or near misses involving their models or similar models that are likelyto indicate that the systemic risks stemming from at least one of their models are notacceptable have occurred; and/or(3) the systemic risks stemming from at least one of their models have changed or are likely tochange materially, e. g. safety and/or security mitigations have become or are likely tobecome materially less effective, or at least one of their models has developed or is likely todevelop materially changed capabilities and/or propensities. A Framework assessment will include the following:(1) Framework adequacy: An assessment of whether the processes and measures in the Framework are appropriate for the systemic risks stemming from the Signatories’ models. This assessment will take into account how the models are currently being developed, made available on the market, and/or used, and how they are expected to be developed, made available on the market, and/or used over the next 12 months.(2) Framework adherence: An assessment focused on the Signatories’ adherence to the Framework, including: (a) any instances of, and reasons for, non-adherence to the Frameworksince the last Framework assessment; and (b) any measures, including safety and securitymitigations, that need to be implemented to ensure continued adherence to the Framework. If point(s) (a) and/or (b) give rise to risks of future non-adherence, Signatories will makeremediation plans as part of their Framework assessment. Measure 1.4 Framework notifications Signatories will provide the AI Office with (unredacted) access to their Framework, and updatesthereof, within five business days of either being confirmed. Commitment 2 Systemic risk identification LEGAL TEXT: Article 55(1) and recital 110 AI Act Signatories commit to identifying the systemic risks stemming from the model. The purpose ofsystemic risk identification includes facilitating systemic risk analysis (pursuant to Commitment 3) andsystemic risk acceptance determination (pursuant to Commitment 4). Systemic risk identification involves two elements:(1) following a structured process to identify the systemic risks stemming from the model (asspecified in Measure 2.1); and(2) developing systemic risk scenarios for each identified systemic risk (as specified in Measure 2.2). Figure 4. Systemic risk identification process. The text of the Commitments and Measures takes precedence. Measure 2.1 Systemic risk identification process Signatories will identify:(1) the systemic risks obtained through the following process:(a) compiling a list of risks that could stem from the model and be systemic, based onthe types of risks in Appendix 1.1, taking into account:(i) model-independent information (pursuant to Measure 3.1);(ii) relevant information about the model and similar models, includinginformation from post-market monitoring (pursuant to Measure 3.5), andinformation about serious incidents and near misses (pursuant to Commitment 9); and(iii) any other relevant information communicated directly or via public releasesto the Signatory by the AI Office, the Scientific Panel of Independent Experts, or other initiatives, such as the International Network of AI Safety Institutes, endorsed for this purpose by the AI Office;(b) analysing relevant characteristics of the risks compiled pursuant to point (a), such astheir nature (based on Appendix 1.2) and sources (based on Appendix 1.3); and(c) identifying, based on point (b), the systemic risks stemming from the model; and(2) the specified systemic risks in Appendix 1.4. Measure 2.2 Systemic risk scenarios Signatories will develop appropriate systemic risk scenarios, including regarding the number and levelof detail of these systemic risk scenarios, for each identified systemic risk (pursuant to Measure 2.1). Commitment 3 Systemic risk analysis LEGAL TEXT: Article 55(1) and recital 114 AI Act Signatories commit to analysing each identified systemic risk (pursuant to Commitment 2). Thepurpose of systemic risk analysis includes facilitating systemic risk acceptance determination(pursuant to Commitment 4). Systemic risk analysis involves five elements for each identified systemic risk, which may overlap andmay need to be implemented recursively:(1) gathering model-independent information (as specified in Measure 3.1);(2) conducting model evaluations (as specified in Measure 3.2);(3) modelling the systemic risk (as specified in Measure 3.3); and(4) estimating the systemic risk (as specified in Measure 3.4); while(5) conducting post-market monitoring (as specified in Measure 3.5). Measure 3.1 Model-independent information Signatories will gather model-independent information relevant to the systemic risk. Signatories will search for and gather such information with varying degrees of breadth and depthappropriate for the systemic risk, using methods such as:(1) web searches (e. g. making use of open-source intelligence methods in collecting andanalysing information gathered from open sources);(2) literature reviews;(3) market analyses (e. g. focused on capabilities of other models available on the market);(4) reviews of training data (e. g. for indications of data poisoning or tampering);(5) reviewing and analysing historical incident data and incident databases;(6) forecasting of general trends (e. g. forecasts concerning the development of algorithmicefficiency, compute use, data availability, and energy use);(7) expert interviews and/or panels; and/or(8) lay interviews, surveys, community consultations, or other participatory research methodsinvestigating, e. g. the effects of models on natural persons, including vulnerable groups. Measure 3.2 Model evaluations Signatories will conduct at least state-of-the-art model evaluations in the modalities relevant to thesystemic risk to assess the model’s capabilities, propensities, affordances, and/or effects, as specifiedin Appendix 3. Signatories will ensure that such model evaluations are designed and conducted using methods thatare appropriate for the model and the systemic risk, and include open-ended testing of the model toimprove the understanding of the systemic risk, with a view to identifying unexpected behaviours, capability boundaries, or emergent properties. Examples of model evaluation methods are: Q&A sets, task-based evaluations, benchmarks, red-teaming and other methods of adversarial testing, humanuplift studies, model organisms, simulations, and/or proxy evaluations for classified materials. Further, the design of the model evaluations will be informed by the model-independent informationgathered pursuant to Measure 3.1. Measure 3.3 Systemic risk modelling Signatories will conduct systemic risk modelling for the systemic risk. To this end, Signatories will:(1) use at least state-of-the-art risk modelling methods;(2) build on the systemic risk scenarios developed pursuant to Measure 2.2; and(3) take into account at least the information gathered pursuant to Measure 2.1 and this Commitment. Measure 3.4 Systemic risk estimation Signatories will estimate the probability and severity of harm for the systemic risk. Signatories will use at least state-of-the-art risk estimation methods and take into account at leastthe information gathered pursuant to Commitment 2, this Commitment, and Commitment 9. Estimates of systemic risk will be expressed as a risk score, risk matrix, probability distribution, or inother adequate formats, and may be quantitative, semi-quantitative, and/or qualitative. Examples ofsuch estimates of systemic risks are: (1) a qualitative systemic risk score (e. g. “moderate” or “critical”);(2) a qualitative systemic risk matrix (e. g. “probability: unlikely” x “impact: high”); and/or (3) aquantitative systemic risk matrix (e. g. “X-Y%” x “X-Y EUR damage”). Measure 3.5 Post-market monitoring Signatories will conduct appropriate post-market monitoring to gather information relevant toassessing whether the systemic risk could be determined to not be acceptable (pursuant to Measure4.1) and to inform whether a Model Report update is necessary (pursuant to Measure 7.6). Further, Signatories will use best efforts to conduct post-market monitoring to gather information relevant toproducing estimates of timelines (pursuant to Measure 1.1, point (2)(c)). To these ends, post-market monitoring will:(1) gather information about the model’s capabilities, propensities, affordances, and/or effects;(2) take into account the exemplary methods listed below; and(3) if Signatories themselves provide and/or deploy AI systems that integrate their own model, include monitoring the model as part of these AI systems. The following are examples of post-market monitoring methods for the purpose of point (2) above:(1) collecting end-user feedback;(2) providing (anonymous) reporting channels;(3) providing (serious) incident reporting forms;(4) providing bug bounties;(5) establishing community-driven model evaluations and public leaderboards;(6) conducting frequent dialogues with affected stakeholders;(7) monitoring software repositories, known malware, public forums, and/or social media forpatterns of use;(8) supporting the scientific study of the model’s capabilities, propensities, affordances, and/oreffects in collaboration with academia, civil society, regulators, and/or independentresearchers;(9) implementing privacy-preserving logging and metadata analysis techniques of the model’sinputs and outputs using, e. g. watermarks, metadata, and/or other at least state-of-the-artprovenance techniques;(10) collecting relevant information about breaches of the model’s use restrictions andsubsequent incidents arising from such breaches; and/or(11) monitoring aspects of models that are relevant for assessing and mitigating systemic risk andare not transparent to third parties, e. g. hidden chains-of-thought for models for which theparameters are not publicly available for download. To facilitate post-market monitoring, Signatories will provide an adequate number of independentexternal evaluators with adequate free access to:(1) the model’s most capable model version(s) with regard to the systemic risk that is madeavailable on the market;(2) the chains-of-thought of the model version(s) in point (1), if available; and(3) the model version(s) corresponding to the model version(s) in point (1) with the fewest safetymitigations implemented with regard to the systemic risk (such as the helpful-only modelversion, if it exists) and, as available, its chains-of-thought; unless the model is considered a similarly safe or safer model with regard to the same systemic risk(pursuant to Appendix 2.2). Such access to a model may be provided by Signatories through an API, on-premise access (including transport), access via Signatory-provided hardware, or by making themodel parameters publicly available for download, as appropriate. For the purpose of selecting independent external evaluators for the preceding paragraph, Signatories will publish suitable criteria for assessing applications. The number of such evaluators, theselection criteria, and security measures may differ for points (1), (2), and (3) in the precedingparagraph. Signatories will only access, store, and/or analyse evaluation results from independent externalevaluators to assess and mitigate systemic risk from the model. In particular, Signatories refrain fromtraining their models on the inputs and/or outputs from such test runs without express permissionfrom the evaluators. Additionally, Signatories will not take any legal or technical retaliation againstthe independent external evaluators as a consequence of their testing and/or publication of findingsas long as the evaluators:(1) do not intentionally disrupt model availability through the testing, unless expresslypermitted;(2) do not intentionally access, modify, and/or use sensitive or confidential user data in violationof Union law, and if evaluators do access such data, collect only what is necessary, refrain(3) do not intentionally use their access for activities that pose a significant risk to public safetyand security;(4) do not use findings to threaten Signatories, users, or other actors in the value chain, providedthat disclosure under pre-agreed policies and timelines will not be counted as such coercion; and(5) adhere to the Signatory’s publicly available procedure for responsible vulnerability disclosure, which will specify at least that the Signatory cannot delay or block publication for more than30 business days from the date that the Signatory is made aware of the findings, unless alonger timeline is exceptionally necessary such as if disclosure of the findings would materiallyincrease the systemic risk. Signatories that are SMEs or SMCs may contact the AI Office, which may provide support or resourcesto facilitate adherence to this Measure. Commitment 4 Systemic risk acceptance determination LEGAL TEXT: Article 55(1) AI Act Signatories commit to specifying systemic risk acceptance criteria and determining whether thesystemic risks stemming from the model are acceptable (as specified in Measure 4.1). Signatoriescommit to deciding whether or not to proceed with the development, the making available on themarket, and/or the use of the model based on the systemic risk acceptance determination (asspecified in Measure 4.2). Measure 4.1 Systemic risk acceptance criteria and acceptance determination Signatories will describe and justify (in the Framework pursuant to Measure 1.1, point (2)(a)) howthey will determine whether the systemic risks stemming from the model are acceptable. To do so, Signatories will:(1) for each identified systemic risk (pursuant to Measure 2.1), at least:(a) define appropriate systemic risk tiers that:(i) are defined in terms of model capabilities, and may additionally incorporatemodel propensities, risk estimates, and/or other suitable metrics;(ii) are measurable; and(iii) comprise at least one systemic risk tier that has not been reached by themodel; or(b) define other appropriate systemic risk acceptance criteria, if systemic risk tiers arenot suitable for the systemic risk and the systemic risk is not a specified systemic risk(pursuant to Appendix 1.4);(2) describe how they will use these tiers and/or other criteria to determine whether eachidentified systemic risk (pursuant to Measure 2.1) and the overall systemic risk areacceptable; and(3) justify how the use of these tiers and/or other criteria pursuant to point (2) ensures that eachidentified systemic risk (pursuant to Measure 2.1) and the overall systemic risk areacceptable. Signatories will apply the systemic risk acceptance criteria to each identified systemic risk (pursuantto Measure 2.1), incorporating a safety margin (as specified in the following paragraph), to determinewhether each identified systemic risk (pursuant to Measure 2.1) and the overall systemic risk areacceptable. This acceptance determination will take into account at least the information gatheredvia systemic risk identification and analysis (pursuant to Commitments 2 and 3). The safety margin will:(1) be appropriate for the systemic risk; and(2) take into account potential limitations, changes, and uncertainties of:(a) systemic risk sources (e. g. capability improvements after the time of assessment);(b) systemic risk assessments (e. g. under-elicitation of model evaluations or historicalaccuracy of similar assessments); and(c) the effectiveness of safety and security mitigations (e. g. mitigations beingcircumvented, deactivated, or subverted). Measure 4.2 Proceeding or not proceeding based on systemic risk acceptancedetermination Signatories will only proceed with the development, the making available on the market, and/or theuse of the model, if the systemic risks stemming from the model are determined to be acceptable(pursuant to Measure 4.1). If the systemic risks stemming from the model are not determined to be acceptable or are reasonablyforeseeable to be soon not determined to be acceptable (pursuant to Measure 4.1), Signatories willtake appropriate measures to ensure the systemic risks stemming from the model are and will remainacceptable prior to proceeding. In particular, Signatories will:(1) not make the model available on the market, restrict the making available on the market (e. g. via adjusting licenses or usage restrictions), withdraw, or recall the model, as necessary;(2) implement safety and/or security mitigations (pursuant to Commitments 5 and 6); and(3) conduct another round of systemic risk identification (pursuant to Commitment 2), systemicrisk analysis (pursuant to Commitment 3), and systemic risk acceptance determination(pursuant to this Commitment). Commitment 5 Safety mitigations LEGAL TEXT: Article 55(1) and recital 114 AI Act Signatories commit to implementing appropriate safety mitigations along the entire model lifecycle, as specified in the Measure for this Commitment, to ensure the systemic risks stemming from themodel are acceptable (pursuant to Commitment 4). Measure 5.1 Appropriate safety mitigations Signatories will implement safety mitigations that are appropriate, including sufficiently robust underadversarial pressure (e. g. fine-tuning attacks or jailbreaking), taking into account the model’s releaseand distribution strategy. Examples of safety mitigations are:(1) filtering and cleaning training data, e. g. data that might result in undesirable modelpropensities such as unfaithful chain-of-thought traces;(2) monitoring and filtering the model’s inputs and/or outputs;(3) changing the model behaviour in the interests of safety, such as fine-tuning the model torefuse certain requests or provide unhelpful responses;(4) staging the access to the model, e. g. by limiting API access to vetted users, graduallyexpanding access based on post-market monitoring, and/or not making the modelparameters publicly available for download initially;(5) offering tools for other actors to use to mitigate the systemic risks stemming from the model;(6) techniques that provide high-assurance quantitative safety guarantees concerning themodel’s behaviour;(7) techniques to enable safe ecosystems of AI agents, such as model identifications, specialisedcommunication protocols, or incident monitoring tools; and/or(8) other emerging safety mitigations, such as for achieving transparency into chain-of-thoughtreasoning or defending against a model’s ability to subvert its other safety mitigations. Commitment 6 Security mitigations LEGAL TEXT: Article 55(1), and recitals 114 and 115 AI Act Signatories commit to implementing an adequate level of cybersecurity protection for their modelsand their physical infrastructure along the entire model lifecycle, as specified in the Measures for this Commitment, to ensure the systemic risks stemming from their models that could arise fromunauthorised releases, unauthorised access, and/or model theft are acceptable (pursuant to Commitment 4). A model is exempt from this Commitment if the model’s capabilities are inferior to the capabilities ofat least one model for which the parameters are publicly available for download. Signatories will implement these security mitigations for a model until its parameters are madepublicly available for download or securely deleted. Measure 6.1 Security Goal Signatories will define a goal that specifies the threat actors that their security mitigations areintended to protect against (“Security Goal”), including non-state external threats, insider threats, and other expected threat actors, taking into account at least the current and expected capabilitiesof their models. Measure 6.2 Appropriate security mitigations Signatories will implement appropriate security mitigations to meet the Security Goal, including thesecurity mitigations pursuant to Appendix 4. If Signatories deviate from any of the security mitigationslisted in Appendices 4.1 to 4.5, points (a), e. g. due to the Signatory’s organisational context and digitalinfrastructure, they will implement alternative security mitigations that achieve the respectivemitigation objectives. The implementation of the required security mitigations may be staged appropriately in line with theincrease in model capabilities along the entire model lifecycle. Commitment 7 Safety and Security Model Reports LEGAL TEXT: Articles 55(1) and 56(5) AI Act Signatories commit to reporting to the AI Office information about their model and their systemic riskassessment and mitigation processes and measures by creating a Safety and Security Model Report(“Model Report”) before placing a model on the market (as specified in Measures 7.1 to 7.5). Further, Signatories commit to keeping the Model Report up-to-date (as specified in Measure 7.6) andnotifying the AI Office of their Model Report (as specified in Measure 7.7). If Signatories have already provided relevant information to the AI Office in other reports and/ornotifications, they may reference those reports and/or notifications in their Model Report. Signatories may create a single Model Report for several models if the systemic risk assessment andmitigation processes and measures for one model cannot be understood without reference to theother model(s). Signatories that are SMEs or SMCs may reduce the level of detail in their Model Report to the extentnecessary to reflect size and capacity constraints. Measure 7.1 Model description and behaviour Signatories will provide in the Model Report:(1) a high-level description of the model’s architecture, capabilities, propensities, andaffordances, and how the model has been developed, including its training method and data, as well as how these differ from other models they have made available on the market;(2) a description of how the model has been used and is expected to be used, including its use inthe development, oversight, and/or evaluation of models;(3) a description of the model versions that are going to be made or are currently made availableon the market and/or used, including differences in systemic risk mitigations and systemicrisks; and(4) a specification (e. g. via valid hyperlinks) of how Signatories intend the model to operate(often known as a “model specification”), including by:(a) specifying the principles that the model is intended to follow;(b) stating how the model is intended to prioritise different kinds of principles andinstructions;(c) listing topics on which the model is intended to refuse instructions; and(d) providing the system prompt. Measure 7.2 Reasons for proceeding Signatories will provide in the Model Report:(1) a detailed justification for why the systemic risks stemming from the model are acceptable, including details of the safety margins incorporated (pursuant to Measure 4.1);(2) the reasonably foreseeable conditions under which the justification in point (1) would nolonger hold; and(3) a description of how the decision to proceed with the development, making available on themarket, and/or use (pursuant to Measure 4.2) was made, including whether input fromexternal actors informed such a decision (pursuant to Measure 1.1, point (2)(d)), and whetherand how input from independent external evaluators pursuant to Appendix 3.5 informedsuch a decision. Measure 7.3 Documentation of systemic risk identification, analysis, and mitigation Signatories will provide in the Model Report:(1) a description of the results of their systemic risk identification and analysis and anyinformation relevant to understanding them including:(a) a description of their systemic risk identification process for risks belonging to thetypes of risks in Appendix 1.1 (pursuant to Measure 2.1, point (1));(b) explanations of uncertainties and assumptions about how the model would be usedand integrated into AI systems;(c) a description of the results of their systemic risk modelling for the systemic risks(pursuant to Measure 3.3);(d) a description of the systemic risks stemming from the model and a justificationtherefor, including: (i) the systemic risk estimates (pursuant to Measure 3.4); and (ii)a comparison between systemic risks with safety and security mitigationsimplemented and with the model fully elicited (pursuant to Appendix 3.2);(e) all results of model evaluations relevant to understanding the systemic risksstemming from the model and descriptions of: (i) how the evaluations wereconducted; (ii) the tests and tasks involved in the model evaluations; (iii) how themodel evaluations were scored; (iv) how the model was elicited (pursuant to Appendix 3.2); (v) how the scores compare to human baselines (where applicable), across the model versions, and across the evaluation settings;(f) at least five, random samples of inputs and outputs from each relevant modelevaluation, such as completions, generations, and/or trajectories, to facilitateindependent interpretation of the model evaluation results and understanding of thesystemic risks stemming from the model. If particular trajectories materially informthe understanding of a systemic risk, such trajectories will also be provided. Further, Signatories will provide a sufficiently large number of random samples of inputs andoutputs from a relevant model evaluation if subsequently asked by the AI Office;(g) a description of the access and other resources provided to: (i) internal modelevaluation teams (pursuant to Appendix 3.4); and (ii) independent externalevaluators pursuant to Appendix 3.5. Alternatively to the preceding point (ii), Signatories will procure any such independent external evaluators to provide therequisite information directly to the AI Office at the same time that the Signatorysupplies its Model Report to the AI Office; and(h) if they make use of the “similarly safe or safer model” concept pursuant to Appendix2, provide a justification of how the criteria for “safe reference model” (pursuant to Appendix 2.1) and the criteria for “similarly safe or safer model” (pursuant to Appendix 2.2) are fulfilled;(2) a description of: (a) all safety mitigations implemented (pursuant to Commitment 5); (b) howthey fulfil the requirements of Measure 5.1; and (c) their limitations (e. g. if training onexamples of undesirable model behaviour makes identifying future instances of suchbehaviour more difficult);(3) a description of: (a) the Security Goal (pursuant to Measure 6.1); (b) all security mitigationsimplemented (pursuant to Measure 6.2); (c) how the mitigations meet the Security Goal, including the extent to which they align with relevant international standards or otherrelevant guidance (such as the RAND Securing AI Model Weights report); and (d) if Signatorieshave deviated from a listed security mitigation in one (or more) of Appendices 4.1 to 4.5, points (a), a justification for how the alternative security mitigations they have implementedachieve the respective mitigation objectives; and(4) a high-level description of: (a) the techniques and assets they intend to use to further developthe model over the next six months, including through the use of other AI models and/or AIsystems; (b) how such future versions and more advanced models may differ from the Signatory’s current ones, in terms of capabilities and propensities; and (c) any new ormaterially updated safety and security mitigations that they intend to implement for suchmodels. Measure 7.4 External reports Signatories will provide in the Model Report:(1) any available reports (e. g. via valid hyperlinks) from:(a) independent external evaluators involved in model evaluations pursuant to Appendix3.5; and(b) security reviews undertaken by an independent external party pursuant to Appendix4.5; to the extent that respects existing confidentiality (including commercial confidentiality)obligations and allows such external evaluators or parties to maintain control over thepublication of their findings, without implicit endorsement by the Signatories of the contentof such reports;(2) if no independent external evaluator was involved in model evaluations pursuant to Appendix3.5, a justification of how the conditions in Appendix 3.5, first paragraph, points (1) or (2)were met; and(3) if at least one independent external evaluator was involved in model evaluations pursuant to Appendix 3.5, an explanation of the choice of evaluator based on the qualification criteria. Measure 7.5 Material changes to the systemic risk landscape Signatories will ensure that the Model Report contains information relevant for the AI Office tounderstand whether and how the development, making available on the market, and/or use of themodel result in material changes in the systemic risk landscape that are relevant for theimplementation of systemic risk assessment and mitigation measures and processes under this Chapter. Examples of such information are:(1) a description of scaling laws that suggest novel ways of improving model capabilities;(2) a summary of the characteristics of novel architectures that materially improve the state ofthe art in computational efficiency or model capabilities;(3) a description of information relevant to assessing the effectiveness of mitigations, e. g. if themodel’s chain-of-thought is less legible by humans; and/or(4) a description of training techniques that materially improve the efficiency or feasibility of Measure 7.6 Model Report updates Signatories will update their Model Report if they have reasonable grounds to believe that thejustification for why the systemic risks stemming from the model are acceptable (pursuant to Measure 7.2, point (1)) has been materially undermined. Examples of such grounds are:(1) one of the conditions listed pursuant to Measure 7.2, point (2), has materialised;(2) the model’s capabilities, propensities, and/or affordances have changed or will changematerially, such as through further post-training, access to additional tools, or increase ininference compute;(3) the model’s use and/or integrations into AI systems have changed or will change materially;(4) serious incidents and/or near misses involving the model or a similar model have occurred; and/or(5) developments have occurred that materially undermine the external validity of modelevaluations conducted, materially improve the state of the art of model evaluation methods, and/or for other reasons suggest that the systemic risk assessment conducted is materiallyinaccurate. Model Report updates should be completed within a reasonable amount of time after the Signatorybecomes aware of the grounds that necessitate an update, e. g. after discovering them as part of theircontinuous systemic risk assessment and mitigation (pursuant to Measure 1.2, second paragraph). Ifa Model Report update is triggered by a deliberate change to a model and that change is madeavailable on the market, the Model Report update and the underlying full systemic risk assessmentand mitigation process (pursuant to Measure 1.2, third paragraph) need to be completed before thechange is made available on the market. Further, if the model is amongst their respective most capable models available on the market, Signatories will provide the AI Office with an updated Model Report at least every six months. Signatories do not need to do so if: (1) the model’s capabilities, propensities, and/or affordances havenot changed since they have last provided the AI Office with the Model Report, or update thereof; (2)they will place a more capable model on the market in less than a month; and/or (3) the model isconsidered similarly safe or safer (pursuant to Appendix 2.2) for each identified systemic risk(pursuant to Measure 2.1). The updated Model Report will contain:(1) the updated information specified in Measures 7.1 to 7.5 based on the results of the fullsystemic risk assessment and mitigation process (pursuant to Measure 1.2, third paragraph); and(2) a changelog, describing how and why the Model Report has been updated, along with aversion number and the date of change. Measure 7.7 Model Report notifications Signatories will provide the AI Office with access to the Model Report (without redactions, unless theyare required by national security laws to which Signatories are subject) by the time they place a modelon the market, e. g. through a publicly accessible link or through a sufficiently secure channel specifiedby the AI Office. If a Model Report is updated, Signatories will provide the AI Office with access to theupdated Model Report (without redactions, unless they are required by national security laws to To facilitate the placing on the market of a model, Signatories may delay providing the AI Office witha Model Report, or an update thereof, by up to 15 business days. This may be done only if the AIOffice considers the Signatory to be acting in good faith and if the Signatory provides an interim Model Report, containing the information specified in Measures 7.2 and 7.5, to the AI Office without delay. Commitment 8 Systemic risk responsibility allocation LEGAL TEXT: Article 55(1) and recital 114 AI Act Signatories commit to: (1) defining clear responsibilities for managing the systemic risks stemmingfrom their models across all levels of the organisation (as specified in Measure 8.1); (2) allocatingappropriate resources to actors who have been assigned responsibilities for managing systemic risk(as specified in Measure 8.2); and (3) promoting a healthy risk culture (as specified in Measure 8.3). Measure 8.1 Definition of clear responsibilities Signatories will clearly define responsibilities for managing the systemic risks stemming from theirmodels across all levels of the organisation. This includes the following responsibilities:(1) Systemic risk oversight: Overseeing the Signatories’ systemic risk assessment and mitigationprocesses and measures.(2) Systemic risk ownership: Managing systemic risks stemming from Signatories’ models, including the systemic risk assessment and mitigation processes and measures, and managingthe response to serious incidents.(3) Systemic risk support and monitoring: Supporting and monitoring the Signatories’ systemicrisk assessment and mitigation processes and measures.(4) Systemic risk assurance: Providing internal and, as appropriate, external assurance about theadequacy of the Signatories’ systemic risk assessment and mitigation processes and measuresto the management body in its supervisory function or another suitable independent body(such as a council or board). Signatories will allocate these responsibilities, as suitable for the Signatories’ governance structureand organisational complexity, across the following levels of their organisation:(1) the management body in its supervisory function or another suitable independent body (suchas a council or board);(2) the management body in its executive function;(3) relevant operational teams;(4) if available, internal assurance providers (e. g. an internal audit function); and(5) if available, external assurance providers (e. g. third-party auditors). This Measure is presumed to be fulfilled, if Signatories, as appropriate for the systemic risks stemmingfrom their models, adhere to all of the following:(1) Systemic risk oversight: The responsibility for overseeing the Signatory’s systemic riskmanagement processes and measures has been assigned to a specific committee of themanagement body in its supervisory function (e. g. a risk committee or audit committee) orone or multiple suitable independent bodies (such as councils or boards). For Signatories thatare SMEs or SMCs, this responsibility may be primarily assigned to an individual member ofthe management body in its supervisory function.(2) Systemic risk ownership: The responsibility for managing systemic risks from models hasbeen assigned to suitable members of the management body in its executive function whoare also responsible for relevant Signatory core business activities that may give rise tosystemic risk, such as research and product development (e. g. Head of Research or Head of Product). The members of the management body in its executive function have assignedlower-level responsibilities to operational managers who oversee parts of the systemic-risk-producing business activities (e. g. specific research domains or specific products). Dependingon the organisational complexity, there may be a cascading responsibility structure.(3) Systemic risk support and monitoring: The responsibility for supporting and monitoring the Signatory’s systemic risk management processes and measures, including conducting riskassessments, has been assigned to at least one member of the management body in itsexecutive function (e. g. a Chief Risk Officer or a Vice President, Safety & Security Framework). This member(s) must not also be responsible for the Signatory’s core business activities thatmay produce systemic risk (e. g. research and product development). For Signatories that are SMEs or SMCs, there is at least one individual in the management body in its executivefunction tasked with supporting and monitoring the Signatory’s systemic risk assessment andmitigation processes and measures.(4) Systemic risk assurance: The responsibility for providing assurance about the adequacy ofthe Signatory’s systemic risk assessment and mitigation processes and measures to themanagement body in its supervisory function or another suitable independent body (such asa council or board) has been assigned to a relevant party (e. g. a Chief Audit Executive, a Headof Internal Audit, or a relevant sub-committee). This individual is supported by an internalaudit function, or equivalent, and external assurance as appropriate. The Signatories’ internalassurance activities are appropriate. For Signatories that are SMEs or SMCs, the managementbody in its supervisory function periodically assesses the Signatory’s systemic risk assessmentand mitigation processes and measures (e. g. by approving the Signatory’s Frameworkassessment). Measure 8.2 Allocation of appropriate resources Signatories will ensure that their management bodies oversee the allocation of resources to thosewho have been assigned responsibilities (pursuant to Measure 8.1) that are appropriate for thesystemic risks stemming from their models. The allocation of such resources will include:(1) human resources;(2) financial resources;(3) access to information and knowledge; and(4) computational resources. Measure 8.3 Promotion of a healthy risk culture Signatories will promote a healthy risk culture and take appropriate measures to ensure that actorswho have been assigned responsibilities for managing the systemic risks stemming from their models(pursuant to Measure 8.1) take a reasoned and balanced approach to systemic risk. Examples of indicators of a healthy risk culture for the purpose of this Measure are:(1) setting the tone for a healthy systemic risk culture from the top, e. g. by the leadership clearlycommunicating the Signatory’s Framework to staff;(2) allowing clear communication and challenge of decisions concerning systemic risk;(3) setting incentives and affording sufficient independence of staff involved in systemic riskassessment and mitigation to discourage excessive systemic-risk-taking and encourage anunbiased assessment of the systemic risks stemming from their models;(4) anonymous surveys find that staff are comfortable raising concerns about systemic risks, areaware of channels for doing so, and understand the Signatory’s Framework;(5) internal reporting channels are actively used and reports are acted upon appropriately;(6) annually informing workers of the Signatory’s whistleblower protection policy and makingsuch policy readily available to workers such as by publishing it on their website; and/or(7) not retaliating in any form, including any direct or indirect detrimental action such astermination, demotion, legal action, negative evaluations, or creation of hostile workenvironments, against any person publishing or providing information acquired in the contextof work-related activities performed for the Signatory to competent authorities aboutsystemic risks stemming from their models for which the person has reasonable grounds tobelieve its veracity. Commitment 9 Serious incident reporting LEGAL TEXT: Article 55(1), and recitals 114 and 115 AI Act Signatories commit to implementing appropriate processes and measures for keeping track of, documenting, and reporting to the AI Office and, as applicable, to national competent authorities, without undue delay relevant information about serious incidents along the entire model lifecycleand possible corrective measures to address them, as specified in the Measures of this Commitment. Further, Signatories commit to providing resourcing of such processes and measures appropriate forthe severity of the serious incident and the degree of involvement of their model. Measure 9.1 Methods for serious incident identification Signatories will consider the exemplary methods in Measure 3.5 to keep track of relevant informationabout serious incidents. Additionally, Signatories will:(1) review other sources of information, such as police and media reports, posts on social media, research papers, and incident databases; and(2) facilitate the reporting of relevant information about serious incidents by downstreammodifiers, downstream providers, users, and other third parties to:(a) the Signatory; or(b) the AI Office and, as applicable, national competent authorities; by informing such third parties of direct reporting channels, if available, without prejudice toany of their reporting obligations under Article 73 AI Act. Measure 9.2 Relevant information for serious incident tracking, documentation, andreporting Signatories will keep track of, document, and report to the AI Office and, as applicable, to nationalcompetent authorities, at least the following information to the best of their knowledge, redactedto the extent necessary to comply with other Union law applicable to such information:(1) the start and end dates of the serious incident, or best approximations thereof if the precisedates are unclear;(2) the resulting harm and the victim or affected group of the serious incident;(3) the chain of events that (directly or indirectly) led to the serious incident;(4) the model involved in the serious incident;(5) a description of material available setting out the model’s involvement in the serious incident;(6) what, if anything, the Signatory intends to do or has done in response to the serious incident;(7) what, if anything, the Signatory recommends the AI Office and, as applicable, nationalcompetent authorities to do in response to the serious incident;(8) a root cause analysis with a description of the model’s outputs that (directly or indirectly) ledto the serious incident and the factors that contributed to their generation, including theinputs used and any failures or circumventions of systemic risk mitigations; and(9) any patterns detected during post-market monitoring (pursuant to Measure 3.5) that canreasonably be assumed to be connected to the serious incident, such as individual oraggregate data on near misses. Signatories will investigate the causes and effects of serious incidents, including the informationwithin the preceding list, with a view to informing systemic risk assessment. If Signatories do not yethave certain relevant information from the preceding list, they will record that in their seriousincident reports. The level of detail in serious incident reports will be appropriate for the severity ofthe incident. Measure 9.3 Reporting timelines Signatories will provide the information in points (1) to (7) of Measure 9.2 in an initial report that issubmitted to the AI Office and, as applicable, to national competent authorities, at the followingpoints in time, save in exceptional circumstances, if the involvement of their model (directly orindirectly) led to:(1) a serious and irreversible disruption of the management or operation of criticalinfrastructure, or if the Signatories establish or suspect with reasonable likelihood such acausal relationship between their model and the disruption, not later than two days after the Signatories become aware of the involvement of their model in the incident;(2) a serious cybersecurity breach, including the (self-)exfiltration of model weights andcyberattacks, or if the Signatories establish or suspect with reasonable likelihood such acausal relationship between their model and the breach, not later than five days after the Signatories become aware of the involvement of their model in the incident;(3) a death of a person, or if the Signatories establish or suspect with reasonable likelihood sucha causal relationship between their model and the death, not later than 10 days after the Signatories become aware of the involvement of their model in the incident; and(4) serious harm to a person’s health (mental and/or physical), an infringement of obligationsunder Union law intended to protect fundamental rights, and/or serious harm to property orthe environment, or if the Signatories establish or suspect with reasonable likelihood such acausal relationship between their model and the harms or infringements, not later than 15days after the Signatories become aware of the involvement of their model in the incident. For unresolved serious incidents, Signatories will update the information in their initial report andadd further information required by Measure 9.2, as available, in an intermediate report that issubmitted to the AI Office and, as applicable, to national competent authorities, at least every fourweeks after the initial report. Signatories will submit a final report, covering all the information required by Measure 9.2, to the AIOffice and, as applicable, to national competent authorities, not later than 60 days after the seriousincident has been resolved. If multiple similar serious incidents occur within the reporting timelines, Signatories may include themin the report(s) of the first serious incident, while respecting the timelines for reporting for the firstserious incident. Measure 9.4 Retention period Signatories will keep documentation of all relevant information gathered in adhering to this Commitment for at least five years from the date of the documentation or the date of the seriousincident, whichever is later, without prejudice to Union law applicable to such information. Commitment 10 Additional documentation andtransparency LEGAL TEXT: Articles 53(1)(a) and 55(1) AI Act Signatories commit to documenting the implementation of this Chapter (as specified in Measure 10.1)and publish summarised versions of their Framework and Model Reports as necessary (as specifiedin Measure 10.2). Measure 10.1 Additional documentation Signatories will draw up and keep up-to-date the following information for the purpose of providingit to the AI Office upon request:(1) a detailed description of the model’s architecture;(2) a detailed description of how the model is integrated into AI systems, explaining howsoftware components build or feed into each other and integrate into the overall processing, insofar as the Signatory is aware of such information;(3) a detailed description of the model evaluations conducted pursuant to this Chapter, includingtheir results and strategies; and(4) a detailed description of the safety mitigations implemented (pursuant to Commitment 5). Documentation will be retained at least 10 years after the model has been placed on the market. Further, Signatories will keep track of the following information, to the extent it is not already coveredby the first paragraph, for the purpose of evidencing adherence to this Chapter to the AI Office uponrequest:(1) their processes, measures, and key decisions that form part of their systemic risk assessmentand mitigation; and(2) justifications for choices of a particular best practice, state-of-the-art, or other moreinnovative process or measure if a Signatory relies upon it for adherence to this Chapter. Signatories need not collect the information of the third paragraph in one medium or place but maycompile it upon the AI Office’s request. Measure 10.2 Public transparency If and insofar as necessary to assess and/or mitigate systemic risks, Signatories will publish (e. g. viatheir websites) a summarised version of their Framework and Model Report(s), and updates thereof(pursuant to Commitments 1 and 7), with removals to not undermine the effectiveness of safetyand/or security mitigations and to protect sensitive commercial information. For Model Reports, suchpublication will include high-level descriptions of the systemic risk assessment results and the safetyand security mitigations implemented. For Frameworks, such publication is not necessary if all of the Signatory’s models are similarly safe or safer models pursuant to Appendix 2.2. For Model Reports, such publication is not necessary if the model is a similarly safe or safer model pursuant to Appendix2.2.

--- DOCUMENT: EU_Trustworthy_AI_Ethics_Guidelines.pdf ---

INDEPENDENTH -L E GIGH EVEL XPERT ROUP ONA IRTIFICIAL NTELLIGENCESET UP BY THE EUROPEAN COMMISSIONE GTHICS UIDELINEST AIFOR RUSTWORTHYETHICSGUIDELINES FORTRUSTWORTHYAIHigh-Level Expert Group on Artificial Intelligence This document was written by the High-Level Expert Group on AI (AI HLEG). The members of the AI HLEGnamed in this document support the overall framework for Trustworthy AI put forward in these Guidelines, although they do not necessarily agree with every single statement in the document. The Trustworthy AI assessment list presented in Chapter III of this document will undergo a piloting phase bystakeholders to gather practical feedback. A revised version of the assessment list, taking into account thefeedback gathered through the piloting phase, will be presented to the European Commission in early 2020. The AI HLEG is an independent expert group that was set up by the European Commission in June 2018. Contact Nathalie Smuha AI HLEG Coordinator E-mail CNECT-HLG-AI@ec. europa. eu European Commission B-1049 Brussels Document made public on 8 April 2019. A first draft of this document was released on 18 December 2018 and was subject to an open consultation whichgenerated feedback from more than 500 contributors. We wish to explicitly and warmly thank all those who contributedtheir feedback on the document's first draft, which was considered in the preparation of this revised version. Neither the European Commission nor any person acting on behalf of the Commission is responsible for the use whichmight be made of the following information. The contents of this working document are the sole responsibility of the High-Level Expert Group on Artificial Intelligence (AI HLEG). Although Commission staff facilitated the preparation of the Guidelines, the views expressed in this document reflect the opinion of the AI HLEG and may not in any circumstances beregarded as reflecting an official position of the European Commission. More information on the High-Level Expert Group on Artificial Intelligence is available online ( europa. eu/digital-single-market/en/high-level-expert-group-artificial-intelligence). The reuse policy of European Commission documents is regulated by Decision 2011/833/EU (OJ L 330, 14.12.2011, p.39). For any use or reproduction of photos or other material that is not under the EU copyright, permission must be soughtdirectly from the copyright holders. TABLE OF CONTENTSEXECUTIVE SUMMARY 2A. INTRODUCTION 4B. A FRAMEWORK FOR TRUSTWORTHY AI 6I. Chapter I: Foundations of Trustworthy AI 9II. Chapter II: Realising Trustworthy AI 141. Requirements of Trustworthy AI 142. Technical and non-technical methods to realise Trustworthy AI 22III. Chapter III: Assessing Trustworthy AI 24C. EXAMPLES OF OPPORTUNITIES AND CRITICAL CONCERNS RAISED BY AI 32D. CONCLUSION 35GLOSSARY 36EXECUTIVE SUMMARYThe aim of the Guidelines is to promote Trustworthy AI. Trustworthy AI has three components, which should bemet throughout the system's entire life cycle: (1) it should be lawful, complying with all applicable laws andregulations (2) it should be ethical, ensuring adherence to ethical principles and values and (3) it should be robust, both from a technical and social perspective since, even with good intentions, AI systems can cause unintentionalharm. Each component in itself is necessary but not sufficient for the achievement of Trustworthy AI. Ideally, allthree components work in harmony and overlap in their operation. If, in practice, tensions arise between thesecomponents, society should endeavour to align them. These Guidelines set out a framework for achieving Trustworthy AI. The framework does not explicitly deal with Trustworthy AI’s first component (lawful AI).1 Instead, it aims to offer guidance on the second and thirdcomponents: fostering and securing ethical and robust AI. Addressed to all stakeholders, these Guidelines seek to gobeyond a list of ethical principles, by providing guidance on how such principles can be operationalised in socio-technical systems. Guidance is provided in three layers of abstraction, from the most abstract in Chapter I to themost concrete in Chapter III, closing with examples of opportunities and critical concerns raised by AI systems. I. Based on an approach founded on fundamental rights, Chapter I identifies the ethical principles and theircorrelated values that must be respected in the development, deployment and use of AI systems. Key guidance derived from Chapter I: Develop, deploy and use AI systems in a way that adheres to the ethical principles of: respect for humanautonomy, prevention of harm, fairness and explicability. Acknowledge and address the potential tensionsbetween these principles. Pay particular attention to situations involving more vulnerable groups such as children, persons withdisabilities and others that have historically been disadvantaged or are at risk of exclusion, and to situationswhich are characterised by asymmetries of power or information, such as between employers and workers, or between businesses and consumers.2 Acknowledge that, while bringing substantial benefits to individuals and society, AI systems also posecertain risks and may have a negative impact, including impacts which may be difficult to anticipate, identify or measure (e. g. on democracy, the rule of law and distributive justice, or on the human minditself.) Adopt adequate measures to mitigate these risks when appropriate, and proportionately to themagnitude of the risk. II. Drawing upon Chapter I, Chapter II provides guidance on how Trustworthy AI can be realised, by listing sevenrequirements that AI systems should meet. Both technical and non-technical methods can be used for theirimplementation. Key guidance derived from Chapter II: Ensure that the development, deployment and use of AI systems meets the seven key requirements for Trustworthy AI: (1) human agency and oversight, (2) technical robustness and safety, (3) privacy and datagovernance, (4) transparency, (5) diversity, non-discrimination and fairness, (6) environmental and societalwell-being and (7) accountability. Consider technical and non-technical methods to ensure the implementation of those requirements.1 All normative statements in this document aim to reflect guidance towards achieving the second and third component oftrustworthy AI (ethical and robust AI). These statements are hence not meant to provide legal advice or to offer guidance oncompliance with applicable laws, though it is acknowledged that many of these statements are to some extent already reflectedin existing laws. In this regard, see §21 and following.2 See articles 24 to 27 of the Charter of Fundamental Rights of the EU (EU Charter), dealing with the rights of the child and theelderly, the integration of persons with disabilities and workers’ rights. See also article 38 dealing with consumer protection. Foster research and innovation to help assess AI systems and to further the achievement of therequirements; disseminate results and open questions to the wider public, and systematically train a newgeneration of experts in AI ethics. Communicate, in a clear and proactive manner, information to stakeholders about the AI system’scapabilities and limitations, enabling realistic expectation setting, and about the manner in which therequirements are implemented. Be transparent about the fact that they are dealing with an AI system. Facilitate the traceability and auditability of AI systems, particularly in critical contexts or situations. Involve stakeholders throughout the AI system’s life cycle. Foster training and education so that allstakeholders are aware of and trained in Trustworthy AI. Be mindful that there might be fundamental tensions between different principles and requirements. Continuously identify, evaluate, document and communicate these trade-offs and their solutions. III. Chapter III provides a concrete and non-exhaustive Trustworthy AI assessment list aimed at operationalisingthe key requirements set out in Chapter II. This assessment list will need to be tailored to the specific use caseof the AI system.3Key guidance derived from Chapter III: Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems, and adapt it to thespecific use case in which the system is being applied. Keep in mind that such an assessment list will never be exhaustive. Ensuring Trustworthy AI is not aboutticking boxes, but about continuously identifying and implementing requirements, evaluating solutions, ensuring improved outcomes throughout the AI system’s lifecycle, and involving stakeholders in this. A final section of the document aims to concretise some of the issues touched upon throughout the framework, byoffering examples of beneficial opportunities that should be pursued, and critical concerns raised by AI systems thatshould be carefully considered. While these Guidelines aim to offer guidance for AI applications in general by building a horizontal foundation toachieve Trustworthy AI, different situations raise different challenges. It should therefore be explored whether, inaddition to this horizontal framework, a sectorial approach is needed, given the context-specificity of AI systems. These Guidelines do not intend to substitute any form of current or future policymaking or regulation, nor do theyaim to deter the introduction thereof. They should be seen as a living document to be reviewed and updated overtime to ensure their continuous relevance as the technology, our social environments, and our knowledge evolve. This document is a starting point for the discussion about “Trustworthy AI for Europe”.4Beyond Europe, the Guidelines also aim to foster research, reflection and discussion on an ethical framework for AIsystems at a global level.3 In line with the scope of the framework, this assessment list does not provide any advice on ensuring legal compliance (lawful AI), but limits itself to offering guidance on meeting the second and third components of trustworthy AI (ethical and robust AI).4 This ideal is intended to apply to AI systems developed, deployed and used in the Member States of the European Union (EU), aswell as to systems developed or produced elsewhere but deployed and used in the EU. When referring to "Europe" in thisdocument, we mean this to encompass the EU Member States. However, these Guidelines also aspire to be relevant outside the EU. In this regard, it can also be noted that both Norway and Switzerland are part of the Coordinated Plan on AI agreed andpublished in December 2018 by the Commission and Member States. A. INTRODUCTIONIn its Communication of 25 April 2018 and 7 December 2018, the European Commission set out its vision forartificial intelligence (AI), which supports “ethical, secure and cutting-edge AI made in Europe”.5 Three pillarsunderpin the Commission’s vision: (i) increasing public and private investments in AI to boost its uptake, (ii)preparing for socio-economic changes, and (iii) ensuring an appropriate ethical and legal framework to strengthen European values. To support the implementation of this vision, the Commission established the High-Level Expert Group on Artificial Intelligence (AI HLEG), an independent group mandated with the drafting of two deliverables: (1) AI Ethics Guidelines and (2) Policy and Investment Recommendations. This document contains the AI Ethics Guidelines, which have been revised following further deliberation by our Group in light of feedback received from the public consultation on the draft published on 18 December 2018. Itbuilds on the work of the European Group on Ethics in Science and New Technologies6 and takes inspiration fromother similar efforts.7Over the past months, the 52 of us met, discussed and interacted, committed to the European motto: united indiversity. We believe that AI has the potential to significantly transform society. AI is not an end in itself, but rathera promising means to increase human flourishing, thereby enhancing individual and societal well-being and thecommon good, as well as bringing progress and innovation. In particular, AI systems can help to facilitate theachievement of the UN’s Sustainable Development Goals, such as promoting gender balance and tackling climatechange, rationalising our use of natural resources, enhancing our health, mobility and production processes, andsupporting how we monitor progress against sustainability and social cohesion indicators. To do this, AI systems8 need to be human-centric, resting on a commitment to their use in the service of humanityand the common good, with the goal of improving human welfare and freedom. While offering great opportunities, AI systems also give rise to certain risks that must be handled appropriately and proportionately. We now have animportant window of opportunity to shape their development. We want to ensure that we can trust the socio-technical environments in which they are embedded. We also want producers of AI systems to get a competitiveadvantage by embedding Trustworthy AI in their products and services. This entails seeking to maximise thebenefits of AI systems while at the same time preventing and minimising their risks. In a context of rapid technological change, we believe it is essential that trust remains the bedrock of societies, communities, economies and sustainable development. We therefore identify Trustworthy AI as our foundationalambition, since human beings and communities will only be able to have confidence in the technology’sdevelopment and its applications when a clear and comprehensive framework for achieving its trustworthiness is inplace. This is the path that we believe Europe should follow to become the home and leader of cutting-edge and ethicaltechnology. It is through Trustworthy AI that we, as European citizens, will seek to reap its benefits in a way that isaligned with our foundational values of respect for human rights, democracy and the rule of law. Trustworthy AITrustworthiness is a prerequisite for people and societies to develop, deploy and use AI systems. Without AIsystems – and the human beings behind them – being demonstrably worthy of trust, unwanted consequences mayensue and their uptake might be hindered, preventing the realisation of the potentially vast social and economic5 COM(2018)237 and COM(2018)795. Note that the term “made in Europe” is used throughout the Commission’s communication. The scope of these Guidelines however aims to encompass not only those AI systems made in Europe, but also those developedelsewhere and deployed or used in Europe. Throughout this document, we hence aim to promote trustworthy AI “for” Europe.6 The European Group on Ethics in Science and New Technologies (EGE) is an advisory group of the Commission.7 See Section 3.3 of COM(2018)237.8 The Glossary at the end of this document provides a definition of AI systems for the purpose of this document. This definition isfurther elaborated on in a dedicated document prepared by the AI HLEG that accompanies these Guidelines, titled "A definitionof AI: Main capabilities and scientific disciplines". benefits that they can bring. To help Europe realise those benefits, our vision is to ensure and scale Trustworthy AI. Trust in the development, deployment and use of AI systems concerns not only the technology’s inherentproperties, but also the qualities of the socio-technical systems involving AI applications.9 Analogous to questions of(loss of) trust in aviation, nuclear power or food safety, it is not simply components of the AI system but the systemin its overall context that may or may not engender trust. Striving towards Trustworthy AI hence concerns not onlythe trustworthiness of the AI system itself, but requires a holistic and systemic approach, encompassing thetrustworthiness of all actors and processes that are part of the system’s socio-technical context throughout itsentire life cycle. Trustworthy AI has three components, which should be met throughout the system's entire life cycle:1. it should be lawful, complying with all applicable laws and regulations;2. it should be ethical, ensuring adherence to ethical principles and values; and3. it should be robust, both from a technical and social perspective, since, even with good intentions, AIsystems can cause unintentional harm. Each of these three components is necessary but not sufficient in itself to achieve Trustworthy AI.10 Ideally, all threework in harmony and overlap in their operation. In practice, however, there may be tensions between theseelements (e. g. at times the scope and content of existing law might be out of step with ethical norms). It is ourindividual and collective responsibility as a society to work towards ensuring that all three components help tosecure Trustworthy AI.11A trustworthy approach is key to enabling “responsible competitiveness”, by providing the foundation upon whichall those affected by AI systems can trust that their design, development and use are lawful, ethical and robust. These Guidelines are intended to foster responsible and sustainable AI innovation in Europe. They seek to makeethics a core pillar for developing a unique approach to AI, one that aims to benefit, empower and protect bothindividual human flourishing and the common good of society. We believe that this will enable Europe to positionitself as a global leader in cutting-edge AI worthy of our individual and collective trust. Only by ensuringtrustworthiness will European individuals fully reap AI systems’ benefits, secure in the knowledge that measures arein place to safeguard against their potential risks. Just as the use of AI systems does not stop at national borders, neither does their impact. Global solutions aretherefore required for the global opportunities and challenges that AI systems bring forth. We therefore encourageall stakeholders to work towards a global framework for Trustworthy AI, building international consensus whilepromoting and upholding our fundamental rights-based approach. Audience and Scope These guidelines are addressed to all AI stakeholders designing, developing, deploying, implementing, using or beingaffected by AI, including but not limited to companies, organisations, researchers, public services, governmentagencies, institutions, civil society organisations, individuals, workers and consumers. Stakeholders committedtowards achieving Trustworthy AI can voluntarily opt to use these Guidelines as a method to operationalise theircommitment, in particular by using the practical assessment list of Chapter III when developing, deploying or using AI systems. This assessment list can also complement – and hence be incorporated in – existing assessmentprocesses. The Guidelines aim to provide guidance for AI applications in general, building a horizontal foundation to achieve Trustworthy AI. However, different situations raise different challenges. AI music recommendation systems do not9 These systems comprise humans, state actors, corporations, infrastructure, software, protocols, standards, governance, existinglaws, oversight mechanisms, incentive structures, auditing procedures, best practices reporting and others.10 This does not exclude the fact that additional conditions may be(come) necessary.11 This also means that the legislature or policy-makers may need to review the adequacy of existing law where these might be outof step with ethical principles. raise the same ethical concerns as AI systems proposing critical medical treatments. Likewise, differentopportunities and challenges arise from AI systems used in the context of business-to-consumer, business-to-business, employer-to-employee and public-to-citizen relationships, or more generally, in different sectors or usecases. Given the context-specificity of AI systems, the implementation of these Guidelines needs to be adapted tothe particular AI-application. Moreover, the necessity of an additional sectorial approach, to complement the moregeneral horizontal framework proposed in this document, should be explored. To gain a better understanding of how this guidance can be implemented at a horizontal level, and of those mattersthat require a sectorial approach, we invite all stakeholders to pilot the Trustworthy AI assessment list (Chapter III)that operationalises this framework and to provide us feedback. Based on the feedback gathered through thispiloting phase, we will revise the assessment list of these Guidelines by early 2020. The piloting phase will belaunched by the summer of 2019 and last until the end of the year. All interested stakeholders will be able toparticipate by indicating their interest through the European AI Alliance. B. A FRAMEWORK FOR TRUSTWORTHY AIThese Guidelines articulate a framework for achieving Trustworthy AI based on fundamental rights as enshrined inthe Charter of Fundamental Rights of the European Union (EU Charter), and in relevant international human rightslaw. Below, we briefly touch upon Trustworthy AI’s three components. Lawful AIAI systems do not operate in a lawless world. A number of legally binding rules at European, national andinternational level already apply or are relevant to the development, deployment and use of AI systems today. Legalsources include, but are not limited to: EU primary law (the Treaties of the European Union and its Charter of Fundamental Rights), EU secondary law (such as the General Data Protection Regulation, the Product Liability Directive, the Regulation on the Free Flow of Non-Personal Data, anti-discrimination Directives, consumer law and Safety and Health at Work Directives), the UN Human Rights treaties and the Council of Europe conventions (such asthe European Convention on Human Rights), and numerous EU Member State laws. Besides horizontally applicablerules, various domain-specific rules exist that apply to particular AI applications (such as for instance the Medical Device Regulation in the healthcare sector). The law provides both positive and negative obligations, which means that it should not only be interpreted withreference to what cannot be done, but also with reference to what should be done and what may be done. The lawnot only prohibits certain actions but also enables others. In this regard, it can be noted that the EU Charter containsarticles on the ‘freedom to conduct a business’ and the ’freedom of the arts and sciences’, alongside articlesaddressing areas that we are more familiar with when looking to ensure AI’s trustworthiness, such as for instancedata protection and non-discrimination. The Guidelines do not explicitly deal with the first component of Trustworthy AI (lawful AI), but instead aim to offerguidance on fostering and securing the second and third components (ethical and robust AI). While the two latterare to a certain extent often already reflected in existing laws, their full realisation may go beyond existing legalobligations. Nothing in this document shall be construed or interpreted as providing legal advice or guidance concerning howcompliance with any applicable existing legal norms and requirements can be achieved. Nothing in this documentshall create legal rights nor impose legal obligations towards third parties. We however recall that it is the duty ofany natural or legal person to comply with laws – whether applicable today or adopted in the future according tothe development of AI. These Guidelines proceed on the assumption that all legal rights and obligations that applyto the processes and activities involved in developing, deploying and using AI systems remain mandatory andmust be duly observed. Ethical AIAchieving Trustworthy AI requires not only compliance with the law, which is but one of its three components. Lawsare not always up to speed with technological developments, can at times be out of step with ethical norms or maysimply not be well suited to addressing certain issues. For AI systems to be trustworthy, they should hence also beethical, ensuring alignment with ethical norms. Robust AIEven if an ethical purpose is ensured, individuals and society must also be confident that AI systems will not causeany unintentional harm. Such systems should perform in a safe, secure and reliable manner, and safeguards shouldbe foreseen to prevent any unintended adverse impacts. It is therefore important to ensure that AI systems arerobust. This is needed both from a technical perspective (ensuring the system’s technical robustness as appropriatein a given context, such as the application domain or life cycle phase), and from a social perspective (in dueconsideration of the context and environment in which the system operates). Ethical and robust AI are hence closely intertwined and complement each other. The principles put forward in Chapter I, and the requirements derived from these principles in Chapter II, address both components. The framework The Guidance in this document is provided in three chapters, from most abstract in Chapter I to most concrete in Chapter III: Chapter I – Foundations of Trustworthy AI: sets out the foundations of Trustworthy AI by laying out itsfundamental-rights12 based approach. It identifies and describes the ethical principles that must beadhered to in order to ensure ethical and robust AI. Chapter II – Realising Trustworthy AI: translates these ethical principles into seven key requirements that AI systems should implement and meet throughout their entire life cycle. In addition, it offers bothtechnical and non-technical methods that can be used for their implementation. Chapter III – Assessing Trustworthy AI: sets out a concrete and non-exhaustive Trustworthy AI assessmentlist to operationalise the requirements of Chapter II, offering AI practitioners practical guidance. Thisassessment should be tailored to the particular system's application. The document’s final section lists examples of beneficial opportunities and critical concerns raised by AI systems, which should serve to stimulate further debate. The Guidelines’ structure is illustrated in Figure 1 below12 Fundamental rights lie at the foundation of both international and EU human rights law and underpin the legally enforceablerights guaranteed by the EU Treaties and the EU Charter. Being legally binding, compliance with fundamental rights hence fallsunder trustworthy AI's first component (lawful AI). Fundamental rights can however also be understood as reflecting specialmoral entitlements of all individuals arising by virtue of their humanity, regardless of their legally binding status. In that sense, they hence also form part of the second component of trustworthy AI (ethical AI). Figure 1: The Guidelines as a framework for Trustworthy AII. Chapter I: Foundations of Trustworthy AIThis Chapter sets out the foundations of Trustworthy AI, grounded in fundamental rights and reflected by fourethical principles that should be adhered to in order to ensure ethical and robust AI. It draws heavily on the field ofethics. AI ethics is a sub-field of applied ethics, focusing on the ethical issues raised by the development, deployment anduse of AI. Its central concern is to identify how AI can advance or raise concerns to the good life of individuals, whether in terms of quality of life, or human autonomy and freedom necessary for a democratic society. Ethical reflection on AI technology can serve multiple purposes. First, it can stimulate reflection on the need toprotect individuals and groups at the most basic level. Second, it can stimulate new kinds of innovations that seek tofoster ethical values, such as those helping to achieve the UN Sustainable Development Goals13, which are firmlyembedded in the forthcoming EU Agenda 2030.14 While this document mostly concerns itself with the first purposementioned, the importance that ethics could have in the second should not be underestimated. Trustworthy AI canimprove individual flourishing and collective wellbeing by generating prosperity, value creation and wealthmaximization. It can contribute to achieving a fair society, by helping to increase citizens’ health and well-being inways that foster equality in the distribution of economic, social and political opportunity. It is therefore imperative that we understand how to best support AI development, deployment and use to ensurethat everyone can thrive in an AI-based world, and to build a better future while at the same time being globallycompetitive. As with any powerful technology, the use of AI systems in our society raises several ethical challenges, for instance relating to their impact on people and society, decision-making capabilities and safety. If we areincreasingly going to use the assistance of or delegate decisions to AI systems, we need to make sure these systemsare fair in their impact on people’s lives, that they are in line with values that should not be compromised and ableto act accordingly, and that suitable accountability processes can ensure this. Europe needs to define what normative vision of an AI-immersed future it wants to realise, and understand whichnotion of AI should be studied, developed, deployed and used in Europe to achieve this vision. With this document, we intend to contribute to this effort by introducing the notion of Trustworthy AI, which we believe is the right wayto build a future with AI. A future where democracy, the rule of law and fundamental rights underpin AI systems andwhere such systems continuously improve and defend democratic culture will also enable an environment whereinnovation and responsible competitiveness can thrive. A domain-specific ethics code – however consistent, developed and fine-grained future versions of it may be – cannever function as a substitute for ethical reasoning itself, which must always remain sensitive to contextual detailsthat cannot be captured in general Guidelines. Beyond developing a set of rules, ensuring Trustworthy AI requires usto build and maintain an ethical culture and mind-set through public debate, education and practical learning.1. Fundamental rights as moral and legal entitlements We believe in an approach to AI ethics based on the fundamental rights enshrined in the EU Treaties,15 the EUCharter and international human rights law.16 Respect for fundamental rights, within a framework of democracy andthe rule of law, provides the most promising foundations for identifying abstract ethical principles and values, whichcan be operationalised in the context of AI. The EU Treaties and the EU Charter prescribe a series of fundamental rights that EU member states and EUinstitutions are legally obliged to respect when implementing EU law. These rights are described in the EU Charter13 europa. eu/commission/publications/reflection-paper-towards-sustainable-europe-2030_en14 un. org/? menu=130015 The EU is based on a constitutional commitment to protect the fundamental and indivisible rights of human beings, to ensurerespect for the rule of law, to foster democratic freedom and promote the common good. These rights are reflected in Articles 2and 3 of the Treaty on European Union, and in the Charter of Fundamental Rights of the EU.16 Other legal instruments reflect and provide further specification of these commitments, such as for instance the Council of Europe’s European Social Charter or specific legislation such as the EU’s General Data Protection Regulation. by reference to dignity, freedoms, equality and solidarity, citizens’ rights and justice. The common foundation thatunites these rights can be understood as rooted in respect for human dignity – thereby reflecting what we describeas a “human-centric approach” in which the human being enjoys a unique and inalienable moral status of primacy inthe civil, political, economic and social fields.17While the rights set out in the EU Charter are legally binding,18 it is important to recognise that fundamental rightsdo not provide comprehensive legal protection in every case. For the EU Charter, for instance, it is important tounderline that its field of application is limited to areas of EU law. International human rights law and in particularthe European Convention on Human Rights are legally binding on EU Member States, including in areas that falloutside the scope of EU law. At the same time, fundamental rights are also bestowed on individuals and (to acertain degree) groups by virtue of their moral status as human beings, independently of their legal force. Understood as legally enforceable rights, fundamental rights therefore fall under the first component of Trustworthy AI (lawful AI), which safeguards compliance with the law. Understood as the rights of everyone, rootedin the inherent moral status of human beings, they also underpin the second component of Trustworthy AI (ethical AI), dealing with ethical norms that are not necessarily legally binding yet crucial to ensure trustworthiness. Sincethis document does not aim to offer guidance on the former component, for the purpose of these non-bindingguidelines, references to fundamental rights reflect the latter component.2. From fundamental rights to ethical principles2.1 Fundamental rights as a basis for Trustworthy AIAmong the comprehensive set of indivisible rights set out in international human rights law, the EU Treaties and the EU Charter, the below families of fundamental rights are particularly apt to cover AI systems. Many of these rightsare, in specified circumstances, legally enforceable in the EU so that compliance with their terms is legallyobligatory. But even after compliance with legally enforceable fundamental rights has been achieved, ethicalreflection can help us understand how the development, deployment and use of AI systems may implicatefundamental rights and their underlying values, and can help provide more fine-grained guidance when seeking toidentify what we should do rather than what we (currently) can do with technology. Respect for human dignity. Human dignity encompasses the idea that every human being possesses an “intrinsicworth”, which should never be diminished, compromised or repressed by others – nor by new technologies like AIsystems.19 In this context, respect for human dignity entails that all people are treated with respect due to them asmoral subjects, rather than merely as objects to be sifted, sorted, scored, herded, conditioned or manipulated. AIsystems should hence be developed in a manner that respects, serves and protects humans’ physical and mentalintegrity, personal and cultural sense of identity, and satisfaction of their essential needs.20Freedom of the individual. Human beings should remain free to make life decisions for themselves. This entailsfreedom from sovereign intrusion, but also requires intervention from government and non-governmentalorganisations to ensure that individuals or people at risk of exclusion have equal access to AI’s benefits andopportunities. In an AI context, freedom of the individual for instance requires mitigation of (in)direct illegitimatecoercion, threats to mental autonomy and mental health, unjustified surveillance, deception and unfairmanipulation. In fact, freedom of the individual means a commitment to enabling individuals to wield even highercontrol over their lives, including (among other rights) protection of the freedom to conduct a business, thefreedom of the arts and science, freedom of expression, the right to private life and privacy, and freedom of17 It should be noted that a commitment to human-centric AI and its anchoring in fundamental rights requires collective societaland constitutional foundations in which individual freedom and respect for human dignity is both practically possible andmeaningful, rather than implying an unduly individualistic account of the human.18 Pursuant to Article 51 of the Charter, it applies to EU Institutions and to EU member states when implementing EU law.19 C. Mc Crudden, Human Dignity and Judicial Interpretation of Human Rights, EJIL, 19(4), 2008.20 For an understanding of “human dignity” along these lines see E. Hilgendorf, Problem Areas in the Dignity Debate and the Ensemble Theory of Human Dignity, in: D. Grimm, A. Kemmerer, C. Möllers (eds.), Human Dignity in Context. Explorations of a Contested Concept, 2018, pp. 325 ff. assembly and association. Respect for democracy, justice and the rule of law. All governmental power in constitutional democracies must belegally authorised and limited by law. AI systems should serve to maintain and foster democratic processes andrespect the plurality of values and life choices of individuals. AI systems must not undermine democratic processes, human deliberation or democratic voting systems. AI systems must also embed a commitment to ensure that theydo not operate in ways that undermine the foundational commitments upon which the rule of law is founded, mandatory laws and regulation, and to ensure due process and equality before the law. Equality, non-discrimination and solidarityincluding the rights of persons at risk of exclusion. Equal respect for themoral worth and dignity of all human beings must be ensured. This goes beyond non-discrimination, which toleratesthe drawing of distinctions between dissimilar situations based on objective justifications. In an AI context, equalityentails that the system’s operations cannot generate unfairly biased outputs (e. g. the data used to train AI systemsshould be as inclusive as possible, representing different population groups). This also requires adequate respect forpotentially vulnerable persons and groups,21 such as workers, women, persons with disabilities, ethnic minorities, children, consumers or others at risk of exclusion. Citizens’ rights. Citizens benefit from a wide array of rights, including the right to vote, the right to goodadministration or access to public documents, and the right to petition the administration. AI systems offersubstantial potential to improve the scale and efficiency of government in the provision of public goods and servicesto society. At the same time, citizens’ rights could also be negatively impacted by AI systems and should besafeguarded. When the term “citizens’ rights” is used here, this is not to deny or neglect the rights of third-countrynationals and irregular (or illegal) persons in the EU who also have rights under international law, and – therefore –in the area of AI systems.2.2 Ethical Principles in the Context of AI Systems22Many public, private, and civil organizations have drawn inspiration from fundamental rights to produce ethicalframeworks for AI systems.23 In the EU, the European Group on Ethics in Science and New Technologies (“EGE”)proposed a set of 9 basic principles, based on the fundamental values laid down in the EU Treaties and Charter.24We build further on this work, recognising most of the principles hitherto propounded by various groups, whileclarifying the ends that all principles seek to nurture and support. These ethical principles can inspire new andspecific regulatory instruments, can help interpreting fundamental rights as our socio-technical environmentevolves over time, and can guide the rationale for AI systems’ development, deployment and use – adaptingdynamically as society itself evolves. AI systems should improve individual and collective wellbeing. This section lists four ethical principles, rooted infundamental rights, which must be respected in order to ensure that AI systems are developed, deployed and usedin a trustworthy manner. They are specified as ethical imperatives, such that AI practitioners should always strive toadhere to them. Without imposing a hierarchy, we list the principles here below in manner that mirrors the order ofappearance of the fundamental rights upon which they are based in the EU Charter.2521 For a description of the term as used throughout this document, see the Glossary.22 These principles also apply to the development, deployment and use of other technologies, and hence are not specific to AIsystems. In what follows, we have aimed to set out their relevance specifically in an AI-related context.23 Reliance on fundamental rights also helps to limit regulatory uncertainty as it can build on the basis of decades of practice offundamental rights protection in the EU, thereby offering clarity, readability and foreseeability.24 More recently, the AI4People’s taskforce has surveyed the aforementioned EGE principles as well as 36 other ethical principlesput forward to date and subsumed them under four overarching principles: L. Floridi, J. Cowls, M. Beltrametti, R. Chatila, P. Chazerand, V. Dignum, C. Luetge, R. Madelin, U. Pagallo, F. Rossi, B. Schafer, P. Valcke, E. J. M. Vayena (2018), "AI4People —An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations”, Minds and Machines 28(4):689-707.25 Respect for human autonomy is strongly associated with the right to human dignity and liberty (reflected in Articles 1 and 6 ofthe Charter). The prevention of harm is strongly linked to the protection of physical or mental integrity (reflected in Article 3). These are the principles of:(i) Respect for human autonomy(ii) Prevention of harm(iii) Fairness(iv) Explicability Many of these are to a large extent already reflected in existing legal requirements for which mandatory complianceis required and hence also fall within the scope of lawful AI, which is Trustworthy AI’s first component.26 Yet, as setout above, while many legal obligations reflect ethical principles, adherence to ethical principles goes beyond formalcompliance with existing laws.27 The principle of respect for human autonomy The fundamental rights upon which the EU is founded are directed towards ensuring respect for the freedom andautonomy of human beings. Humans interacting with AI systems must be able to keep full and effective self-determination over themselves, and be able to partake in the democratic process. AI systems should notunjustifiably subordinate, coerce, deceive, manipulate, condition or herd humans. Instead, they should be designedto augment, complement and empower human cognitive, social and cultural skills. The allocation of functionsbetween humans and AI systems should follow human-centric design principles and leave meaningful opportunityfor human choice. This means securing human oversight28 over work processes in AI systems. AI systems may alsofundamentally change the work sphere. It should support humans in the working environment, and aim for thecreation of meaningful work. The principle of prevention of harm AI systems should neither cause nor exacerbate harm29 or otherwise adversely affect human beings.30 This entailsthe protection of human dignity as well as mental and physical integrity. AI systems and the environments in whichthey operate must be safe and secure. They must be technically robust and it should be ensured that they are notopen to malicious use. Vulnerable persons should receive greater attention and be included in the development, deployment and use of AI systems. Particular attention must also be paid to situations where AI systems can causeor exacerbate adverse impacts due to asymmetries of power or information, such as between employers andemployees, businesses and consumers or governments and citizens. Preventing harm also entails consideration ofthe natural environment and all living beings. The principle of fairness The development, deployment and use of AI systems must be fair. While we acknowledge that there are manydifferent interpretations of fairness, we believe that fairness has both a substantive and a procedural dimension. The substantive dimension implies a commitment to: ensuring equal and just distribution of both benefits and costs, and ensuring that individuals and groups are free from unfair bias, discrimination and stigmatisation. If unfair biasescan be avoided, AI systems could even increase societal fairness. Equal opportunity in terms of access to education, goods, services and technology should also be fostered. Moreover, the use of AI systems should never lead topeople being deceived or unjustifiably impaired in their freedom of choice. Additionally, fairness implies that AIpractitioners should respect the principle of proportionality between means and ends, and consider carefully how to Fairness is closely linked to the rights to Non-discrimination, Solidarity and Justice (reflected in Articles 21 and following). Explicability and Responsibility are closely linked to the rights relating to Justice (as reflected in Article 47).26 Think for instance of the GDPR or EU consumer protection regulations.27 For further reading on this subject, see for instance L. Floridi, Soft Ethics and the Governance of the Digital, Philosophy &Technology, March 2018, Volume 31, Issue 1, pp 1–8.28 The concept of human oversight is further developed as one of the key requirements set out in Chapter II here below.29 Harms can be individual or collective, and can include intangible harm to social, cultural and political environments.30 This also encompasses the way of living of individuals and social groups, avoiding for instance cultural harm. balance competing interests and objectives.31 The procedural dimension of fairness entails the ability to contest andseek effective redress against decisions made by AI systems and by the humans operating them.32 In order to do so, the entity accountable for the decision must be identifiable, and the decision-making processes should beexplicable. The principle of explicability Explicability is crucial for building and maintaining users’ trust in AI systems. This means that processes need to betransparent, the capabilities and purpose of AI systems openly communicated, and decisions – to the extentpossible – explainable to those directly and indirectly affected. Without such information, a decision cannot be dulycontested. An explanation as to why a model has generated a particular output or decision (and what combinationof input factors contributed to that) is not always possible. These cases are referred to as ‘black box’ algorithms andrequire special attention. In those circumstances, other explicability measures (e. g. traceability, auditability andtransparent communication on system capabilities) may be required, provided that the system as a whole respectsfundamental rights. The degree to which explicability is needed is highly dependent on the context and the severityof the consequences if that output is erroneous or otherwise inaccurate.332 .3 Tensions between the principles Tensions may arise between the above principles, for which there is no fixed solution. In line with the EUfundamental commitment to democratic engagement, due process and open political participation, methods ofaccountable deliberation to deal with such tensions should be established. For instance, in various applicationdomains, the principle of prevention of harm and the principle of human autonomy may be in conflict. Consider asan example the use of AI systems for ‘predictive policing’, which may help to reduce crime, but in ways that entailsurveillance activities that impinge on individual liberty and privacy. Furthermore, AI systems’ overall benefitsshould substantially exceed the foreseeable individual risks. While the above principles certainly offer guidancetowards solutions, they remain abstract ethical prescriptions. AI practitioners can hence not be expected to find theright solution based on the principles above, yet they should approach ethical dilemmas and trade-offs viareasoned, evidence-based reflection rather than intuition or random discretion. There may be situations, however, where no ethically acceptable trade-offs can be identified. Certain fundamentalrights and correlated principles are absolute and cannot be subject to a balancing exercise (e. g. human dignity). Key guidance derived from Chapter I: Develop, deploy and use AI systems in a way that adheres to the ethical principles of: respect for humanautonomy, prevention of harm, fairness and explicability. Acknowledge and address the potential tensionsbetween these principles. Pay particular attention to situations involving more vulnerable groups such as children, persons withdisabilities and others that have historically been disadvantaged or are at risk of exclusion, and to situationswhich are characterised by asymmetries of power or information, such as between employers and workers, orbetween businesses and consumers.3431 This is relates to the principle of proportionality (reflected in the maxim that one should not ‘use a sledge hammer to crack anut’). Measures taken to achieve an end (e. g. the data extraction measures implemented to realise the AI optimisation function)should be limited to what is strictly necessary. It also entails that when several measures compete for the satisfaction of an end, preference should be given to the one that is least adverse to fundamental rights and ethical norms (e. g. AI developers shouldalways prefer public sector data to personal data). Reference can also be made to the proportionality between user anddeployer, considering the rights of companies (including intellectual property and confidentiality) on the one hand, and the rightsof the user on the other.32 Including by using their right of association and to join a trade union in a working environment, as provided for by Article 12 ofthe EU Charter of fundamental rights.33 For example, little ethical concern may flow from inaccurate shopping recommendations generated by an AI system, in contrastto AI systems that evaluate whether an individual convicted of a criminal offence should be released on parole.34 See articles 24 to 27 of the EU Charter, dealing with the rights of the child and the elderly, the integration of persons withdisabilities and workers’ rights. See also article 38 dealing with consumer protection. Acknowledge that, while bringing substantial benefits to individuals and society, AI systems also pose certainrisks and may have a negative impact, including impacts which may be difficult to anticipate, identify ormeasure (e. g. on democracy, the rule of law and distributive justice, or on the human mind itself.) Adoptadequate measures to mitigate these risks when appropriate, and proportionately to the magnitude of the risk. II. Chapter II: Realising Trustworthy AIThis Chapter offers guidance on the implementation and realisation of Trustworthy AI, via a list of sevenrequirements that should be met, building on the principles outlined in Chapter I. In addition, available technicaland non-technical methods are introduced for the implementation of these requirements throughout the AIsystem’s life cycle.1. Requirements of Trustworthy AIThe principles outlined in Chapter I must be translated into concrete requirements to achieve Trustworthy AI. Theserequirements are applicable to different stakeholders partaking in AI systems’ life cycle: developers, deployers andend-users, as well as the broader society. By developers, we refer to those who research, design and/or develop AIsystems. By deployers, we refer to public or private organisations that use AI systems within their businessprocesses and to offer products and services to others. End-users are those engaging with the AI system, directly orindirectly. Finally, the broader society encompasses all others that are directly or indirectly affected by AI systems. Different groups of stakeholders have different roles to play in ensuring that the requirements are met: a. Developers should implement and apply the requirements to design and development processes; b. Deployers should ensure that the systems they use and the products and services they offer meet therequirements; c. End-users and the broader society should be informed about these requirements and able to request thatthey are upheld. The below list of requirements is non-exhaustive.35 It includes systemic, individual and societal aspects:1 Human agency and oversight Including fundamental rights, human agency and human oversight2 Technical robustness and safety Including resilience to attack and security, fall back plan and general safety, accuracy, reliability andreproducibility3 Privacy and data governance Including respect for privacy, quality and integrity of data, and access to data4 Transparency Including traceability, explainability and communication5 Diversity, non-discrimination and fairness Including the avoidance of unfair bias, accessibility and universal design, and stakeholder participation6 Societal and environmental wellbeing Including sustainability and environmental friendliness, social impact, society and democracy7 Accountability Including auditability, minimisation and reporting of negative impact, trade-offs and redress.35 Without imposing a hierarchy, we list the principles here below in manner that mirrors the order of appearance of the principlesand rights to which they relate in the EU Charter. Figure 2: Interrelationship of the seven requirements: all are of equal importance, support each other, and should beimplemented and evaluated throughout the AI system’s lifecycle While all requirements are of equal importance, context and potential tensions between them will need to be takeninto account when applying them across different domains and industries. Implementation of these requirementsshould occur throughout an AI system’s entire life cycle and depends on the specific application. While mostrequirements apply to all AI systems, special attention is given to those directly or indirectly affecting individuals. Therefore, for some applications (for instance in industrial settings), they may be of lesser relevance. The above requirements include elements that are in some cases already reflected in existing laws. We reiteratethat – in line with Trustworthy AI’s first component – it is the responsibility of AI practitioners to ensure that theycomply with their legal obligations, both as regards horizontally applicable rules as well as domain-specificregulation. In the following paragraphs, each requirement is explained in more detail.1.1 Human agency and oversight AI systems should support human autonomy and decision-making, as prescribed by the principle of respect forhuman autonomy. This requires that AI systems should both act as enablers to a democratic, flourishing andequitable society by supporting the user’s agency and foster fundamental rights, and allow for human oversight. Fundamental rights. Like many technologies, AI systems can equally enable and hamper fundamental rights. Theycan benefit people for instance by helping them track their personal data, or by increasing the accessibility ofeducation, hence supporting their right to education. However, given the reach and capacity of AI systems, they canalso negatively affect fundamental rights. In situations where such risks exist, a fundamental rights impactassessment should be undertaken. This should be done prior to the system’s development and include anevaluation of whether those risks can be reduced or justified as necessary in a democratic society in order to respectthe rights and freedoms of others. Moreover, mechanisms should be put into place to receive external feedbackregarding AI systems that potentially infringe on fundamental rights. Human agency. Users should be able to make informed autonomous decisions regarding AI systems. They should begiven the knowledge and tools to comprehend and interact with AI systems to a satisfactory degree and, wherepossible, be enabled to reasonably self-assess or challenge the system. AI systems should support individuals inmaking better, more informed choices in accordance with their goals. AI systems can sometimes be deployed toshape and influence human behaviour through mechanisms that may be difficult to detect, since they may harnesssub-conscious processes, including various forms of unfair manipulation, deception, herding and conditioning, all ofwhich may threaten individual autonomy. The overall principle of user autonomy must be central to the system’sfunctionality. Key to this is the right not to be subject to a decision based solely on automated processing when thisproduces legal effects on users or similarly significantly affects them.36Human oversight. Human oversight helps ensuring that an AI system does not undermine human autonomy orcauses other adverse effects. Oversight may be achieved through governance mechanisms such as a human-in-the-loop (HITL), human-on-the-loop (HOTL), or human-in-command (HIC) approach. HITL refers to the capability forhuman intervention in every decision cycle of the system, which in many cases is neither possible nor desirable. HOTL refers to the capability for human intervention during the design cycle of the system and monitoring thesystem’s operation. HIC refers to the capability to oversee the overall activity of the AI system (including its broadereconomic, societal, legal and ethical impact) and the ability to decide when and how to use the system in anyparticular situation. This can include the decision not to use an AI system in a particular situation, to establish levelsof human discretion during the use of the system, or to ensure the ability to override a decision made by a system. Moreover, it must be ensured that public enforcers have the ability to exercise oversight in line with their mandate. Oversight mechanisms can be required in varying degrees to support other safety and control measures, dependingon the AI system’s application area and potential risk. All other things being equal, the less oversight a human canexercise over an AI system, the more extensive testing and stricter governance is required.1.2 Technical robustness and safety A crucial component of achieving Trustworthy AI is technical robustness, which is closely linked to the principle ofprevention of harm. Technical robustness requires that AI systems be developed with a preventative approach torisks and in a manner such that they reliably behave as intended while minimising unintentional and unexpectedharm, and preventing unacceptable harm. This should also apply to potential changes in their operatingenvironment or the presence of other agents (human and artificial) that may interact with the system in anadversarial manner. In addition, the physical and mental integrity of humans should be ensured. Resilience to attack and security. AI systems, like all software systems, should be protected against vulnerabilitiesthat can allow them to be exploited by adversaries, e. g. hacking. Attacks may target the data (data poisoning), themodel (model leakage) or the underlying infrastructure, both software and hardware. If an AI system is attacked, e. g. in adversarial attacks, the data as well as system behaviour can be changed, leading the system to makedifferent decisions, or causing it to shut down altogether. Systems and data can also become corrupted by maliciousintention or by exposure to unexpected situations. Insufficient security processes can also result in erroneousdecisions or even physical harm. For AI systems to be considered secure,37 possible unintended applications of the AI system (e. g. dual-use applications) and potential abuse of the system by malicious actors should be taken intoaccount, and steps should be taken to prevent and mitigate these.38Fallback plan and general safety. AI systems should have safeguards that enable a fallback plan in case of problems.36 Reference can be made to Article 22 of the GDPR where this right is already enshrined.37 See e. g. considerations under 2.7 of the European Union’s Coordinated Plan on Artificial Intelligence.38 There may be a strong imperative to develop a virtuous circle in research and development between understanding of attacks, development of adequate protection, and improvement of evaluation methodologies. To achieve this, convergence between the AI community and the security community should be promoted. In addition, it is the responsibility of all relevant actors to createcommon cross-border safety and security norms and to establish an environment of mutual trust, fostering internationalcollaboration. For possible measures, see Malicious Use of AI, Avin S., Brundage M. et. al., 2018. This can mean that AI systems switch from a statistical to rule-based procedure, or that they ask for a humanoperator before continuing their action.39 It must be ensured that the system will do what it is supposed to dowithout harming living beings or the environment. This includes the minimisation of unintended consequences anderrors. In addition, processes to clarify and assess potential risks associated with the use of AI systems, acrossvarious application areas, should be established. The level of safety measures required depends on the magnitudeof the risk posed by an AI system, which in turn depends on the system’s capabilities. Where it can be foreseen thatthe development process or the system itself will pose particularly high risks, it is crucial for safety measures to bedeveloped and tested proactively. Accuracy. Accuracy pertains to an AI system’s ability to make correct judgements, for example to correctly classifyinformation into the proper categories, or its ability to make correct predictions, recommendations, or decisionsbased on data or models. An explicit and well-formed development and evaluation process can support, mitigateand correct unintended risks from inaccurate predictions. When occasional inaccurate predictions cannot beavoided, it is important that the system can indicate how likely these errors are. A high level of accuracy is especiallycrucial in situations where the AI system directly affects human lives. Reliability and Reproducibility. It is critical that the results of AI systems are reproducible, as well as reliable. Areliable AI system is one that works properly with a range of inputs and in a range of situations. This is needed toscrutinise an AI system and to prevent unintended harms. Reproducibility describes whether an AI experimentexhibits the same behaviour when repeated under the same conditions. This enables scientists and policy makers toaccurately describe what AI systems do. Replication files40 can facilitate the process of testing and reproducingbehaviours.1.3 Privacy and data governance Closely linked to the principle of prevention of harm is privacy, a fundamental right particularly affected by AIsystems. Prevention of harm to privacy also necessitates adequate data governance that covers the quality andintegrity of the data used, its relevance in light of the domain in which the AI systems will be deployed, its accessprotocols and the capability to process data in a manner that protects privacy. Privacy and data protection. AI systems must guarantee privacy and data protection throughout a system’s entirelifecycle.41 This includes the information initially provided by the user, as well as the information generated aboutthe user over the course of their interaction with the system (e. g. outputs that the AI system generated for specificusers or how users responded to particular recommendations). Digital records of human behaviour may allow AIsystems to infer not only individuals’ preferences, but also their sexual orientation, age, gender, religious or politicalviews. To allow individuals to trust the data gathering process, it must be ensured that data collected about themwill not be used to unlawfully or unfairly discriminate against them. Quality and integrity of data. The quality of the data sets used is paramount to the performance of AI systems. When data is gathered, it may contain socially constructed biases, inaccuracies, errors and mistakes. This needs tobe addressed prior to training with any given data set. In addition, the integrity of the data must be ensured. Feeding malicious data into an AI system may change its behaviour, particularly with self-learning systems. Processes and data sets used must be tested and documented at each step such as planning, training, testing anddeployment. This should also apply to AI systems that were not developed in-house but acquired elsewhere. Access to data. In any given organisation that handles individuals’ data (whether someone is a user of the system ornot), data protocols governing data access should be put in place. These protocols should outline who can accessdata and under which circumstances. Only duly qualified personnel with the competence and need to accessindividual’s data should be allowed to do so.39 Scenarios where human intervention would not immediately be possible should also be considered.40 This concerns files that will replicate each step of the AI system’s development process, from research and initial data collectionto the results.41 Reference can be made to existing privacy laws, such as the GDPR or the forthcoming e Privacy Regulation.1.4 Transparency This requirement is closely linked with the principle of explicability and encompasses transparency of elementsrelevant to an AI system: the data, the system and the business models. Traceability. The data sets and the processes that yield the AI system’s decision, including those of data gatheringand data labelling as well as the algorithms used, should be documented to the best possible standard to allow fortraceability and an increase in transparency. This also applies to the decisions made by the AI system. This enablesidentification of the reasons why an AI-decision was erroneous which, in turn, could help prevent future mistakes. Traceability facilitates auditability as well as explainability. Explainability. Explainability concerns the ability to explain both the technical processes of an AI system and therelated human decisions (e. g. application areas of a system). Technical explainability requires that the decisionsmade by an AI system can be understood and traced by human beings. Moreover, trade-offs might have to be madebetween enhancing a system's explainability (which may reduce its accuracy) or increasing its accuracy (at the costof explainability). Whenever an AI system has a significant impact on people’s lives, it should be possible to demanda suitable explanation of the AI system’s decision-making process. Such explanation should be timely and adaptedto the expertise of the stakeholder concerned (e. g. layperson, regulator or researcher). In addition, explanations ofthe degree to which an AI system influences and shapes the organisational decision-making process, design choicesof the system, and the rationale for deploying it, should be available (hence ensuring business model transparency). Communication. AI systems should not represent themselves as humans to users; humans have the right to beinformed that they are interacting with an AI system. This entails that AI systems must be identifiable as such. Inaddition, the option to decide against this interaction in favour of human interaction should be provided whereneeded to ensure compliance with fundamental rights. Beyond this, the AI system’s capabilities and limitationsshould be communicated to AI practitioners or end-users in a manner appropriate to the use case at hand. Thiscould encompass communication of the AI system's level of accuracy, as well as its limitations.1.5 Diversity, non-discrimination and fairness In order to achieve Trustworthy AI, we must enable inclusion and diversity throughout the entire AI system’s lifecycle. Besides the consideration and involvement of all affected stakeholders throughout the process, this alsoentails ensuring equal access through inclusive design processes as well as equal treatment. This requirement isclosely linked with the principle of fairness. Avoidance of unfair bias. Data sets used by AI systems (both for training and operation) may suffer from theinclusion of inadvertent historic bias, incompleteness and bad governance models. The continuation of such biasescould lead to unintended (in)direct prejudice and discrimination42 against certain groups or people, potentiallyexacerbating prejudice and marginalisation. Harm can also result from the intentional exploitation of (consumer)biases or by engaging in unfair competition, such as the homogenisation of prices by means of collusion or a non-transparent market.43 Identifiable and discriminatory bias should be removed in the collection phase wherepossible. The way in which AI systems are developed (e. g. algorithms’ programming) may also suffer from unfairbias. This could be counteracted by putting in place oversight processes to analyse and address the system’spurpose, constraints, requirements and decisions in a clear and transparent manner. Moreover, hiring from diversebackgrounds, cultures and disciplines can ensure diversity of opinions and should be encouraged. Accessibility and universal design. Particularly in business-to-consumer domains, systems should be user-centricand designed in a way that allows all people to use AI products or services, regardless of their age, gender, abilitiesor characteristics. Accessibility to this technology for persons with disabilities, which are present in all societal42 For a definition of direct and indirect discrimination, see for instance Article 2 of Council Directive 2000/78/EC of 27 November2000 establishing a general framework for equal treatment in employment and occupation. See also Article 21 of the Charter of Fundamental Rights of the EU.43 See the EU Agency for Fundamental Rights’ paper: “Big Data: Discrimination in data-supported decision making”, 2018, europa. eu/en/publication/2018/big-data-discrimination. groups, is of particular importance. AI systems should not have a one-size-fits-all approach and should consider Universal Design44 principles addressing the widest possible range of users, following relevant accessibilitystandards.45 This will enable equitable access and active participation of all people in existing and emergingcomputer-mediated human activities and with regard to assistive technologies.46Stakeholder Participation. In order to develop AI systems that are trustworthy, it is advisable to consultstakeholders who may directly or indirectly be affected by the system throughout its life cycle. It is beneficial tosolicit regular feedback even after deployment and set up longer term mechanisms for stakeholder participation, forexample by ensuring workers information, consultation and participation throughout the whole process ofimplementing AI systems at organisations.1.6 Societal and environmental well-being In line with the principles of fairness and prevention of harm, the broader society, other sentient beings and theenvironment should be also considered as stakeholders throughout the AI system’s life cycle. Sustainability andecological responsibility of AI systems should be encouraged, and research should be fostered into AI solutionsaddressing areas of global concern, such as for instance the Sustainable Development Goals. Ideally, AI systemsshould be used to benefit all human beings, including future generations. Sustainable and environmentally friendly AI. AI systems promise to help tackling some of the most pressing societalconcerns, yet it must be ensured that this occurs in the most environmentally friendly way possible. The system’sdevelopment, deployment and use process, as well as its entire supply chain, should be assessed in this regard, e. g. via a critical examination of the resource usage and energy consumption during training, opting for less harmfulchoices. Measures securing the environmental friendliness of AI systems’ entire supply chain should be encouraged. Social impact. Ubiquitous exposure to social AI systems47 in all areas of our lives (be it in education, work, care orentertainment) may alter our conception of social agency, or impact our social relationships and attachment. While AI systems can be used to enhance social skills,48 they can equally contribute to their deterioration. This could alsoaffect people’s physical and mental wellbeing. The effects of these systems must therefore be carefully monitoredand considered. Society and Democracy. Beyond assessing the impact of an AI system’s development, deployment and use onindividuals, this impact should also be assessed from a societal perspective, taking into account its effect oninstitutions, democracy and society at large. The use of AI systems should be given careful consideration particularlyin situations relating to the democratic process, including not only political decision-making but also electoralcontexts.1.7 Accountability The requirement of accountability complements the above requirements, and is closely linked to the principle offairness. It necessitates that mechanisms be put in place to ensure responsibility and accountability for AI systemsand their outcomes, both before and after their development, deployment and use. Auditability. Auditability entails the enablement of the assessment of algorithms, data and design processes. Thisdoes not necessarily imply that information about business models and intellectual property related to the AI44 Article 42 of the Public Procurement Directive requires technical specifications to consider accessibility and ‘design for all’.45 For instance EN 301 549.46 This requirement links to the United Nations Convention on the Rights of Persons with Disabilities.47 This denotes AI systems communicating and interacting with humans by simulating sociality in human robot interaction(embodied AI) or as avatars in virtual reality. By doing so, those systems have the potential to change our socio-cultural practicesand the fabric of our social life.48 See for instance the EU-funded project developing AI-based software that enables robots to interact more effectively withautistic children in human-led therapy sessions, helping to improve their social and communication skills: europa. eu/research/infocentre/article_en. cfm? id=/research/headlines/news/article_19_03_12_en. html? infocentre&item=Infocentre&artid=49968system must always be openly available. Evaluation by internal and external auditors, and the availability of suchevaluation reports, can contribute to the trustworthiness of the technology. In applications affecting fundamentalrights, including safety-critical applications, AI systems should be able to be independently audited. Minimisation and reporting of negative impacts. Both the ability to report on actions or decisions that contributeto a certain system outcome, and to respond to the consequences of such an outcome, must be ensured. Identifying, assessing, documenting and minimising the potential negative impacts of AI systems is especially crucialfor those (in)directly affected. Due protection must be available for whistle-blowers, NGOs, trade unions or otherentities when reporting legitimate concerns about an AI system. The use of impact assessments (e. g. red teaming orforms of Algorithmic Impact Assessment) both prior to and during the development, deployment and use of AIsystems can be helpful to minimise negative impact. These assessments must be proportionate to the risk that the AI systems pose. Trade-offs. When implementing the above requirements, tensions may arise between them, which may lead toinevitable trade-offs. Such trade-offs should be addressed in a rational and methodological manner within the stateof the art. This entails that relevant interests and values implicated by the AI system should be identified and that, ifconflict arises, trade-offs should be explicitly acknowledged and evaluated in terms of their risk to ethical principles, including fundamental rights. In situations in which no ethically acceptable trade-offs can be identified, thedevelopment, deployment and use of the AI system should not proceed in that form. Any decision about whichtrade-off to make should be reasoned and properly documented. The decision-maker must be accountable for themanner in which the appropriate trade-off is being made, and should continually review the appropriateness of theresulting decision to ensure that necessary changes can be made to the system where needed.49Redress. When unjust adverse impact occurs, accessible mechanisms should be foreseen that ensure adequateredress.50 Knowing that redress is possible when things go wrong is key to ensure trust. Particular attention shouldbe paid to vulnerable persons or groups.2. Technical and non-technical methods to realise Trustworthy AITo implement the above requirements, both technical and non-technical methods can be employed. Theseencompass all stages of an AI system’s life cycle. An evaluation of the methods employed to implement therequirements, as well as reporting and justifying51 changes to the implementation processes, should occur on anongoing basis. AI systems are continuously evolving and acting in a dynamic environment. The realisation of Trustworthy AI is therefore a continuous process, as depicted in Figure 3 here below. Figure 3: Realising Trustworthy AI throughout the system’s entire life cycle49 Different governance models can help achieving this. E. g. the presence of an internal and/or external ethical (and sector specific)expert or board might be useful to highlight areas of potential conflict and suggest ways in which that conflict might best beresolved. Meaningful consultation and discussion with stakeholders, including those at risk of being adversely affected by an AIsystem is useful too. European universities should take a leading role in training the ethics experts needed.50 See also the European Union Agency for Fundamental Rights' Opinion on ‘Improving access to remedy in the area of business andhuman rights at the EU level’, 2017, europa. eu/en/opinion/2017/business-human-rights.51 This entails e. g. justification of the choices in the system’s design, development and deployment to implement the requirements. The following methods can be either complementary or alternative to each other, since different requirements –and different sensitivities – may raise the need for different methods of implementation. This overview is neithermeant to be comprehensive or exhaustive, nor mandatory. Rather, its aim is to offer a list of suggested methodsthat may help to implement Trustworthy AI.2.1. Technical methods This section describes technical methods to ensure Trustworthy AI that can be incorporated in the design, development and use phases of an AI system. The methods listed below vary in level of maturity.52▪ Architectures for Trustworthy AIRequirements for Trustworthy AI should be “translated” into procedures and/or constraints on procedures, whichshould be anchored in the AI system’s architecture. This could be accomplished through a set of “white list” rules(behaviours or states) that the system should always follow, “black list” restrictions on behaviours or states that thesystem should never transgress, and mixtures of those or more complex provable guarantees regarding the system’sbehaviour. Monitoring of the system’s compliance with these restrictions during operations may be achieved by aseparate process. AI systems with learning capabilities that can dynamically adapt their behaviour can be understood as non-deterministic systems possibly exhibiting unexpected behaviour. These are often considered through the theoreticallens of a “sense-plan-act” cycle. Adapting this architecture to ensure Trustworthy AI requires the requirements’integration at all three steps of the cycle: (i) at the “sense”-step, the system should be developed such that itrecognises all environmental elements necessary to ensure adherence to the requirements; (ii) at the “plan”-step, the system should only consider plans that adhere to the requirements; (iii) at the “act”-step, the system’s actionsshould be restricted to behaviours that realise the requirements. The architecture as sketched above is generic and only provides an imperfect description for most AI systems. Nevertheless, it gives anchor points for constraints and policies that should be reflected in specific modules to resultin an overall system that is trustworthy and perceived as such.▪ Ethics and rule of law by design (X-by-design)Methods to ensure values-by-design provide precise and explicit links between the abstract principles which thesystem is required to respect and the specific implementation decisions. The idea that compliance with norms canbe implemented into the design of the AI system is key to this method. Companies are responsible for identifyingthe impact of their AI systems from the very start, as well as the norms their AI system ought to comply with toavert negative impacts. Different “by-design” concepts are already widely used, e. g. privacy-by-design and security-by-design. As indicated above, to earn trust AI needs to be secure in its processes, data and outcomes, and shouldbe designed to be robust to adversarial data and attacks. It should implement a mechanism for fail-safe shutdownand enable resumed operation after a forced shut-down (such as an attack).▪ Explanation methods For a system to be trustworthy, we must be able to understand why it behaved a certain way and why it provided agiven interpretation. A whole field of research, Explainable AI (XAI) tries to address this issue to better understandthe system’s underlying mechanisms and find solutions. Today, this is still an open challenge for AI systems based onneural networks. Training processes with neural nets can result in network parameters set to numerical values thatare difficult to correlate with results. Moreover, sometimes small changes in data values might result in dramaticchanges in interpretation, leading the system to e. g. confuse a school bus with an ostrich. This vulnerability can alsobe exploited during attacks on the system. Methods involving XAI research are vital not only to explain the system’s52 While some of these methods are already available today, others still require more research. Those areas where further researchis needed will also inform the AI HLEG's second deliverable, i. e. the Policy and Investment Recommendations. behaviour to users, but also to deploy reliable technology.▪ Testing and validating Due to the non-deterministic and context-specific nature of AI systems, traditional testing is not enough. Failures ofthe concepts and representations used by the system may only manifest when a programme is applied tosufficiently realistic data. Consequently, to verify and validate processing of data, the underlying model must becarefully monitored during both training and deployment for its stability, robustness and operation within well-understood and predictable bounds. It must be ensured that the outcome of the planning process is consistent withthe input, and that the decisions are made in a way allowing validation of the underlying process. Testing and validation of the system should occur as early as possible, ensuring that the system behaves as intendedthroughout its entire life cycle and especially after deployment. It should include all components of an AI system, including data, pre-trained models, environments and the behaviour of the system as a whole. The testing processesshould be designed and performed by an as diverse group of people as possible. Multiple metrics should bedeveloped to cover the categories that are being tested for different perspectives. Adversarial testing by trusted anddiverse “red teams” deliberately attempting to “break” the system to find vulnerabilities, and “bug bounties” thatincentivise outsiders to detect and responsibly report system errors and weaknesses, can be considered. Finally, itmust be ensured that the outputs or actions are consistent with the results of the preceding processes, comparingthem to the previously defined policies to ensure that they are not violated.▪ Quality of Service Indicators Appropriate quality of service indicators can be defined for AI systems to ensure that there is a baselineunderstanding as to whether they have been tested and developed with security and safety considerations in mind. These indicators could include measures to evaluate the testing and training of algorithms as well as traditionalsoftware metrics of functionality, performance, usability, reliability, security and maintainability.2.2. Non-technical methods This section describes a variety of non-technical methods that can serve a valuable role in securing and maintaining Trustworthy AI. These too should be evaluated on an ongoing basis.▪ Regulation As mentioned above, regulation to support AI’s trustworthiness already exists today – think of product safetylegislation and liability frameworks. To the extent we consider that regulation may need to be revised, adapted orintroduced, both as a safeguard and as an enabler, this will be raised in our second deliverable, consisting of AIPolicy and Investment Recommendations.▪ Codes of conduct Organisations and stakeholders can sign up to the Guidelines and adapt their charter of corporate responsibility, Key Performance Indicators (“KPIs”), their codes of conduct or internal policy documents to add the striving towards Trustworthy AI. An organisation working on or with AI systems can, more generally, document its intentions, as wellas underwrite them with standards of certain desirable values such as fundamental rights, transparency and theavoidance of harm.▪ Standardisation Standards, for example for design, manufacturing and business practices, can function as a quality managementsystem for AI users, consumers, organisations, research institutions and governments by offering the ability torecognise and encourage ethical conduct through their purchasing decisions. Beyond conventional standards, co-regulatory approaches exist: accreditation systems, professional codes of ethics or standards for fundamental rightscompliant design. Current examples are e. g. ISO Standards or the IEEE P7000 standards series, but in the future apossible ‘Trustworthy AI' label might be suitable, confirming by reference to specific technical standards that thesystem, for instance, adheres to safety, technical robustness and transparency.▪ Certification As it cannot be expected that everyone is able to fully understand the workings and effects of AI systems, consideration can be given to organisations that can attest to the broader public that an AI system is transparent, accountable and fair.53 These certifications would apply standards developed for different application domains and AI techniques, appropriately aligned with the industrial and societal standards of different contexts. Certificationcan however never replace responsibility. It should hence be complemented by accountability frameworks, including disclaimers as well as review and redress mechanisms.54▪ Accountability via governance frameworks Organisations should set up governance frameworks, both internal and external, ensuring accountability for theethical dimensions of decisions associated with the development, deployment and use of AI systems. This can, forinstance, include the appointment of a person in charge of ethics issues relating to AI systems, or aninternal/external ethics panel or board. Amongst the possible roles of such a person, panel or board, is to provideoversight and advice. As set out above, certification specifications and bodies can also play a role to this end. Communication channels should be ensured with industry and/or public oversight groups, sharing best practices, discussing dilemmas or reporting emerging issues of ethical concerns. Such mechanisms can complement butcannot replace legal oversight (e. g. in the form of the appointment of a data protection officer or equivalentmeasures, legally required under data protection law).▪ Education and awareness to foster an ethical mind-set Trustworthy AI encourages the informed participation of all stakeholders. Communication, education and trainingplay an important role, both to ensure that knowledge of the potential impact of AI systems is widespread, and tomake people aware that they can participate in shaping the societal development. This includes all stakeholders, e. g. those involved in making the products (the designers and developers), the users (companies or individuals) andother impacted groups (those who may not purchase or use an AI system but for whom decisions are made by an AIsystem, and society at large). Basic AI literacy should be fostered across society. A prerequisite for educating thepublic is to ensure the proper skills and training of ethicists in this space.▪ Stakeholder participation and social dialogue The benefits of AI systems are many, and Europe needs to ensure that they are available to all. This requires anopen discussion and the involvement of social partners and stakeholders, including the general public. Manyorganisations already rely on stakeholder panels to discuss the use of AI systems and data analytics. These panelsinclude various members, such as legal experts, technical experts, ethicists, consumer representatives and workers. Actively seeking participation and dialogue on the use and impact of AI systems supports the evaluation of resultsand approaches, and can particularly be helpful in complex cases.▪ Diversity and inclusive design teams Diversity and inclusion play an essential role when developing AI systems that will be employed in the real world. Itis critical that, as AI systems perform more tasks on their own, the teams that design, develop, test and maintain, deploy and procure these systems reflect the diversity of users and of society in general. This contributes toobjectivity and consideration of different perspectives, needs and objectives. Ideally, teams are not only diverse interms of gender, culture, age, but also in terms of professional backgrounds and skill sets.53 As advocated by e. g. the IEEE Ethically Aligned Design Initiative: ieee. org/industry-connections/ec/autonomous-systems. html.54 For more on the limitations of certification, see: org/AI_Now_2018_Report. pdf. Key guidance derived from Chapter II: Ensure that the AI system’s entire life cycle meets the seven key requirements for Trustworthy AI: (1)human agency and oversight, (2) technical robustness and safety, (3) privacy and data governance, (4)transparency, (5) diversity, non-discrimination and fairness, (6) environmental and societal well-being and(7) accountability. Consider technical and non-technical methods to ensure the implementation of those requirements. Foster research and innovation to help assessing AI systems and to further the achievement of therequirements; disseminate results and open questions to the wider public, and systematically train a newgeneration of experts in AI ethics. Communicate, in a clear and proactive manner, information to stakeholders about the AI system’scapabilities and limitations, enabling realistic expectation setting, and about the manner in which therequirements are implemented. Be transparent about the fact that they are dealing with an AI system. Facilitate the traceability and auditability of AI systems, particularly in critical contexts and situations. Involve stakeholders throughout the AI system’s life cycle. Foster training and education so that allstakeholders are aware of and trained in Trustworthy AI. Be mindful that there might be fundamental tensions between different principles and requirements. Continuously identify, evaluate, document and communicate these trade-offs and their solutions. III. Chapter III: Assessing Trustworthy AIBased on the key requirements of Chapter II, this Chapter sets out a non-exhaustive Trustworthy AI assessmentlist (pilot version) to operationalise Trustworthy AI. It particularly applies to AI systems that directly interactwith users, and is primarily addressed to developers and deployers of AI systems (whether self-developed oracquired from third parties). This assessment list does not address the operationalisation of the first componentof Trustworthy AI (lawful AI). Compliance with this assessment list is not evidence of legal compliance, nor is itintended as guidance to ensure compliance with applicable law. Given the application-specificity of AI systems, the assessment list will need to be tailored to the specific use case and context in which the system operates. Inaddition, this chapter offers a general recommendation on how to implement the assessment list for Trustworthy AI though a governance structure embracing both operational and management level. The assessment list and governance structure will be developed in close collaboration with stakeholders acrossthe public and private sector. The process will be driven as a piloting process, allowing for extensive feedbackfrom two parallel processes: a) a qualitative process, ensuring representability, where a small selection of companies, organisationsand institutions (from different sectors and of different sizes) will sign up to pilot the assessment listand the governance structure in practice and to provide in-depth feedback; b) a quantitative process where all interested stakeholders can sign up to pilot the assessment list andprovide feedback through an open consultation. After the piloting phase, we will integrate the results from the feedback process into the assessment list andprepare a revised version in early 2020. The aim is to achieve a framework that can be horizontally used acrossall applications and hence offer a foundation for ensuring Trustworthy AI in all domains. Once such foundationhas been established, a sectorial or application-specific framework could be developed. Governance Stakeholders may wish to consider how the Trustworthy AI assessment list can be implemented in theirorganisation. This can be done by incorporating the assessment process into existing governance mechanisms, or by implementing new processes. This choice will depend on the internal structure of the organisation as wellas its size and available resources. Research demonstrates that management attention at the highest level is essential to achieve change.55 It alsodemonstrates that involving all stakeholders in a company, organisation or institution fosters the acceptanceand the relevance of the introduction of any new process (whether or not technological).56 Therefore, werecommend implementing a process that embraces both the involvement of operational level as well as topmanagement level. Level Relevant roles (depending on the organisation)Management and Top management discusses and evaluates the AI systems’ development, deployment or Board procurement and serves as an escalation board for evaluating all AI innovations anduses, when critical concerns are detected. It involves those impacted by the possibleintroduction of AI systems (e. g. workers) and their representatives throughout theprocess via information, consultation and participation procedures. Compliance/Legal The responsibility department monitors the use of the assessment list and its necessarydepartment/Corporate evolution to meet the technological or regulatory changes. It updates the standards orresponsibility internal policies on AI systems and ensures that the use of such systems complies withdepartment the current legal and regulatory framework and to the values of the organisation. Product and Service The Product and Service Development department uses the assessment list to evaluate Development or AI-based products and services and logs all the results. These results are discussed atequivalent management level, which ultimately approves the new or revised AI-based applications. Quality Assurance The Quality Assurance department (or equivalent) ensures and checks the results of theassessment list and takes action to escalate an issue higher up if the result is notsatisfactory or if unforeseen results are detected. HR The HR department ensures the right mix of competences and diversity of profiles fordevelopers of AI systems. It ensures that the appropriate level of training is delivered on Trustworthy AI inside the organisation. Procurement The procurement department ensures that the process to procure AI-based products orservices includes a check of Trustworthy AI. Day-to-day Operations Developers and project managers include the assessment list in their daily work anddocument the results and outcomes of the assessment. Using the Trustworthy AI assessment list When using the assessment list in practice, we recommend paying attention not only to the areas of concernbut also to the questions that cannot be (easily) answered. One potential problem might be the lack of diversityof skills and competences in the team developing and testing the AI system, and therefore it might be necessaryto involve other stakeholders inside or outside the organisation. It is strongly recommended to log all resultsboth in technical terms and in management terms, ensuring that the problem solving can be understood at alllevels in the governance structure. This assessment list is meant to guide AI practitioners to achieve Trustworthy AI. The assessment should betailored to the specific use case in a proportionate way. During the piloting phase, specific sensitive areas mightbe revealed and the need for further specifications in such cases will be evaluated in the next steps. While this55 mckinsey. com/business-functions/operations/our-insights/secrets-of-successful-change-implementation56 See for instance A. Bryson, E. Barth and H. Dale-Olsen, The Effects of Organisational change on worker well-being and themoderating role of trade unions, ILRReview, 66(4), July 2013; Jirjahn, U. and Smith, S. C. (2006). ‘What Factors Lead Managementto Support or Oppose Employee Participation—With and Without Works Councils? Hypotheses and Evidence from Germany’s Industrial Relations, 45(4), 650–680; Michie, J. and Sheehan, M. (2003). ‘Labour market deregulation, “flexibility” and innovation’, Cambridge Journal of Economics, 27(1), 123–143. assessment list does not provide concrete answers to address the raised questions, it encourages reflection onhow Trustworthy AI can be operationalised, and on the potential steps that should be taken in this regard. Relation to existing law and processes It is also important for AI practitioners to recognise that there are various existing laws mandating particularprocesses or prohibiting particular outcomes, which may overlap and coincide with some of the measures listedin the assessment list. For example, data protection law sets out a series of legal requirements that must bemet by those engaged in the collection and processing of personal data. Yet, because Trustworthy AI alsorequires the ethical handling of data, internal procedures and policies aimed at securing compliance with dataprotection laws might also help to facilitate ethical data handling and can hence complement existing legalprocesses. Compliance with this assessment list is not, however, evidence of legal compliance, nor is it intendedas guidance to ensure compliance with applicable laws. Moreover, many AI practitioners already have existing assessment tools and software development processesin place to ensure compliance also with non-legal standards. The below assessment should not necessarily becarried out as a stand-alone exercise, but can be incorporated into such existing practices. TRUSTWORTHY AI ASSESSMENT LIST (PILOT VERSION)1. Human agency and oversight Fundamental rights: Did you carry out a fundamental rights impact assessment where there could be a negative impact onfundamental rights? Did you identify and document potential trade-offs made between the differentprinciples and rights? Does the AI system interact with decisions by human (end) users (e. g. recommended actions ordecisions to take, presenting of options)? Could the AI system affect human autonomy by interfering with the (end) user’s decision-makingprocess in an unintended way? Did you consider whether the AI system should communicate to (end) users that a decision, content, advice or outcome is the result of an algorithmic decision? In case of a chat bot or other conversational system, are the human end users made aware thatthey are interacting with a non-human agent? Human agency: Is the AI system implemented in work and labour process? If so, did you consider the task allocationbetween the AI system and humans for meaningful interactions and appropriate human oversight andcontrol? Does the AI system enhance or augment human capabilities? Did you take safeguards to prevent overconfidence in or overreliance on the AI system for workprocesses? Human oversight: Did you consider the appropriate level of human control for the particular AI system and use case? Can you describe the level of human control or involvement? Who is the “human in control” and what are the moments or tools for human intervention? Did you put in place mechanisms and measures to ensure human control or oversight? Did you take any measures to enable audit and to remedy issues related to governing AIautonomy? Is there is a self-learning or autonomous AI system or use case? If so, did you put in place morespecific mechanisms of control and oversight? Which detection and response mechanisms did you establish to assess whether something couldgo wrong? Did you ensure a stop button or procedure to safely abort an operation where needed? Does thisprocedure abort the process entirely, in part, or delegate control to a human?2. Technical robustness and safety Resilience to attack and security: Did you assess potential forms of attacks to which the AI system could be vulnerable? Did you consider different types and natures of vulnerabilities, such as data pollution, physicalinfrastructure, cyber-attacks? Did you put measures or systems in place to ensure the integrity and resilience of the AI systemagainst potential attacks? Did you verify how your system behaves in unexpected situations and environments? Did you consider to what degree your system could be dual-use? If so, did you take suitablepreventative measures against this case (including for instance not publishing the research ordeploying the system)? Fallback plan and general safety: Did you ensure that your system has a sufficient fallback plan if it encounters adversarial attacks orother unexpected situations (for example technical switching procedures or asking for a humanoperator before proceeding)? Did you consider the level of risk raised by the AI system in this specific use case? Did you put any process in place to measure and assess risks and safety? Did you provide the necessary information in case of a risk for human physical integrity? Did you consider an insurance policy to deal with potential damage from the AI system? Did you identify potential safety risks of (other) foreseeable uses of the technology, includingaccidental or malicious misuse? Is there a plan to mitigate or manage these risks? Did you assess whether there is a probable chance that the AI system may cause damage or harm tousers or third parties? Did you assess the likelihood, potential damage, impacted audience andseverity? Did you consider the liability and consumer protection rules, and take them into account? Did you consider the potential impact or safety risk to the environment or to animals? Did your risk analysis include whether security or network problems such as cybersecurityhazards could pose safety risks or damage due to unintentional behaviour of the AI system? Did you estimate the likely impact of a failure of your AI system when it provides wrong results, becomes unavailable, or provides societally unacceptable results (for example discrimination)? Did you define thresholds and did you put governance procedures in place to triggeralternative/fallback plans? Did you define and test fallback plans? Accuracy Did you assess what level and definition of accuracy would be required in the context of the AI systemand use case? Did you assess how accuracy is measured and assured? Did you put in place measures to ensure that the data used is comprehensive and up to date? Did you put in place measures in place to assess whether there is a need for additional data, forexample to improve accuracy or to eliminate bias? Did you verify what harm would be caused if the AI system makes inaccurate predictions? Did you put in place ways to measure whether your system is making an unacceptable amount ofinaccurate predictions? Did you put in place a series of steps to increase the system's accuracy? Reliability and reproducibility: Did you put in place a strategy to monitor and test if the AI system is meeting the goals, purposes andintended applications? Did you test whether specific contexts or particular conditions need to be taken into account toensure reproducibility? Did you put in place verification methods to measure and ensure different aspects of thesystem's reliability and reproducibility? Did you put in place processes to describe when an AI system fails in certain types of settings? Did you clearly document and operationalise these processes for the testing and verification ofthe reliability of AI systems? Did you establish mechanisms of communication to assure (end-)users of the system’s reliability?3. Privacy and data governance Respect for privacy and data Protection: Depending on the use case, did you establish a mechanism allowing others to flag issues related toprivacy or data protection in the AI system’s processes of data collection (for training and operation)and data processing? Did you assess the type and scope of data in your data sets (for example whether they containpersonal data)? Did you consider ways to develop the AI system or train the model without or with minimal use ofpotentially sensitive or personal data? Did you build in mechanisms for notice and control over personal data depending on the use case(such as valid consent and possibility to revoke, when applicable)? Did you take measures to enhance privacy, such as via encryption, anonymisation and aggregation? Where a Data Privacy Officer (DPO) exists, did you involve this person at an early stage in the process? Quality and integrity of data: Did you align your system with relevant standards (for example ISO, IEEE) or widely adopted protocolsfor daily data management and governance? Did you establish oversight mechanisms for data collection, storage, processing and use? Did you assess the extent to which you are in control of the quality of the external data sources used? Did you put in place processes to ensure the quality and integrity of your data? Did you consider otherprocesses? How are you verifying that your data sets have not been compromised or hacked? Access to data: What protocols, processes and procedures did you follow to manage and ensure proper datagovernance? Did you assess who can access users’ data, and under what circumstances? Did you ensure that these persons are qualified and required to access the data, and that theyhave the necessary competences to understand the details of data protection policy? Did you ensure an oversight mechanism to log when, where, how, by whom and for whatpurpose data was accessed?4. Transparency Traceability: Did you establish measures that can ensure traceability? This could entail documenting the followingmethods: Methods used for designing and developing the algorithmic system: o Rule-based AI systems: the method of programming or how the model was built; o Learning-based AI systems; the method of training the algorithm, including which inputdata was gathered and selected, and how this occurred. Methods used to test and validate the algorithmic system: o Rule-based AI systems; the scenarios or cases used in order to test and validate; o Learning-based model: information about the data used to test and validate. Outcomes of the algorithmic system: o The outcomes of or decisions taken by the algorithm, as well as potential other decisionsthat would result from different cases (for example, for other subgroups of users). Explainability: Did you assess: to what extent the decisions and hence the outcome made by the AI system can be understood? to what degree the system’s decision influences the organisation’s decision-making processes? why this particular system was deployed in this specific area? what the system’s business model is (for example, how does it create value for the organisation)? Did you ensure an explanation as to why the system took a certain choice resulting in a certainoutcome that all users can understand? Did you design the AI system with interpretability in mind from the start? Did you research and try to use the simplest and most interpretable model possible for theapplication in question? Did you assess whether you can analyse your training and testing data? Can you change andupdate this over time? Did you assess whether you can examine interpretability after the model’s training anddevelopment, or whether you have access to the internal workflow of the model? Communication: Did you communicate to (end-)users – through a disclaimer or any other means – that they areinteracting with an AI system and not with another human? Did you label your AI system as such? Did you establish mechanisms to inform (end-)users on the reasons and criteria behind the AIsystem’s outcomes? Did you communicate this clearly and intelligibly to the intended audience? Did you establish processes that consider users’ feedback and use this to adapt the system? Did you communicate around potential or perceived risks, such as bias? Depending on the use case, did you consider communication and transparency towards otheraudiences, third parties or the general public? Did you clarify the purpose of the AI system and who or what may benefit from the product/service? Did you specify usage scenarios for the product and clearly communicate these to ensure that itis understandable and appropriate for the intended audience? Depending on the use case, did you think about human psychology and potential limitations, such as risk of confusion, confirmation bias or cognitive fatigue? Did you clearly communicate characteristics, limitations and potential shortcomings of the AI system? In case of the system's development: to whoever is deploying it into a product or service? In case of the system's deployment: to the (end-)user or consumer?5. Diversity, non-discrimination and fairness Unfair bias avoidance: Did you establish a strategy or a set of procedures to avoid creating or reinforcing unfair bias in the AIsystem, both regarding the use of input data as well as for the algorithm design? Did you assess and acknowledge the possible limitations stemming from the composition of theused data sets? Did you consider diversity and representativeness of users in the data? Did you test for specificpopulations or problematic use cases? Did you research and use available technical tools to improve your understanding of the data, model and performance? Did you put in place processes to test and monitor for potential biases during the development, deployment and use phase of the system? Depending on the use case, did you ensure a mechanism that allows others to flag issues related tobias, discrimination or poor performance of the AI system? Did you establish clear steps and ways of communicating on how and to whom such issues can beraised? Did you consider others, potentially indirectly affected by the AI system, in addition to the (end)-users? Did you assess whether there is any possible decision variability that can occur under the sameconditions? If so, did you consider what the possible causes of this could be? In case of variability, did you establish a measurement or assessment mechanism of the potentialimpact of such variability on fundamental rights? Did you ensure an adequate working definition of “fairness” that you apply in designing AI systems? Is your definition commonly used? Did you consider other definitions before choosing this one? Did you ensure a quantitative analysis or metrics to measure and test the applied definition offairness? Did you establish mechanisms to ensure fairness in your AI systems? Did you consider otherpotential mechanisms? Accessibility and universal design: Did you ensure that the AI system accommodates a wide range of individual preferences andabilities? Did you assess whether the AI system usable by those with special needs or disabilities or thoseat risk of exclusion? How was this designed into the system and how is it verified? Did you ensure that information about the AI system is accessible also to users of assistivetechnologies? Did you involve or consult this community during the development phase of the AI system? Did you take the impact of your AI system on the potential user audience into account? Did you assess whether the team involved in building the AI system is representative of yourtarget user audience? Is it representative of the wider population, considering also of othergroups who might tangentially be impacted? Did you assess whether there could be persons or groups who might be disproportionatelyaffected by negative implications? Did you get feedback from other teams or groups that represent different backgrounds andexperiences? Stakeholder participation: Did you consider a mechanism to include the participation of different stakeholders in the AI system’sdevelopment and use? Did you pave the way for the introduction of the AI system in your organisation by informing andinvolving impacted workers and their representatives in advance?6. Societal and environmental well-being Sustainable and environmentally friendly AI: Did you establish mechanisms to measure the environmental impact of the AI system’s development, deployment and use (for example the type of energy used by the data centres)? Did you ensure measures to reduce the environmental impact of your AI system’s life cycle? Social impact: In case the AI system interacts directly with humans: Did you assess whether the AI system encourages humans to develop attachment and empathytowards the system? Did you ensure that the AI system clearly signals that its social interaction is simulated and that ithas no capacities of “understanding” and “feeling”? Did you ensure that the social impacts of the AI system are well understood? For example, did youassess whether there is a risk of job loss or de-skilling of the workforce? What steps have been takento counteract such risks? Society and democracy: Did you assess the broader societal impact of the AI system’s use beyond the individual (end-)user, such as potentially indirectly affected stakeholders?7. Accountability Auditability: Did you establish mechanisms that facilitate the system’s auditability, such as ensuring traceabilityand logging of the AI system’s processes and outcomes? Did you ensure, in applications affecting fundamental rights (including safety-critical applications) thatthe AI system can be audited independently? Minimising and reporting negative Impact: Did you carry out a risk or impact assessment of the AI system, which takes into account differentstakeholders that are (in)directly affected? Did you provide training and education to help developing accountability practices? Which workers or branches of the team are involved? Does it go beyond the development phase? Do these trainings also teach the potential legal framework applicable to the AI system? Did you consider establishing an ‘ethical AI review board’ or a similar mechanism to discussoverall accountability and ethics practices, including potentially unclear grey areas? Did you foresee any kind of external guidance or put in place auditing processes to oversee ethics andaccountability, in addition to internal initiatives? Did you establish processes for third parties (e. g. suppliers, consumers, distributors/vendors) orworkers to report potential vulnerabilities, risks or biases in the AI system? Documenting trade-offs: Did you establish a mechanism to identify relevant interests and values implicated by the AI systemand potential trade-offs between them? How do you decide on such trade-offs? Did you ensure that the trade-off decision was documented? Ability to redress: Did you establish an adequate set of mechanisms that allows for redress in case of the occurrence ofany harm or adverse impact? Did you put mechanisms in place both to provide information to (end-)users/third parties aboutopportunities for redress? We invite all stakeholders to pilot this Assessment List in practice and to provide feedback on itsimplementability, completeness, relevance for the specific AI application or domain, as well as overlap orcomplementarity with existing compliance or assessment processes. Based on this feedback, a revisedversion of the Trustworthy AI assessment list will be proposed to the Commission in early 2020Key guidance derived from Chapter III: Adopt a Trustworthy AI assessment list when developing, deploying or using AI systems, and adapt it tothe specific use case in which the system is being applied. Keep in mind that such assessment list will never be exhaustive. Ensuring Trustworthy AI is not aboutticking boxes, but about continuously identifying requirements, evaluating solutions and ensuringimproved outcomes throughout the AI system’s lifecycle, and involving stakeholders therein. C. EXAMPLES OF OPPORTUNITIES AND CRITICAL CONCERNS RAISED BY AIIn the following section, we provide examples of AI development and use that should be encouraged, as well asexamples of where AI development, deployment or use can run counter to our values and may raise specificconcerns. A balance must be struck between what should and what can be done with AI, and due care must begiven to what should not be done with AI.1. Examples of Trustworthy AI’s opportunities Trustworthy AI can represent a great opportunity to support the mitigation of pressing challenges facing societysuch as an ageing population, growing social inequality and environmental pollution. This potential is also reflectedglobally, such as with the UN Sustainable Development Goals.57 The following section looks at how to encourage a European AI strategy that tackles some of these challenges. Climate action and sustainable infrastructure While tackling climate change should be a top priority for policy-makers across the world, digital transformation and Trustworthy AI have a great potential to reduce humans’ impact on the environment and enable the efficient andeffective use of energy and natural resources.58 Trustworthy AI can, for instance, be coupled to big data in order todetect energy needs more accurately, resulting in more efficient energy infrastructure and consumption.59Looking at sectors like public transportation, AI systems for intelligent transport systems60 can be used to minimisequeuing, optimise routing, allow vision impaired people to be more independent,61 optimise energy efficientengines and thereby enhance decarbonisation efforts and reduce the environmental footprint, for a greener society. Currently, worldwide, one human dies every 23 seconds in a car accident.62 AI systems could help to reduce thenumber fatalities significantly, for instance through better reaction times and better adherence to rules.63 Health and well-being Trustworthy AI technologies can be used – and are already being used – to render treatment smarter and moretargeted, and to help preventing life-threatening diseases.64 Doctors and medical professionals can potentiallyperform a more accurate and detailed analysis of a patient’s complex health data, even before people get sick, andprovide tailored preventive treatment.65 In the context of Europe’s ageing population, AI technologies and roboticscan be valuable tools to assist caregivers, support elderly care,66 and monitor patients’ conditions on a real time57 un. org/? menu=130058 A number of EU projects aim for the development of Smart Grids and Energy Storage, which have the potential tocontribute to a successful digitally supported energy transition, including through AI-based and other digital solutions. Tocomplement the work of those individual projects, the Commission has launched the BRIDGE initiative, allowing ongoing Horizon2020 Smart Grid and Energy Storage projects to create a common view on cross cutting issues: h2020-bridge. eu/.59 See for instance the Encompass project: encompass-project. eu/.60 New AI-based solutions help prepare cities for the future of mobility. See for instance the EU funded project called Fabulos: eu/.61 See for instance the PRO4VIP project, which is part of the European Vision 2020 strategy to combat preventable blindness, especially due to old age. Mobility and orientation was one of the project's priority areas.62 who. int/news-room/fact-sheets/detail/road-traffic-injuries.63 The European UP-Drive project for instance aims to address the outlined transport-related challenges by providing contributionsenabling gradual automation of and collaboration among vehicles , facilitating a safer, more inclusive and more affordabletransportation system. eu/.64 See for instance the REVOLVER (Repeated Evolution of Cancer) project: healtheuropa. eu/personalised-cancer-treatment/87958/, or the Murab project which conducts more accurate biopsies, and which aims at diagnosing cancer and otherillnesses faster: europa. eu/digital-single-market/en/news/murab-eu-funded-project-success-story.65 See for instance the Live INCITE project: www. karolinska. se/en/live-incite. This consortium of healthcare procurers challenges theindustry to develop smart AI and other ICT solutions that enable lifestyle interventions in the perioperative process. The targetconcerns new innovative e Health solutions that can influence patients in a personalised way to take the necessary actions bothprior and after surgery in their lifestyle to optimise the healthcare outcome.66 The EU-funded project CARESSES deals with robots for elderly care, focusing on their cultural sensitivity: they adapt their way ofacting and speaking to match the culture and habits of the elderly person they are assisting: org/en/project/. See also the AI application called Alfred, a virtual assistant helping older people stay active: basis, thus saving lives.67Trustworthy AI can also assist on a broader scale. For example, it can examine and identify general trends in thehealthcare and treatment sector,68 leading to earlier detection of diseases, more efficient development ofmedicines, more targeted treatments69 and ultimately more lives saved. Quality education and digital transformation New technological, economic and environmental changes mean that society needs to become more proactive. Governments, industry leaders, educational institutions and unions face a responsibility to bring the citizens into thenew digital era ensuring they have the right skills to fill the future jobs. Trustworthy AI technologies could assist inmore accurately forecasting which jobs and professions will be disrupted by technology, which new roles will becreated and which skills will be needed. This could help governments, unions and industry with planning the(re)skilling of workers. It could also give citizens who may fear redundancy a path of development into a new role. In addition, AI can be a great tool to fight educational inequalities and create personalised and adaptable educationprogrammes that could help everyone acquire new qualifications, skills and competences according to his or herown ability to learn.70 It could increase both the learning speed and the quality of education – reaching fromprimary school to university.2. Examples of critical concerns raised by AIA critical AI concern arises one of the components of Trustworthy AI is violated. Many of the concerns listed belowwill already fall within the scope of existing legal requirements, which are mandatory and must therefore becomplied with. Yet even in circumstances where compliance with legal requirements has been demonstrated, thesemay not address the full range of ethical concerns that may arise. As our understanding of the adequacy of rules andethical principles invariably evolves and may change over time, the following non-exhaustive list of concerns may beshortened, expanded, edited or updated in the future. Identifying and tracking individuals with AIAI enables the ever more efficient identification of individual persons by both public and private entities. Noteworthy examples of a scalable AI identification technology are face recognition and other involuntary methodsof identification using biometric data (i. e. lie detection, personality assessment through micro expressions, andautomatic voice detection). Identification of individuals is sometimes the desirable outcome, aligned with ethicalprinciples (for example in detecting fraud, money laundering, or terrorist financing). However, automaticidentification raises strong concerns of both a legal and ethical nature, as it may have an unexpected impact onmany psychological and sociocultural levels. A proportionate use of control techniques in AI is needed to uphold theautonomy of European citizens. Clearly defining if, when and how AI can be used for automated identification ofindividuals and differentiating between the identification of an individual vs the tracing and tracking of an individual, europa. eu/digital-single-market/en/news/alfred-virtual-assistant-helping-older-people-stay-active. Moreover, the EMPATTICS project (EMpowering PAtients for a Be TTer Information and improvement of the Communication Systems) willresearch and define how health care professionals and patients use ICT technologies including AI systems to plan interventionswith patients and to monitor the progression of their physical and mental state: www. empattics. eu.67 See for instance the My Health Avatar (www. myhealthavatar. eu), which offers a digital representation of a patient's health status. The research project launched an app and an online platform that collects, and gives access to, your digital long-term health-status information. This takes on the form of a life-long health companion ('avatar'). My Health Avatar also predicts your risk forstroke, diabetes, cardiovascular disease and hypertension.68 See for instance the ENRICHME project (www. enrichme. eu), which tackles the progressive decline of cognitive capacity in theageing population. An integrated platform for Ambient Assisted Living (AAL) and a mobile service robot for long-term monitoringand interaction will help the elderly to remain independent and active for longer.69 See for instance the use of AI by Sophia Genetics, which leverages statistical inference, pattern recognition and machine learningto maximize the value of genomics and radiomics data: sophiagenetics. com/home. html.70 See for instance the Ma THi Si S project, aimed at providing a solution for affect-based learning in a comfortable learningenvironment, comprising of high-end technological devices and algorithms: ( eu/). See also IBM’s Watson Classroom or Century Tech’s platform. and between targeted surveillance and mass surveillance, will be crucial for the achievement of Trustworthy AI. Theapplication of such technologies must be clearly warranted in existing law.71 Where the legal basis for such activity is“consent”, practical means72 must be developed which allow meaningful and verified consent to be given to beingautomatically identified by AI or equivalent technologies. This also applies to the usage of “anonymous” personaldata that can be re-personalised. Covert AI systems Human beings should always know if they are directly interacting with another human being or a machine, and it isthe responsibility of AI practitioners that this is reliably achieved. AI practitioners should therefore ensure thathumans are made aware of – or able to request and validate the fact that – they interact with an AI system (forinstance, by issuing clear and transparent disclaimers). Note that borderline cases exist and complicate the matter(e. g. an AI-filtered voice spoken by a human). It should be borne in mind that the confusion between humans andmachines could have multiple consequences such as attachment, influence, or reduction of the value of beinghuman.73 The development of human-like robots74 should therefore undergo careful ethical assessment. AI enabled citizen scoring in violation of fundamental rights Societies should strive to protect the freedom and autonomy of all citizens. Any form of citizen scoring can lead tothe loss of this autonomy and endanger the principle of non-discrimination. Scoring should only be used if there is aclear justification, and where measures are proportionate and fair. Normative citizen scoring (general assessment of“moral personality” or “ethical integrity”) in all aspects and on a large scale by public authorities or private actorsendangers these values, especially when used not in accordance with fundamental rights, and when useddisproportionately and without a delineated and communicated legitimate purpose. Today, citizen scoring – on a large or smaller scale – is already often used in purely descriptive and domain-specificscorings (e. g. school systems, e-learning, and driver licences). Even in those more narrow applications, a fullytransparent procedure should be made available to citizens, including information on the process, purpose andmethodology of the scoring. Note that transparency cannot prevent non-discrimination or ensure fairness, and isnot the panacea against the problem of scoring. Ideally the possibility of opting out of the scoring mechanism whenpossible without detriment should be provided – otherwise mechanisms for challenging and rectifying the scoresmust be given. This is particularly important in situations where an asymmetry of power exists between the parties. Such opt-out options should be ensured in the technology’s design in circumstances where this is necessary toensure compliance with fundamental rights and is necessary in a democratic society. Lethal autonomous weapon systems (LAWS)Currently, an unknown number of countries and industries are researching and developing lethal autonomousweapon systems, ranging from missiles capable of selective targeting to learning machines with cognitive skills todecide whom, when and where to fight without human intervention. This raises fundamental ethical concerns, suchas the fact that it could lead to an uncontrollable arms race on a historically unprecedented level, and createmilitary contexts in which human control is almost entirely relinquished and the risks of malfunction are notaddressed. The European Parliament has called for the urgent development of a common, legally binding positionaddressing ethical and legal questions of human control, oversight, accountability and implementation ofinternational human rights law, international humanitarian law and military strategies.75 Recalling the European Union’s aim to promote peace as enshrined in Article 3 of the Treaty of the European Union, we stand with, andlook to support, the Parliament’s resolution of 12 September 2018 and all related efforts on LAWS.71 In this regard, Article 6 of the GDPR can be recalled, which provides, among other things, that processing of data shall only belawful if it has a valid legal basis.72 As current mechanisms for giving informed consent in the internet show, consumers typically give consent without meaningfulconsideration. Hence, they can hardly be classified as practical.73 Madary & Metzinger (2016). Real Virtuality: A Code of Ethical Conduct. Recommendations for Good Scientific Practice and the Consumers of VR-Technology. Frontiers in Robotics and AI, 3(3).74 This also applies to AI-driven avatars.75 European Parliament’s Resolution 2018/2752(RSP). Potential longer-term concerns AI development is still domain-specific and requires well-trained human scientists and engineers to precisely specifyits targets. However, extrapolating into the future with a longer time horizon, certain critical long-term concerns canbe hypothesized.76 A risk-based approach suggests that these concerns should be kept into consideration in view ofpossible unknown unknowns and “black swans.”77 The high-impact nature of these concerns, combined with thecurrent uncertainty in corresponding developments, calls for regular assessments of these topics. D. CONCLUSIONThis document constitutes the AI Ethics Guidelines produced by the High-Level Expert Group on Artificial Intelligence (AI HLEG). We recognise the positive impact that AI systems already have and will continue having, both commercially andsocietally. However, we are equally concerned to ensure that the risks and other adverse impacts with which thesetechnologies are associated are properly and proportionately handled. AI is a technology that is both transformativeand disruptive, and its evolution over the last several years has been facilitated by the availability of enormousamounts of digital data, major technological advances in computational power and storage capacity, as well assignificant scientific and engineering innovation in AI methods and tools. AI systems will continue to impact societyand citizens in ways that we cannot yet imagine. In this context, it is important to build AI systems that are worthy of trust, since human beings will only be able toconfidently and fully reap its benefits when the technology, including the processes and people behind thetechnology, are trustworthy. When drafting these Guidelines, Trustworthy AI has, therefore, been our foundationalambition. Trustworthy AI has three components: (1) it should be lawful, ensuring compliance with all applicable laws andregulations, (2) it should be ethical, ensuring adherence to ethical principles and values and (3) it should be robust, both from a technical and social perspective since to ensure that, even with good intentions, AI systems do notcause any unintentional harm. Each component is necessary but not sufficient to achieve Trustworthy AI. Ideally, allthree components work in harmony and overlap in their operation. Where tensions arise, we should endeavour toalign them. In Chapter I, we articulated the fundamental rights and a corresponding set of ethical principles that are crucial in an AI-context. In Chapter II, we listed seven key requirements that AI systems should meet in order to realise Trustworthy AI. We proposed technical and non-technical methods that can help with their implementation. Finally, in Chapter III we provided a Trustworthy AI assessment list that can help operationalising the seven requirements. Ina final section, we provided examples of beneficial opportunities and critical concerns raised by AI systems, onwhich we hope to stimulate further discussion. Europe has a unique vantage point based on its focus on placing the citizen at the heart of its endeavours. This focusis written into the very DNA of the European Union through the Treaties upon which it is built. The currentdocument forms part of a vision that promotes Trustworthy AI which we believe should be the foundation uponwhich Europe can build leadership in innovative, cutting-edge AI systems. This ambitious vision will help securinghuman flourishing of European citizens, both individually and collectively. Our goal is to create a culture of“Trustworthy AI for Europe”, whereby the benefits of AI can be reaped by all in a manner that ensures respect forour foundational values: fundamental rights, democracy and the rule of law.76 While some consider that Artificial General Intelligence, Artificial Consciousness, Artificial Moral Agents, Super-intelligence or Transformative AI can be examples of such long-term concerns (currently non-existent), many others believe these to beunrealistic.77 A black swan event is a very rare, yet high impact, event – so rare, that it might not have been observed. Hence, probability ofoccurrence typically can only be estimated with high uncertainty. GLOSSARYThis glossary pertains to the Guidelines and is meant to help in the understanding of the terms used in thisdocument. Artificial Intelligence or AI systems Artificial intelligence (AI) systems are software (and possibly also hardware) systems designed by humans78 that, given a complex goal, act in the physical or digital dimension by perceiving their environment through dataacquisition, interpreting the collected structured or unstructured data, reasoning on the knowledge, or processingthe information, derived from this data and deciding the best action(s) to take to achieve the given goal. AI systemscan either use symbolic rules or learn a numeric model, and they can also adapt their behaviour by analysing howthe environment is affected by their previous actions. As a scientific discipline, AI includes several approaches and techniques, such as machine learning (of which deeplearning and reinforcement learning are specific examples), machine reasoning (which includes planning, scheduling, knowledge representation and reasoning, search, and optimization), and robotics (which includescontrol, perception, sensors and actuators, as well as the integration of all other techniques into cyber-physicalsystems). A separate document prepared by the AI HLEG and elaborating on the definition of AI used for the purpose of thisdocument is published in parallel, titled "A definition of AI: Main capabilities and scientific disciplines". AI Practitioners By AI practitioners we denote all individuals or organisations that develop (including research, design or providedata for) deploy (including implement) or use AI systems, excluding those that use AI systems in the capacity of end-user or consumer. AI system’s life cycle An AI system’s life cycle encompasses its development (including research, design, data provision, and limited trials), deployment (including implementation) and use phase. Auditability Auditability refers to the ability of an AI system to undergo the assessment of the system’s algorithms, data anddesign processes. This does not necessarily imply that information about business models and Intellectual Propertyrelated to the AI system must always be openly available. Ensuring traceability and logging mechanisms from theearly design phase of the AI system can help enabling the system's auditability. Bias Bias is an inclination of prejudice towards or against a person, object, or position. Bias can arise in many ways in AIsystems. For example, in data-drive AI systems, such as those produced through machine learning, bias in datacollection and training can result in an AI system demonstrating bias. In logic-based AI, such as rule-based systems, bias can arise due to how a knowledge engineer might view the rules that apply in a particular setting. Bias can alsoarise due to online learning and adaptation through interaction. It can also arise through personalisation wherebyusers are presented with recommendations or information feeds that are tailored to the user’s tastes. It does notnecessarily relate to human bias or human-driven data collection. It can arise, for example, through the limitedcontexts in which a system in used, in which case there is no opportunity to generalise it to other contexts. Bias canbe good or bad, intentional or unintentional. In certain cases, bias can result in discriminatory and/or unfairoutcomes, indicated in this document as unfair bias.78 Humans design AI systems directly, but they may also use AI techniques to optimise their design. Ethics Ethics is an academic discipline which is a subfield of philosophy. In general terms, it deals with questions like “Whatis a good action?”, “What is the value of a human life?”, “What is justice?”, or “What is the good life?”. In academicethics, there are four major fields of research: (i) Meta-ethics, mostly concerning the meaning and reference ofnormative sentence, and the question how their truth values can be determined (if they have any); (ii) normativeethics, the practical means of determining a moral course of action by examining the standards for right and wrongaction and assigning a value to specific actions; (iii) descriptive ethics, which aims at an empirical investigation ofpeople's moral behaviour and beliefs; and (iv) applied ethics, concerning what we are obligated (or permitted) to doin a specific (often historically new) situation or a particular domain of (often historically unprecedented)possibilities for action. Applied ethics deals with real-life situations, where decisions have to be made under time-pressure, and often limited rationality. AI Ethics is generally viewed as an example of applied ethics and focuses onthe normative issues raised by the design, development, implementation and use of AI. Within ethical discussions, the terms “moral” and “ethical” are often used. The term “moral” refers to the concrete, factual patterns of behaviour, the customs, and conventions that can be found in specific cultures, groups, orindividuals at a certain time. The term “ethical” refers to an evaluative assessment of such concrete actions andbehaviours from a systematic, academic perspective. Ethical AIIn this document, ethical AI is used to indicate the development, deployment and use of AI that ensures compliancewith ethical norms, including fundamental rights as special moral entitlements, ethical principles and related corevalues. It is the second of the three core elements necessary for achieving Trustworthy AI. Human-Centric AIThe human-centric approach to AI strives to ensure that human values are central to the way in which AI systemsare developed, deployed, used and monitored, by ensuring respect for fundamental rights, including those set out inthe Treaties of the European Union and Charter of Fundamental Rights of the European Union, all of which areunited by reference to a common foundation rooted in respect for human dignity, in which the human being enjoy aunique and inalienable moral status. This also entails consideration of the natural environment and of other livingbeings that are part of the human ecosystem, as well as a sustainable approach enabling the flourishing of futuregenerations to come. Red Teaming Red teaming is the practice whereby a “red team” or independent group challenges an organisation to improve itseffectiveness by assuming an adversarial role or point of view. It is particularly used to help identifying andaddressing potential security vulnerabilities. Reproducibility Reproducibility describes whether an AI experiment exhibits the same behaviour when repeated under the sameconditions. Robust AIRobustness of an AI system encompasses both its technical robustness (appropriate in a given context, such as theapplication domain or life cycle phase) and as well as its robustness from a social perspective (ensuring that the AIsystem duly takes into account the context and environment in which the system operates). This is crucial to ensurethat, even with good intentions, no unintentional harm can occur. Robustness is the third of the three componentsnecessary for achieving Trustworthy AI. Stakeholders By stakeholders we denote all those that research develop, design, deploy or use AI, as well as those that are(directly or indirectly) affected by AI – including but not limited to companies, organisations, researchers, publicservices, institutions, civil society organisations, governments, regulators, social partners, individuals, citizens, workers and consumers. Traceability Traceability of an AI system refers to the capability to keep track of the system’s data, development and deploymentprocesses, typically by means of documented recorded identification. Trust We take the following definition from the literature: “Trust is viewed as: (1) a set of specific beliefs dealing withbenevolence, competence, integrity, and predictability (trusting beliefs); (2) the willingness of one party to dependon another in a risky situation (trusting intention); or (3) the combination of these elements.”79 While “Trust” isusually not a property ascribed to machines, this document aims to stress the importance of being able to trust notonly in the fact that AI systems are legally compliant, ethically adherent and robust, but also that such trust can beascribed to all people and processes involved in the AI system’s life cycle. Trustworthy AITrustworthy AI has three components: (1) it should be lawful, ensuring compliance with all applicable laws andregulations (2) it should be ethical, demonstrating respect for, and ensure adherence to, ethical principles andvalues and (3) it should be robust, both from a technical and social perspective, since, even with good intentions, AIsystems can cause unintentional harm. Trustworthy AI concerns not only the trustworthiness of the AI system itselfbut also comprises the trustworthiness of all processes and actors that are part of the system’s life cycle. Vulnerable Persons and Groups No commonly accepted or widely agreed legal definition of vulnerable persons exists, due to their heterogeneity. What constitutes a vulnerable person or group is often context-specific. Temporary life events (such as childhood orillness), market factors (such as information asymmetry or market power), economic factors (such as poverty), factors linked to one’s identity (such as gender, religion or culture) or other factors can play a role. The Charter of Fundamental Rights of the EU encompasses under Article 21 on non-discrimination the following grounds, whichcan be a reference point amongst others: namely sex, race, colour, ethnic or social origin, genetic features, language, religion or belief, political or any other opinion, membership of a national minority, property, birth, disability, age and sexual orientation. Other articles of law address the rights of specific groups, in addition to thoselisted above. Any such list is not exhaustive, and may change over time. A vulnerable group is a group of personswho share one or several characteristics of vulnerability.79 Siau, K., Wang, W. (2018), Building Trust in Artificial Intelligence, Machine Learning, and Robotics, CUTTER BUSINESSTECHNOLOGY JOURNAL (31), S. 47–53. Pekka Ala-Pietilä, Chair of the AI HLEG Pierre Lucas AI Finland, Huhtamaki, Sanoma Orgalim – Europe’s technology industries Wilhelm Bauer Ieva Martinkenaite Fraunhofer Telenor Urs Bergmann – Co-Rapporteur Thomas Metzinger – Co-Rapporteur Zalando JGU Mainz & European University Association Mária Bieliková Catelijne Muller Slovak University of Technology in Bratislava ALLAI Netherlands & EESCCecilia Bonefeld-Dahl – Co-Rapporteur Markus Noga Digital Europe SAPYann Bonnet Barry O’Sullivan, Vice-Chair of the AI HLEGANSSI University College Cork Loubna Bouarfa Ursula Pachl OKRA BEUCStéphan Brunessaux Nicolas Petit – Co-Rapporteur Airbus University of Liège Raja Chatila Christoph Peylo IEEE Initiative Ethics of Intelligent/Autonomous Systems & Bosch Sorbonne University Mark Coeckelbergh Iris Plöger University of Vienna BDIVirginia Dignum – Co-Rapporteur Stefano Quintarelli Umea University Garden Ventures Luciano Floridi Andrea Renda University of Oxford College of Europe Faculty & CEPSJean-Francois Gagné – Co-Rapporteur Francesca Rossi Element AI IBMChiara Giovannini Cristina San JoséANEC European Banking Federation Joanna Goodey George Sharkov Fundamental Rights Agency Digital SME Alliance Sami Haddadin Philipp Slusallek Munich School of Robotics and MI German Research Centre for AI (DFKI)Gry Hasselbalch Françoise Soulié Fogelman The thinkdotank Data Ethics & Copenhagen University AI Consultant Fredrik Heintz Saskia Steinacker – Co-Rapporteur Linköping University Bayer Fanny Hidvegi Jaan Tallinn Access Now Ambient Sound Investment Eric Hilgendorf Thierry Tingaud University of Würzburg STMicroelectronics Klaus Höckner Jakob Uszkoreit Hilfsgemeinschaft der Blinden und Sehschwachen Google Mari-Noëlle Jégo-Laveissière Aimee Van Wynsberghe – Co-Rapporteur Orange TU Delft Leo Kärkkäinen Thiébaut Weber Nokia Bell Labs ETUCSabine Theresia Köszegi Cecile Wendling TU Wien AXARobert Kroplewski Karen Yeung – Co-Rapporteur Solicitor & Advisor to Polish Government The University of Birmingham Elisabeth Ling RELXUrs Bergmann, Cecilia Bonefeld-Dahl, Virginia Dignum, Jean-François Gagné, Thomas Metzinger, Nicolas Petit, Saskia Steinacker, Aimee Van Wynsberghe and Karen Yeung acted as rapporteurs for this document. Pekka Ala-Pietilä is Chairing the AI HLEG. Barry O'Sullivan is Vice-Chair, coordinating the AI HLEG’s second deliverable. Nozha Boujemaa, Vice-Chair until 1 February 2019 coordinating the first deliverable, also contributed to the content of this document. Nathalie Smuha provided editorial support.