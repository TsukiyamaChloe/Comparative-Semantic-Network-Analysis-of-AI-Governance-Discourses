Introductory note by the Chair and Vice-Chair of the Transparency Chapter. The Transparency Chapter of the Code of Practice describes three Measures which Signatories committo implementing to comply with their transparency obligations under Article 53(1), points (a) and (b), and the corresponding Annexes XI and XII of the AI Act. To facilitate compliance and fulfilment of the commitments contained in Measure 1.1, we includea user-friendly Model Documentation Form which allows Signatories to easily compile the informationrequired by the aforementioned provisions of the AI Act in a single place. The Model Documentation Form indicates for each item whether the information is intended fordownstream providers, the AI Office or national competent authorities. Information intended for the AI Office or national competent authorities is only to be made available following a request from the AI Office, either ex officio or based on a request to the AI Office from national competent authorities. Such requests will state the legal basis and purpose of the request and will concern only items fromthe Form that are strictly necessary for the AI Office to fulfil its tasks under the AI Act at the time ofthe request, or for national competent authorities to exercise their supervisory tasks under the AI Actat the time of the request, in particular to assess compliance of providers high-risk AI systems built ongeneral-purpose AI models where the provider of the system is different from the provider of themodel. In accordance with Article 78 AI Act, the recipients of any of the information contained in the Model Documentation Form are obliged to respect the confidentiality of the information obtained, in particular intellectual property rights and confidential business information or trade secrets, andto put in place adequate and effective cybersecurity measures to protect the security andconfidentiality of the information obtained. Objectives The overarching objective of this Code of Practice (“Code”) is to improve the functioning of theinternal market, to promote the uptake of human-centric and trustworthy artificial intelligence (“AI”), while ensuring a high level of protection of health, safety, and fundamental rights enshrined in the Charter, including democracy, the rule of law, and environmental protection, against harmful effectsof AI in the Union, and to support innovation pursuant to Article 1(1) AI Act. To achieve this overarching objective, the specific objectives of this Code are: A. To serve as a guiding document for demonstrating compliance with the obligations providedfor in Articles 53 and 55 AI Act, while recognising that adherence to the Code does notconstitute conclusive evidence of compliance with these obligations under the AI Act. B. To ensure providers of general-purpose AI models comply with their obligations under the AIAct and to enable the AI Office to assess compliance of providers of general-purpose AImodels who choose to rely on the Code to demonstrate compliance with their obligationsunder the AI Act. Recitals Whereas:(a) The Signatories recognise the particular role and responsibility of providers of general-purpose AI models along the AI value chain, as the models they provide may form the basisfor a range of downstream AI systems, often provided by downstream providers that need agood understanding of the models and their capabilities, both to enable the integration ofsuch models into their products and to fulfil their obligations under the AI Act (see recital 101AI Act).(b) The Signatories recognise that in the case of a fine-tuning or other modification of a general-purpose AI model, where the natural or legal person, public authority, agency or other bodythat modifies the model becomes the provider of the modified model subject to theobligations for providers of general purpose AI models, their Commitments under the Transparency Chapter of the Code should be limited to that modification or fine-tuning, tocomply with the principle of proportionality (see recital 109 AI Act). In this context, Signatories should take into account relevant guidelines by the European Commission.(c) The Signatories recognise that, without exceeding the Commitments under the Transparency Chapter of this Code, when providing information to the AI Office or to downstream providersthey may need to take into account market and technological developments, so that theinformation continues to serve its purpose of allowing the AI Office and national competentauthorities to fulfil their tasks under the AI Act, and downstream providers to integrate the Signatories’ models into AI systems and to comply with their obligations under the AI Act (see Article 56(2), point (a), AI Act). This Chapter of the Code focuses on the documentation obligations from Article 53(1), points (a) and(b), AI Act that are applicable to all providers of general-purpose AI models (without prejudice to theexception laid down in Article 53(2) AI Act), namely those concerning Annex XI, Section 1, and Annex XII AI Act. The documentation obligations concerning Annex XI, Section 2, AI Act, applicable only toproviders of general-purpose AI models with systemic risk are covered by Measure 10.1 of the Safetyand Security Chapter of this Code. Commitment 1 Documentation LEGAL TEXT: Articles 53(1)(a), 53(1)(b), 53(2), 53(7), and Annexes XI and XII AI Act In order to fulfil the obligations in Article 53(1), points (a) and (b), AI Act, Signatories commit todrawing up and keeping up-to-date model documentation in accordance with Measure 1.1, providingrelevant information to providers of AI systems who intend to integrate the general-purpose AI modelinto their AI systems (‘downstream providers’ hereafter), and to the AI Office upon request (possiblyon behalf of national competent authorities upon request to the AI Office when this is strictlynecessary for the exercise of their supervisory tasks under the AI Act, in particular to assess thecompliance of a high-risk AI system built on a general-purpose AI model where the provider of thesystem is different from the provider of the model1), in accordance with Measure 1.2, and ensuringquality, security, and integrity of the documented information in accordance with Measure 1.3. Inaccordance with Article 53(2) AI Act, these Measures do not apply to providers of general-purpose AImodels released under a free and open-source license that satisfy the conditions specified in thatprovision, unless the model is a general-purpose AI model with systemic risk. Measure 1.1 Drawing up and keeping up-to-date model documentation Signatories, when placing a general-purpose AI model on the market, will have documented at leastall the information referred to in the Model Documentation Form below (hereafter this informationis referred to as the ‘Model Documentation’). Signatories may choose to complete the Model Documentation Form provided in the Appendix to comply with this commitment. Signatories will update the Model Documentation to reflect relevant changes in the informationcontained in the Model Documentation, including in relation to updated versions of the same model, while keeping previous versions of the Model Documentation for a period ending 10 years after themodel has been placed on the market. Measure 1.2 Providing relevant information Signatories, when placing a general-purpose AI model on the market, will publicly disclose via theirwebsite, or via other appropriate means if they do not have a website, contact information for the AIOffice and downstream providers to request access to the relevant information contained in the Model Documentation, or other necessary information. Signatories will provide, upon a request from the AI Office pursuant to Articles 91 or 75(3) AI Act forone or more elements of the Model Documentation, or any additional information, that are necessaryfor the AI Office to fulfil its tasks under the AI Act or for national competent authorities to exercisetheir supervisory tasks under the AI Act, in particular to assess compliance of high-risk AI systems builton general-purpose AI models where the provider of the system is different from the provider of themodel,2 the requested information in its most up-to-date form, within the period specified in the AIOffice’s request in accordance with Article 91(4) AI Act.1 See Article 75(1) and (3) AI Act and Article 88(2) AI Act.2 See Article 75(1) and (3) and Article 88(2) AI Act Signatories will provide to downstream providers the information contained in the most up-to-date Model Documentation that is intended for downstream providers, subject to the confidentialitysafeguards and conditions provided for under Articles 53(7) and 78 AI Act. Furthermore, withoutprejudice to the need to observe and protect intellectual property rights and confidential businessinformation or trade secrets in accordance with Union and national law, Signatories will provideadditional information upon a request from downstream providers insofar as such information isnecessary to enable them to have a good understanding of the capabilities and limitations of thegeneral-purpose AI model relevant for its integration into the downstream providers’ AI system andto enable those downstream providers to comply with their obligations pursuant to the AI Act. Signatories will provide such information within a reasonable timeframe, and no later than 14 days ofreceiving the request save for exceptional circumstances. Signatories are encouraged to consider whether the documented information can be disclosed, inwhole or in part, to the public to promote public transparency. Some of this information may also berequired in a summarised form as part of the training content summary that providers must makepublicly available under Article 53(1), point (d), AI Act, according to a template to be provided by the AI Office. Measure 1.3 Ensuring quality, integrity, and security of information Signatories will ensure that the documented information is controlled for quality and integrity, retained as evidence of compliance with obligations in the AI Act, and protected from unintendedalterations. In the context of drawing-up, updating, and controlling the quality and security of theinformation and records, Signatories are encouraged to follow the established protocols and technicalstandards. Model Documentation Form Below is a static version of the Model Documentation Form. In this version, the input fields cannot befilled in. A fillable version of this form is separately available. Objectives The overarching objective of this Code of Practice (“Code”) is to improve the functioning of theinternal market, to promote the uptake of human-centric and trustworthy artificial intelligence (“AI”), while ensuring a high level of protection of health, safety, and fundamental rights enshrined in the Charter, including democracy, the rule of law, and environmental protection, against harmful effectsof AI in the Union, and to support innovation pursuant to Article 1(1) AI Act. To achieve this overarching objective, the specific objectives of this Code are: A. To serve as a guiding document for demonstrating compliance with the obligations providedfor in Articles 53 and 55 AI Act, while recognising that adherence to the Code does notconstitute conclusive evidence of compliance with these obligations under the AI Act. B. To ensure providers of general-purpose AI models comply with their obligations under the AIAct and to enable the AI Office to assess compliance of providers of general-purpose AImodels who choose to rely on the Code to demonstrate compliance with their obligationsunder the AI Act. Recitals Whereas:(a) This Chapter aims to contribute to the proper application of the obligation laid down in Article53(1), point (c), of the AI Act pursuant to which providers that place general-purpose AImodels on the Union market must put in place a policy to comply with Union law on copyrightand related rights, and in particular to identify and comply with, including through state-of-the-art technologies, a reservation of rights expressed by rightsholders pursuant to Article 4(3) of Directive (EU) 2019/790. While Signatories will implement the Measures setout in this Chapter in order to demonstrate compliance with Article 53(1), point (c), of the AIAct, adherence to the Code does not constitute compliance with Union law on copyright andrelated rights.(b) This Chapter in no way affects the application and enforcement of Union law on copyrightand related right which is for the courts of Member States and ultimately the Court of Justiceof the European Union to interpret.(c) The Signatories hereby acknowledge that Union law on copyright and related rights:(i) is provided for in directives addressed to Member States and that for presentpurposes Directive 2001/29/EC, Directive (EU) 2019/790 and Directive 2004/48/ECare the most relevant;(ii) provides for exclusive rights that are preventive in nature and thus is based on priorconsent save where an exception or limitation applies;(iii) provides for an exception or limitation for text and data mining in Article 4(1) of Directive (EU) 2019/790 which shall apply on conditions of lawful access and that theuse of works and other subject matter referred to in that paragraph has not beenexpressly reserved by their rightsholders in an appropriate manner pursuant to Article 4(3) of Directive (EU) 2019/790.(d) The commitments in this Chapter that require proportionate measures should becommensurate and proportionate to the size of providers, taking due account of the interestsof SMEs, including startups.(e) This Chapter does not affect agreements between the Signatories and rightsholdersauthorising the use of works and other protected subject matter.(f) The commitments in this Chapter to demonstrate compliance with the obligation under Article 53(1), point (c), of the AI Act are complementary to the obligation of providers under Article 53(1), point (d), of the AI Act to draw up and make publicly available sufficientlydetailed summaries about the content used by the Signatories for the training of theirgeneral-purpose AI models, according to a template to be provided by the AI Office. Commitment 1 Copyright policy LEGAL TEXT: Article 53(1)(c) AI Act(1) In order to demonstrate compliance with their obligation pursuant to Article 53(1), point (c)of the AI Act to put in place a policy to comply with Union law on copyright and related rights, and in particular to identify and comply with, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790, Signatories commit to drawing up, keeping up-to-date and implementing such a copyrightpolicy. The Measures below do not affect compliance with Union law on copyright and relatedrights. They set out commitments by which the Signatories can demonstrate compliance withthe obligation to put in place a copyright policy for their general-purpose AI models they placeon the Union market.(2) In addition, the Signatories remain responsible for verifying that the Measures included intheir copyright policy as outlined below comply with Member States’ implementation of Union law on copyright and related rights, in particular but not only Article 4(3) of Directive(EU) 2019/790, before carrying out any copyright-relevant act in the territory of the relevant Member State as failure to do so may give rise to liability under Union law on copyright andrelated rights. Measure 1.1 Draw up, keep up-to-date and implement a copyright policy(1) Signatories will draw up, keep up-to-date and implement a policy to comply with Union lawon copyright and related rights for all general-purpose AI models they place on the Unionmarket. Signatories commit to describe that policy in a single document incorporating the Measures set out in this Chapter. Signatories will assign responsibilities within theirorganisation for the implementation and overseeing of this policy.(2) Signatories are encouraged to make publicly available and keep up-to-date a summary oftheir copyright policy. Measure 1.2 Reproduce and extract only lawfully accessible copyright-protected contentwhen crawling the World Wide Web(1) In order to help ensure that Signatories only reproduce and extract lawfully accessible worksand other protected subject matter if they use web-crawlers or have such web-crawlers usedon their behalf to scrape or otherwise compile data for the purpose of text and data miningas defined in Article 2(2) of Directive (EU) 2019/790 and the training of their general-purpose AI models, Signatories commit: a) not to circumvent effective technological measures as defined in Article 6(3) of Directive 2001/29/EC that are designed to prevent or restrict unauthorised acts inrespect of works and other protected subject matter, in particular by respecting anytechnological denial or restriction of access imposed by subscription models orpaywalls, andb) to exclude from their web-crawling websites that make available to the public contentand which are, at the time of web-crawling, recognised as persistently and repeatedlyinfringing copyright and related rights on a commercial scale by courts or publicauthorities in the European Union and the European Economic Area. For the purposeof compliance with this measure, a dynamic list of hyperlinks to lists of these websitesissued by the relevant bodies in the European Union and the European Economic Areawill be made publicly available on an EU website. Measure 1.3 Identify and comply with rights reservations when crawling the World Wide Web(1) In order to help ensure that Signatories will identify and comply with, including through state-of-the-art technologies, machine-readable reservations of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790 if they use web-crawlers or have such web-crawlersused on their behalf to scrape or otherwise compile data for the purpose of text and datamining as defined in Article 2(2) of Directive (EU) 2019/790 and the training of their general-purpose AI models, Signatories commit: a) to employ web-crawlers that read and follow instructions expressed in accordancewith the Robot Exclusion Protocol (robots. txt), as specified in the Internet Engineering Task Force (IETF) Request for Comments No. 9309, and any subsequent version of this Protocol for which the IETF demonstrates that it is technically feasible andimplementable by AI providers and content providers, including rightsholders, andb) to identify and comply with other appropriate machine-readable protocols to expressrights reservations pursuant to Article 4(3) of Directive (EU) 2019/790, for examplethrough asset-based or location-based metadata, that have either have been adoptedby international or European standardisation organisations, or are state-of-the-art, including technically implementable, and widely adopted by rightsholders, considering different cultural sectors, and generally agreed through an inclusiveprocess based on bona fide discussions to be facilitated at EU level with theinvolvement of rightsholders, AI providers and other relevant stakeholders as a moreimmediate solution, while anticipating the development of standards.(2) This commitment does not affect the right of rightsholders to expressly reserve the use ofworks and other protected subject matter for the purposes of text and data mining pursuantto Article 4(3) of Directive (EU) 2019/790 in any appropriate manner, such as machine-readable means in the case of content made publicly available online or by other means. Furthermore, this commitment does not affect the application of Union law on copyright andrelated rights to protected content scraped or crawled from the internet by third parties andused by Signatories for the purpose of text and data mining and the training of their general-purpose AI models, in particular with regard to rights reservations expressed pursuant to Article 4(3) of Directive (EU) 2019/790.(3) Signatories are encouraged to support the processes referred to in the first paragraph, points(a) and (b), of this Measure and engage on a voluntary basis in bona fide discussions withrightsholders and other relevant stakeholders, with the aim to develop appropriate machine-readable standards and protocols to express a rights reservation pursuant to Article 4(3) of Directive (EU) 2019/790.(4) Signatories commit to take appropriate measures to enable affected rightsholders to obtaininformation about the web crawlers employed, their robots. txt features and other measuresthat a Signatory adopts to identify and comply with rights reservations expressed pursuantto Article 4(3) of Directive (EU) 2019/790 at the time of crawling by making public suchinformation and by providing a means for affected rightsholders to be automatically notifiedwhen such information is updated (such as by syndicating a web feed) without prejudice tothe right of information provided for in Article 8 of Directive 2004/48/EC.(5) Signatories that also provide an online search engine as defined in Article 3, point (j), of Regulation (EU) 2022/2065 or control such a provider are encouraged to take appropriatemeasures to ensure that their compliance with a rights reservation in the context of text anddata mining activities and the training of general-purpose AI models referred to in the firstparagraph of this Measure does not directly lead to adverse effects on the indexing of thecontent, domain(s) and/or URL(s), for which a rights reservation has been expressed, in theirsearch engine. Measure 1.4 Mitigate the risk of copyright-infringing outputs(1) In order to mitigate the risk that a downstream AI system, into which a general-purpose AImodel is integrated, generates output that may infringe rights in works or other subjectmatter protected by Union law on copyright or related rights, Signatories commit: a) to implement appropriate and proportionate technical safeguards to prevent theirmodels from generating outputs that reproduce training content protected by Unionlaw on copyright and related rights in an infringing manner, andb) to prohibit copyright-infringing uses of a model in their acceptable use policy, termsand conditions, or other equivalent documents, or in case of general-purpose AImodels released under free and open source licenses to alert users to the prohibitionof copyright infringing uses of the model in the documentation accompanying themodel without prejudice to the free and open source nature of the license.(2) This Measure applies irrespective of whether a Signatory vertically integrates the model intoits own AI system(s) or whether the model is provided to another entity based on contractualrelations. Measure 1.5 Designate a point of contact and enable the lodging of complaints(1) Signatories commit to designate a point of contact for electronic communication withaffected rightsholders and provide easily accessible information about it.(2) Signatories commit to put a mechanism in place to allow affected rightsholders and theirauthorised representatives, including collective management organisations, to submit, byelectronic means, sufficiently precise and adequately substantiated complaints concerningthe non-compliance of Signatories with their commitments pursuant to this Chapter andprovide easily accessible information about it. Signatories will act on complaints in a diligent, non-arbitrary manner and within a reasonable time, unless a complaint is manifestlyunfounded or the Signatory has already responded to an identical complaint by the samerightsholder. This commitment does not affect the measures, remedies and sanctionsavailable to enforce copyright and related rights under Union and national law. Objectives The overarching objective of this Code of Practice (“Code”) is to improve the functioning of theinternal market, to promote the uptake of human-centric and trustworthy artificial intelligence (“AI”), while ensuring a high level of protection of health, safety, and fundamental rights enshrined in the Charter, including democracy, the rule of law, and environmental protection, against harmful effectsof AI in the Union, and to support innovation pursuant to Article 1(1) AI Act. To achieve this overarching objective, the specific objectives of this Code are: A. To serve as a guiding document for demonstrating compliance with the obligations providedfor in Articles 53 and 55 AI Act, while recognising that adherence to the Code does notconstitute conclusive evidence of compliance with these obligations under the AI Act. B. To ensure providers of general-purpose AI models comply with their obligations under the AIAct and to enable the AI Office to assess compliance of providers of general-purpose AImodels who choose to rely on the Code to demonstrate compliance with their obligationsunder the AI Act. Recitals Whereas:(a) Principle of Appropriate Lifecycle Management. The Signatories recognise that providers ofgeneral-purpose AI models with systemic risk should continuously assess and mitigatesystemic risks, taking appropriate measures along the entire model lifecycle (including duringdevelopment that occurs before and after a model has been placed on the market), cooperating with and taking into account relevant actors along the AI value chain (such asstakeholders likely to be affected by the model), and ensuring their systemic riskmanagement is made future-proof by regular updates in response to improving and emergingmodel capabilities (see recitals 114 and 115 AI Act). Accordingly, the Signatories recognisethat implementing appropriate measures will often require Signatories to adopt at least thestate of the art, unless systemic risk can be conclusively ruled out with a less advancedprocess, measure, methodology, method, or technique. Systemic risk assessment is a multi-step process and model evaluations, referring to a range of methods used in assessingsystemic risks of models, are integral along the entire model lifecycle. When systemic riskmitigations are implemented, the Signatories recognise the importance of continuouslyassessing their effectiveness.(b) Principle of Contextual Risk Assessment and Mitigation. The Signatories recognise that this Safety and Security Chapter (“Chapter”) is only relevant for providers of general-purpose AImodels with systemic risk and not AI systems. However, the Signatories also recognise thatthe assessment and mitigation of systemic risks should include, as reasonably foreseeable, the system architecture, other software into which the model may be integrated, and thecomputing resources available at inference time because of their importance to the model’seffects, for example by affecting the effectiveness of safety and security mitigations.(c) Principle of Proportionality to Systemic Risks. The Signatories recognise that the assessmentand mitigation of systemic risks should be proportionate to the risks (Article 56(2), point (d)AI Act). Therefore, the degree of scrutiny in systemic risk assessment and mitigation, inparticular the level of detail in documentation and reporting, should be proportionate to thesystemic risks at the relevant points along the entire model lifecycle. The Signatoriesrecognise that while systemic risk assessment and mitigation is iterative and continuous, theyneed not duplicate assessments that are still appropriate to the systemic risks stemming fromthe model.(d) Principle of Integration with Existing Laws. The Signatories recognise that this Chapter formspart of, and is complemented by, other Union laws. The Signatories further recognise thatconfidentiality (including commercial confidentiality) obligations are preserved to the extentrequired by Union law, and information sent to the European AI Office (“AI Office”) inadherence to this Chapter will be treated pursuant to Article 78 AI Act. Additionally, the Signatories recognise that information about future developments and future businessactivities that they submit to the AI Office will be understood as subject to change. The Signatories further recognise that the Measure under this Chapter to promote a healthy riskculture (Measure 8.3) is without prejudice to any obligations arising from Directive (EU)2019/1937 on the protection of whistleblowers and implementing laws of Member States inconjunction with Article 87 AI Act. The Signatories also recognise that they may be able torely on international standards to the extent they cover the provisions of this Chapter.(e) Principle of Cooperation. The Signatories recognise that systemic risk assessment andmitigation merit significant investment of time and resources. They recognise the advantagesof collaborative efficiency, e. g. by sharing model evaluations methods and/or infrastructure. The Signatories further recognise the importance of cooperation with licensees, downstreammodifiers, and downstream providers in systemic risk assessment and mitigation, and ofengaging expert or lay representatives of civil society, academia, and other relevantstakeholders in understanding the model effects. The Signatories recognise that suchcooperation may involve entering into agreements to share information relevant to systemicrisk assessment and mitigation, while ensuring proportionate protection of sensitiveinformation and compliance with applicable Union law. The Signatories further recognise theimportance of cooperating with the AI Office (Article 53(3) AI Act) to foster collaborationbetween providers of general-purpose AI models with systemic risk, researchers, andregulatory bodies to address emerging challenges and opportunities in the AI landscape.(f) Principle of Innovation in AI Safety and Security. The Signatories recognise that determiningthe most effective methods for understanding and ensuring the safety and security ofgeneral-purpose AI models with systemic risk remains an evolving challenge. The Signatoriesrecognise that this Chapter should encourage providers of general-purpose AI models withsystemic risk to advance the state of the art in AI safety and security and related processesand measures. The Signatories recognise that advancing the state of the art also includesdeveloping targeted methods that specifically address risks while maintaining beneficialcapabilities (e. g. mitigating biosecurity risks without unduly reducing beneficial biomedicalcapabilities), acknowledging that such precision demands greater technical effort andinnovation than less targeted methods. The Signatories further recognise that if providers ofgeneral-purpose AI models with systemic risk can demonstrate equal or superior safety orsecurity outcomes through alternative means that achieve greater efficiency, suchinnovations should be recognised as advancing the state of the art in AI safety and securityand meriting consideration for wider adoption.(g) Precautionary Principle. The Signatories recognise the important role of the Precautionary Principle, particularly for systemic risks for which the lack or quality of scientific data does notyet permit a complete assessment. Accordingly, the Signatories recognise that theextrapolation of current adoption rates and research and development trajectories of modelsshould be taken into account for the identification of systemic risks.(h) Small and medium enterprises (“SMEs”) and small mid-cap enterprises (“SMCs”). Toaccount for differences between providers of general-purpose AI models with systemic riskregarding their size and capacity, simplified ways of compliance for SMEs and SMCs, includingstartups, should be possible as proportionate. For example, SMEs and SMCs may beexempted from some reporting commitments (Article 56(5) AI Act). Signatories that are SMEsor SMCs and are exempted from reporting commitments recognise that they maynonetheless voluntarily adhere to them.(i) Interpretation. The Signatories recognise that all Commitments and Measures shall beinterpreted in light of the objective to assess and mitigate systemic risks. The Signatoriesfurther recognise that given the rapid pace of AI development, purposive interpretationfocused on systemic risk assessment and mitigation is particularly important to ensure this Chapter remains effective, relevant, and future-proof. Additionally, any term appearing inthis Chapter that is defined in the Glossary for this Chapter has the meaning set forth in that Glossary. The Signatories recognise that Appendix 1 should be interpreted, in instances ofdoubt, in good faith in light of: (1) the probability and severity of harm pursuant to thedefinition of ‘risk’ in Article 3(2) AI Act; and (2) the definition of ‘systemic risk’ in Article 3(65)AI Act. The Signatories recognise that this Chapter is to be interpreted in conjunction and inaccordance with any AI Office guidance on the AI Act.(j) Serious Incident Reporting. The Signatories recognise that the reporting of a serious incidentis not an admission of wrongdoing. Further, they recognise that relevant information aboutserious incidents cannot be kept track of, documented, and reported at the model level onlyin retrospect after a serious incident has occurred. The information that could directly orindirectly lead up to such an event is often dispersed and may be lost, overwritten, orfragmented by the time Signatories become aware of a serious incident. This justifies theestablishment of processes and measures to keep track of and document relevantinformation before serious incidents occur. Commitment 1 Safety and Security Framework LEGAL TEXT: Articles 55(1) and 56(5), and recitals 110, 114, and 115 AI Act Signatories commit to adopting a state-of-the-art Safety and Security Framework (“Framework”). Thepurpose of the Framework is to outline the systemic risk management processes and measures that Signatories implement to ensure the systemic risks stemming from their models are acceptable. Signatories commit to a Framework adoption process that involves three steps:(1) creating the Framework (as specified in Measure 1.1);(2) implementing the Framework (as specified in Measure 1.2); and(3) updating the Framework (as specified in Measure 1.3). Further, Signatories commit to notifying the AI Office of their Framework (as specified in Measure1.4). Figure 1. Process for creating, implementing, and updating Frameworks. The text of the Commitments and Measures takes precedence. Measure 1.1 Creating the Framework Signatories will create a state-of-the-art Framework, taking into account the models they aredeveloping, making available on the market, and/or using. The Framework will contain a high-level description of implemented and planned processes andmeasures for systemic risk assessment and mitigation to adhere to this Chapter. In addition, the Framework will contain:(1) a description and justification of the trigger points and their usage, at which the Signatorieswill conduct additional lighter-touch model evaluations along the entire model lifecycle, asspecified in Measure 1.2, second paragraph, point (1)(a);(2) for the Signatories’ determination of whether systemic risk is considered acceptable, asspecified in Commitment 4:(a) a description and justification of the systemic risk acceptance criteria, including thesystemic risk tiers, and their usage as specified in Measure 4.1;(b) a high-level description of what safety and security mitigations Signatories wouldneed to implement once each systemic risk tier is reached;(c) for each systemic risk that Signatories defined systemic risk tiers for as specified in Measure 4.1, estimates of timelines when Signatories reasonably foresee that theywill have a model that exceeds the highest systemic risk tier already reached by anyof their existing models. Such estimates: (i) may consist of time ranges or probabilitydistributions; and (ii) may take into account aggregate forecasts, surveys, and otherestimates produced with other providers. Further, such estimates will be supportedby justifications, including underlying assumptions and uncertainties; and(d) a description of whether and, if so, by what process input from external actors, including governments, influences proceeding with the development, makingavailable on the market, and/or use of the Signatories’ models as specified in Measure 4.2, other than as the result of independent external evaluations;(3) a description of how systemic risk responsibility is allocated for the processes by whichsystemic risk is assessed and mitigated as specified in Commitment 8; and(4) a description of the process by which Signatories will update the Framework, including howthey will determine that an updated Framework is confirmed, as specified in Measure 1.3. Signatories will have confirmed the Framework no later than four weeks after having notified the Commission pursuant to Article 52(1) AI Act and no later than two weeks before placing the modelon the market. Measure 1.2 Implementing the Framework Signatories will implement the processes and measures outlined in their Framework as specified inthe following paragraphs. Along the entire model lifecycle, Signatories will continuously:(1) assess the systemic risks stemming from the model by:(a) conducting lighter-touch model evaluations that need not adhere to Appendix 3 (e. g. automated evaluations) at appropriate trigger points defined in terms of, e. g. time, training compute, development stages, user access, inference compute, and/oraffordances;(b) conducting post-market monitoring after placing the model on the market, asspecified in Measure 3.5;(c) taking into account relevant information about serious incidents (pursuant to Commitment 9); and(d) increasing the breadth and/or depth of assessment or conducting a full systemic riskassessment and mitigation process that is specified in the following paragraph, basedon the results of points (a), (b), and (c); and(2) implement systemic risk mitigations taking into account the results of point (1), includingaddressing serious incidents as appropriate. Figure 2. Illustrative timeline of systemic risk assessment and mitigation along the model lifecycle. The text of the Commitments and Measures takes precedence. In addition, Signatories will implement a full systemic risk assessment and mitigation process thatinvolves four steps, without needing to duplicate parts of the model’s previous systemic riskassessments that are still appropriate:(1) identifying the systemic risks stemming from the model as specified in Commitment 2;(2) analysing each identified systemic risk as specified in Commitment 3;(3) determining whether the systemic risks stemming from the model are acceptable as specifiedin Measure 4.1; and(4) if the systemic risks stemming from the model are not determined to be acceptable, implementing safety and/or security mitigations as specified in Commitments 5 and 6, andre-assessing the systemic risks stemming from the model starting from point (1), as specifiedin Measure 4.2. Signatories will conduct such a full systemic risk assessment and mitigation process at least beforeplacing the model on the market and whenever the conditions specified in Measure 7.6, first andthird paragraph, are met. Signatories will report their implemented measures and processes to the AI Office as specified in Commitment 7. Figure 3. Full systemic risk assessment and mitigation process. The text of the Commitments and Measures takes precedence. Measure 1.3 Updating the Framework Signatories will update the Framework as appropriate, including without undue delay after a Framework assessment (specified in the following paragraphs), to ensure the information in Measure1.1 is kept up-to-date and the Framework is at least state-of-the-art. For any update of the Framework, Signatories will include a changelog, describing how and why the Framework has beenupdated, along with a version number and the date of change. Signatories will conduct an appropriate Framework assessment, if they have reasonable grounds tobelieve that the adequacy of their Framework and/or their adherence thereto has been or will bematerially undermined, or every 12 months starting from their placing of the model on the market, whichever is sooner. Examples of such grounds are:(1) how the Signatories develop models will change materially, which can be reasonably foreseento lead to the systemic risks stemming from at least one of their models not being acceptable;(2) serious incidents and/or near misses involving their models or similar models that are likelyto indicate that the systemic risks stemming from at least one of their models are notacceptable have occurred; and/or(3) the systemic risks stemming from at least one of their models have changed or are likely tochange materially, e. g. safety and/or security mitigations have become or are likely tobecome materially less effective, or at least one of their models has developed or is likely todevelop materially changed capabilities and/or propensities. A Framework assessment will include the following:(1) Framework adequacy: An assessment of whether the processes and measures in the Framework are appropriate for the systemic risks stemming from the Signatories’ models. This assessment will take into account how the models are currently being developed, made available on the market, and/or used, and how they are expected to be developed, made available on the market, and/or used over the next 12 months.(2) Framework adherence: An assessment focused on the Signatories’ adherence to the Framework, including: (a) any instances of, and reasons for, non-adherence to the Frameworksince the last Framework assessment; and (b) any measures, including safety and securitymitigations, that need to be implemented to ensure continued adherence to the Framework. If point(s) (a) and/or (b) give rise to risks of future non-adherence, Signatories will makeremediation plans as part of their Framework assessment. Measure 1.4 Framework notifications Signatories will provide the AI Office with (unredacted) access to their Framework, and updatesthereof, within five business days of either being confirmed. Commitment 2 Systemic risk identification LEGAL TEXT: Article 55(1) and recital 110 AI Act Signatories commit to identifying the systemic risks stemming from the model. The purpose ofsystemic risk identification includes facilitating systemic risk analysis (pursuant to Commitment 3) andsystemic risk acceptance determination (pursuant to Commitment 4). Systemic risk identification involves two elements:(1) following a structured process to identify the systemic risks stemming from the model (asspecified in Measure 2.1); and(2) developing systemic risk scenarios for each identified systemic risk (as specified in Measure 2.2). Figure 4. Systemic risk identification process. The text of the Commitments and Measures takes precedence. Measure 2.1 Systemic risk identification process Signatories will identify:(1) the systemic risks obtained through the following process:(a) compiling a list of risks that could stem from the model and be systemic, based onthe types of risks in Appendix 1.1, taking into account:(i) model-independent information (pursuant to Measure 3.1);(ii) relevant information about the model and similar models, includinginformation from post-market monitoring (pursuant to Measure 3.5), andinformation about serious incidents and near misses (pursuant to Commitment 9); and(iii) any other relevant information communicated directly or via public releasesto the Signatory by the AI Office, the Scientific Panel of Independent Experts, or other initiatives, such as the International Network of AI Safety Institutes, endorsed for this purpose by the AI Office;(b) analysing relevant characteristics of the risks compiled pursuant to point (a), such astheir nature (based on Appendix 1.2) and sources (based on Appendix 1.3); and(c) identifying, based on point (b), the systemic risks stemming from the model; and(2) the specified systemic risks in Appendix 1.4. Measure 2.2 Systemic risk scenarios Signatories will develop appropriate systemic risk scenarios, including regarding the number and levelof detail of these systemic risk scenarios, for each identified systemic risk (pursuant to Measure 2.1). Commitment 3 Systemic risk analysis LEGAL TEXT: Article 55(1) and recital 114 AI Act Signatories commit to analysing each identified systemic risk (pursuant to Commitment 2). Thepurpose of systemic risk analysis includes facilitating systemic risk acceptance determination(pursuant to Commitment 4). Systemic risk analysis involves five elements for each identified systemic risk, which may overlap andmay need to be implemented recursively:(1) gathering model-independent information (as specified in Measure 3.1);(2) conducting model evaluations (as specified in Measure 3.2);(3) modelling the systemic risk (as specified in Measure 3.3); and(4) estimating the systemic risk (as specified in Measure 3.4); while(5) conducting post-market monitoring (as specified in Measure 3.5). Measure 3.1 Model-independent information Signatories will gather model-independent information relevant to the systemic risk. Signatories will search for and gather such information with varying degrees of breadth and depthappropriate for the systemic risk, using methods such as:(1) web searches (e. g. making use of open-source intelligence methods in collecting andanalysing information gathered from open sources);(2) literature reviews;(3) market analyses (e. g. focused on capabilities of other models available on the market);(4) reviews of training data (e. g. for indications of data poisoning or tampering);(5) reviewing and analysing historical incident data and incident databases;(6) forecasting of general trends (e. g. forecasts concerning the development of algorithmicefficiency, compute use, data availability, and energy use);(7) expert interviews and/or panels; and/or(8) lay interviews, surveys, community consultations, or other participatory research methodsinvestigating, e. g. the effects of models on natural persons, including vulnerable groups. Measure 3.2 Model evaluations Signatories will conduct at least state-of-the-art model evaluations in the modalities relevant to thesystemic risk to assess the model’s capabilities, propensities, affordances, and/or effects, as specifiedin Appendix 3. Signatories will ensure that such model evaluations are designed and conducted using methods thatare appropriate for the model and the systemic risk, and include open-ended testing of the model toimprove the understanding of the systemic risk, with a view to identifying unexpected behaviours, capability boundaries, or emergent properties. Examples of model evaluation methods are: Q&A sets, task-based evaluations, benchmarks, red-teaming and other methods of adversarial testing, humanuplift studies, model organisms, simulations, and/or proxy evaluations for classified materials. Further, the design of the model evaluations will be informed by the model-independent informationgathered pursuant to Measure 3.1. Measure 3.3 Systemic risk modelling Signatories will conduct systemic risk modelling for the systemic risk. To this end, Signatories will:(1) use at least state-of-the-art risk modelling methods;(2) build on the systemic risk scenarios developed pursuant to Measure 2.2; and(3) take into account at least the information gathered pursuant to Measure 2.1 and this Commitment. Measure 3.4 Systemic risk estimation Signatories will estimate the probability and severity of harm for the systemic risk. Signatories will use at least state-of-the-art risk estimation methods and take into account at leastthe information gathered pursuant to Commitment 2, this Commitment, and Commitment 9. Estimates of systemic risk will be expressed as a risk score, risk matrix, probability distribution, or inother adequate formats, and may be quantitative, semi-quantitative, and/or qualitative. Examples ofsuch estimates of systemic risks are: (1) a qualitative systemic risk score (e. g. “moderate” or “critical”);(2) a qualitative systemic risk matrix (e. g. “probability: unlikely” x “impact: high”); and/or (3) aquantitative systemic risk matrix (e. g. “X-Y%” x “X-Y EUR damage”). Measure 3.5 Post-market monitoring Signatories will conduct appropriate post-market monitoring to gather information relevant toassessing whether the systemic risk could be determined to not be acceptable (pursuant to Measure4.1) and to inform whether a Model Report update is necessary (pursuant to Measure 7.6). Further, Signatories will use best efforts to conduct post-market monitoring to gather information relevant toproducing estimates of timelines (pursuant to Measure 1.1, point (2)(c)). To these ends, post-market monitoring will:(1) gather information about the model’s capabilities, propensities, affordances, and/or effects;(2) take into account the exemplary methods listed below; and(3) if Signatories themselves provide and/or deploy AI systems that integrate their own model, include monitoring the model as part of these AI systems. The following are examples of post-market monitoring methods for the purpose of point (2) above:(1) collecting end-user feedback;(2) providing (anonymous) reporting channels;(3) providing (serious) incident reporting forms;(4) providing bug bounties;(5) establishing community-driven model evaluations and public leaderboards;(6) conducting frequent dialogues with affected stakeholders;(7) monitoring software repositories, known malware, public forums, and/or social media forpatterns of use;(8) supporting the scientific study of the model’s capabilities, propensities, affordances, and/oreffects in collaboration with academia, civil society, regulators, and/or independentresearchers;(9) implementing privacy-preserving logging and metadata analysis techniques of the model’sinputs and outputs using, e. g. watermarks, metadata, and/or other at least state-of-the-artprovenance techniques;(10) collecting relevant information about breaches of the model’s use restrictions andsubsequent incidents arising from such breaches; and/or(11) monitoring aspects of models that are relevant for assessing and mitigating systemic risk andare not transparent to third parties, e. g. hidden chains-of-thought for models for which theparameters are not publicly available for download. To facilitate post-market monitoring, Signatories will provide an adequate number of independentexternal evaluators with adequate free access to:(1) the model’s most capable model version(s) with regard to the systemic risk that is madeavailable on the market;(2) the chains-of-thought of the model version(s) in point (1), if available; and(3) the model version(s) corresponding to the model version(s) in point (1) with the fewest safetymitigations implemented with regard to the systemic risk (such as the helpful-only modelversion, if it exists) and, as available, its chains-of-thought; unless the model is considered a similarly safe or safer model with regard to the same systemic risk(pursuant to Appendix 2.2). Such access to a model may be provided by Signatories through an API, on-premise access (including transport), access via Signatory-provided hardware, or by making themodel parameters publicly available for download, as appropriate. For the purpose of selecting independent external evaluators for the preceding paragraph, Signatories will publish suitable criteria for assessing applications. The number of such evaluators, theselection criteria, and security measures may differ for points (1), (2), and (3) in the precedingparagraph. Signatories will only access, store, and/or analyse evaluation results from independent externalevaluators to assess and mitigate systemic risk from the model. In particular, Signatories refrain fromtraining their models on the inputs and/or outputs from such test runs without express permissionfrom the evaluators. Additionally, Signatories will not take any legal or technical retaliation againstthe independent external evaluators as a consequence of their testing and/or publication of findingsas long as the evaluators:(1) do not intentionally disrupt model availability through the testing, unless expresslypermitted;(2) do not intentionally access, modify, and/or use sensitive or confidential user data in violationof Union law, and if evaluators do access such data, collect only what is necessary, refrain(3) do not intentionally use their access for activities that pose a significant risk to public safetyand security;(4) do not use findings to threaten Signatories, users, or other actors in the value chain, providedthat disclosure under pre-agreed policies and timelines will not be counted as such coercion; and(5) adhere to the Signatory’s publicly available procedure for responsible vulnerability disclosure, which will specify at least that the Signatory cannot delay or block publication for more than30 business days from the date that the Signatory is made aware of the findings, unless alonger timeline is exceptionally necessary such as if disclosure of the findings would materiallyincrease the systemic risk. Signatories that are SMEs or SMCs may contact the AI Office, which may provide support or resourcesto facilitate adherence to this Measure. Commitment 4 Systemic risk acceptance determination LEGAL TEXT: Article 55(1) AI Act Signatories commit to specifying systemic risk acceptance criteria and determining whether thesystemic risks stemming from the model are acceptable (as specified in Measure 4.1). Signatoriescommit to deciding whether or not to proceed with the development, the making available on themarket, and/or the use of the model based on the systemic risk acceptance determination (asspecified in Measure 4.2). Measure 4.1 Systemic risk acceptance criteria and acceptance determination Signatories will describe and justify (in the Framework pursuant to Measure 1.1, point (2)(a)) howthey will determine whether the systemic risks stemming from the model are acceptable. To do so, Signatories will:(1) for each identified systemic risk (pursuant to Measure 2.1), at least:(a) define appropriate systemic risk tiers that:(i) are defined in terms of model capabilities, and may additionally incorporatemodel propensities, risk estimates, and/or other suitable metrics;(ii) are measurable; and(iii) comprise at least one systemic risk tier that has not been reached by themodel; or(b) define other appropriate systemic risk acceptance criteria, if systemic risk tiers arenot suitable for the systemic risk and the systemic risk is not a specified systemic risk(pursuant to Appendix 1.4);(2) describe how they will use these tiers and/or other criteria to determine whether eachidentified systemic risk (pursuant to Measure 2.1) and the overall systemic risk areacceptable; and(3) justify how the use of these tiers and/or other criteria pursuant to point (2) ensures that eachidentified systemic risk (pursuant to Measure 2.1) and the overall systemic risk areacceptable. Signatories will apply the systemic risk acceptance criteria to each identified systemic risk (pursuantto Measure 2.1), incorporating a safety margin (as specified in the following paragraph), to determinewhether each identified systemic risk (pursuant to Measure 2.1) and the overall systemic risk areacceptable. This acceptance determination will take into account at least the information gatheredvia systemic risk identification and analysis (pursuant to Commitments 2 and 3). The safety margin will:(1) be appropriate for the systemic risk; and(2) take into account potential limitations, changes, and uncertainties of:(a) systemic risk sources (e. g. capability improvements after the time of assessment);(b) systemic risk assessments (e. g. under-elicitation of model evaluations or historicalaccuracy of similar assessments); and(c) the effectiveness of safety and security mitigations (e. g. mitigations beingcircumvented, deactivated, or subverted). Measure 4.2 Proceeding or not proceeding based on systemic risk acceptancedetermination Signatories will only proceed with the development, the making available on the market, and/or theuse of the model, if the systemic risks stemming from the model are determined to be acceptable(pursuant to Measure 4.1). If the systemic risks stemming from the model are not determined to be acceptable or are reasonablyforeseeable to be soon not determined to be acceptable (pursuant to Measure 4.1), Signatories willtake appropriate measures to ensure the systemic risks stemming from the model are and will remainacceptable prior to proceeding. In particular, Signatories will:(1) not make the model available on the market, restrict the making available on the market (e. g. via adjusting licenses or usage restrictions), withdraw, or recall the model, as necessary;(2) implement safety and/or security mitigations (pursuant to Commitments 5 and 6); and(3) conduct another round of systemic risk identification (pursuant to Commitment 2), systemicrisk analysis (pursuant to Commitment 3), and systemic risk acceptance determination(pursuant to this Commitment). Commitment 5 Safety mitigations LEGAL TEXT: Article 55(1) and recital 114 AI Act Signatories commit to implementing appropriate safety mitigations along the entire model lifecycle, as specified in the Measure for this Commitment, to ensure the systemic risks stemming from themodel are acceptable (pursuant to Commitment 4). Measure 5.1 Appropriate safety mitigations Signatories will implement safety mitigations that are appropriate, including sufficiently robust underadversarial pressure (e. g. fine-tuning attacks or jailbreaking), taking into account the model’s releaseand distribution strategy. Examples of safety mitigations are:(1) filtering and cleaning training data, e. g. data that might result in undesirable modelpropensities such as unfaithful chain-of-thought traces;(2) monitoring and filtering the model’s inputs and/or outputs;(3) changing the model behaviour in the interests of safety, such as fine-tuning the model torefuse certain requests or provide unhelpful responses;(4) staging the access to the model, e. g. by limiting API access to vetted users, graduallyexpanding access based on post-market monitoring, and/or not making the modelparameters publicly available for download initially;(5) offering tools for other actors to use to mitigate the systemic risks stemming from the model;(6) techniques that provide high-assurance quantitative safety guarantees concerning themodel’s behaviour;(7) techniques to enable safe ecosystems of AI agents, such as model identifications, specialisedcommunication protocols, or incident monitoring tools; and/or(8) other emerging safety mitigations, such as for achieving transparency into chain-of-thoughtreasoning or defending against a model’s ability to subvert its other safety mitigations. Commitment 6 Security mitigations LEGAL TEXT: Article 55(1), and recitals 114 and 115 AI Act Signatories commit to implementing an adequate level of cybersecurity protection for their modelsand their physical infrastructure along the entire model lifecycle, as specified in the Measures for this Commitment, to ensure the systemic risks stemming from their models that could arise fromunauthorised releases, unauthorised access, and/or model theft are acceptable (pursuant to Commitment 4). A model is exempt from this Commitment if the model’s capabilities are inferior to the capabilities ofat least one model for which the parameters are publicly available for download. Signatories will implement these security mitigations for a model until its parameters are madepublicly available for download or securely deleted. Measure 6.1 Security Goal Signatories will define a goal that specifies the threat actors that their security mitigations areintended to protect against (“Security Goal”), including non-state external threats, insider threats, and other expected threat actors, taking into account at least the current and expected capabilitiesof their models. Measure 6.2 Appropriate security mitigations Signatories will implement appropriate security mitigations to meet the Security Goal, including thesecurity mitigations pursuant to Appendix 4. If Signatories deviate from any of the security mitigationslisted in Appendices 4.1 to 4.5, points (a), e. g. due to the Signatory’s organisational context and digitalinfrastructure, they will implement alternative security mitigations that achieve the respectivemitigation objectives. The implementation of the required security mitigations may be staged appropriately in line with theincrease in model capabilities along the entire model lifecycle. Commitment 7 Safety and Security Model Reports LEGAL TEXT: Articles 55(1) and 56(5) AI Act Signatories commit to reporting to the AI Office information about their model and their systemic riskassessment and mitigation processes and measures by creating a Safety and Security Model Report(“Model Report”) before placing a model on the market (as specified in Measures 7.1 to 7.5). Further, Signatories commit to keeping the Model Report up-to-date (as specified in Measure 7.6) andnotifying the AI Office of their Model Report (as specified in Measure 7.7). If Signatories have already provided relevant information to the AI Office in other reports and/ornotifications, they may reference those reports and/or notifications in their Model Report. Signatories may create a single Model Report for several models if the systemic risk assessment andmitigation processes and measures for one model cannot be understood without reference to theother model(s). Signatories that are SMEs or SMCs may reduce the level of detail in their Model Report to the extentnecessary to reflect size and capacity constraints. Measure 7.1 Model description and behaviour Signatories will provide in the Model Report:(1) a high-level description of the model’s architecture, capabilities, propensities, andaffordances, and how the model has been developed, including its training method and data, as well as how these differ from other models they have made available on the market;(2) a description of how the model has been used and is expected to be used, including its use inthe development, oversight, and/or evaluation of models;(3) a description of the model versions that are going to be made or are currently made availableon the market and/or used, including differences in systemic risk mitigations and systemicrisks; and(4) a specification (e. g. via valid hyperlinks) of how Signatories intend the model to operate(often known as a “model specification”), including by:(a) specifying the principles that the model is intended to follow;(b) stating how the model is intended to prioritise different kinds of principles andinstructions;(c) listing topics on which the model is intended to refuse instructions; and(d) providing the system prompt. Measure 7.2 Reasons for proceeding Signatories will provide in the Model Report:(1) a detailed justification for why the systemic risks stemming from the model are acceptable, including details of the safety margins incorporated (pursuant to Measure 4.1);(2) the reasonably foreseeable conditions under which the justification in point (1) would nolonger hold; and(3) a description of how the decision to proceed with the development, making available on themarket, and/or use (pursuant to Measure 4.2) was made, including whether input fromexternal actors informed such a decision (pursuant to Measure 1.1, point (2)(d)), and whetherand how input from independent external evaluators pursuant to Appendix 3.5 informedsuch a decision. Measure 7.3 Documentation of systemic risk identification, analysis, and mitigation Signatories will provide in the Model Report:(1) a description of the results of their systemic risk identification and analysis and anyinformation relevant to understanding them including:(a) a description of their systemic risk identification process for risks belonging to thetypes of risks in Appendix 1.1 (pursuant to Measure 2.1, point (1));(b) explanations of uncertainties and assumptions about how the model would be usedand integrated into AI systems;(c) a description of the results of their systemic risk modelling for the systemic risks(pursuant to Measure 3.3);(d) a description of the systemic risks stemming from the model and a justificationtherefor, including: (i) the systemic risk estimates (pursuant to Measure 3.4); and (ii)a comparison between systemic risks with safety and security mitigationsimplemented and with the model fully elicited (pursuant to Appendix 3.2);(e) all results of model evaluations relevant to understanding the systemic risksstemming from the model and descriptions of: (i) how the evaluations wereconducted; (ii) the tests and tasks involved in the model evaluations; (iii) how themodel evaluations were scored; (iv) how the model was elicited (pursuant to Appendix 3.2); (v) how the scores compare to human baselines (where applicable), across the model versions, and across the evaluation settings;(f) at least five, random samples of inputs and outputs from each relevant modelevaluation, such as completions, generations, and/or trajectories, to facilitateindependent interpretation of the model evaluation results and understanding of thesystemic risks stemming from the model. If particular trajectories materially informthe understanding of a systemic risk, such trajectories will also be provided. Further, Signatories will provide a sufficiently large number of random samples of inputs andoutputs from a relevant model evaluation if subsequently asked by the AI Office;(g) a description of the access and other resources provided to: (i) internal modelevaluation teams (pursuant to Appendix 3.4); and (ii) independent externalevaluators pursuant to Appendix 3.5. Alternatively to the preceding point (ii), Signatories will procure any such independent external evaluators to provide therequisite information directly to the AI Office at the same time that the Signatorysupplies its Model Report to the AI Office; and(h) if they make use of the “similarly safe or safer model” concept pursuant to Appendix2, provide a justification of how the criteria for “safe reference model” (pursuant to Appendix 2.1) and the criteria for “similarly safe or safer model” (pursuant to Appendix 2.2) are fulfilled;(2) a description of: (a) all safety mitigations implemented (pursuant to Commitment 5); (b) howthey fulfil the requirements of Measure 5.1; and (c) their limitations (e. g. if training onexamples of undesirable model behaviour makes identifying future instances of suchbehaviour more difficult);(3) a description of: (a) the Security Goal (pursuant to Measure 6.1); (b) all security mitigationsimplemented (pursuant to Measure 6.2); (c) how the mitigations meet the Security Goal, including the extent to which they align with relevant international standards or otherrelevant guidance (such as the RAND Securing AI Model Weights report); and (d) if Signatorieshave deviated from a listed security mitigation in one (or more) of Appendices 4.1 to 4.5, points (a), a justification for how the alternative security mitigations they have implementedachieve the respective mitigation objectives; and(4) a high-level description of: (a) the techniques and assets they intend to use to further developthe model over the next six months, including through the use of other AI models and/or AIsystems; (b) how such future versions and more advanced models may differ from the Signatory’s current ones, in terms of capabilities and propensities; and (c) any new ormaterially updated safety and security mitigations that they intend to implement for suchmodels. Measure 7.4 External reports Signatories will provide in the Model Report:(1) any available reports (e. g. via valid hyperlinks) from:(a) independent external evaluators involved in model evaluations pursuant to Appendix3.5; and(b) security reviews undertaken by an independent external party pursuant to Appendix4.5; to the extent that respects existing confidentiality (including commercial confidentiality)obligations and allows such external evaluators or parties to maintain control over thepublication of their findings, without implicit endorsement by the Signatories of the contentof such reports;(2) if no independent external evaluator was involved in model evaluations pursuant to Appendix3.5, a justification of how the conditions in Appendix 3.5, first paragraph, points (1) or (2)were met; and(3) if at least one independent external evaluator was involved in model evaluations pursuant to Appendix 3.5, an explanation of the choice of evaluator based on the qualification criteria. Measure 7.5 Material changes to the systemic risk landscape Signatories will ensure that the Model Report contains information relevant for the AI Office tounderstand whether and how the development, making available on the market, and/or use of themodel result in material changes in the systemic risk landscape that are relevant for theimplementation of systemic risk assessment and mitigation measures and processes under this Chapter. Examples of such information are:(1) a description of scaling laws that suggest novel ways of improving model capabilities;(2) a summary of the characteristics of novel architectures that materially improve the state ofthe art in computational efficiency or model capabilities;(3) a description of information relevant to assessing the effectiveness of mitigations, e. g. if themodel’s chain-of-thought is less legible by humans; and/or(4) a description of training techniques that materially improve the efficiency or feasibility of Measure 7.6 Model Report updates Signatories will update their Model Report if they have reasonable grounds to believe that thejustification for why the systemic risks stemming from the model are acceptable (pursuant to Measure 7.2, point (1)) has been materially undermined. Examples of such grounds are:(1) one of the conditions listed pursuant to Measure 7.2, point (2), has materialised;(2) the model’s capabilities, propensities, and/or affordances have changed or will changematerially, such as through further post-training, access to additional tools, or increase ininference compute;(3) the model’s use and/or integrations into AI systems have changed or will change materially;(4) serious incidents and/or near misses involving the model or a similar model have occurred; and/or(5) developments have occurred that materially undermine the external validity of modelevaluations conducted, materially improve the state of the art of model evaluation methods, and/or for other reasons suggest that the systemic risk assessment conducted is materiallyinaccurate. Model Report updates should be completed within a reasonable amount of time after the Signatorybecomes aware of the grounds that necessitate an update, e. g. after discovering them as part of theircontinuous systemic risk assessment and mitigation (pursuant to Measure 1.2, second paragraph). Ifa Model Report update is triggered by a deliberate change to a model and that change is madeavailable on the market, the Model Report update and the underlying full systemic risk assessmentand mitigation process (pursuant to Measure 1.2, third paragraph) need to be completed before thechange is made available on the market. Further, if the model is amongst their respective most capable models available on the market, Signatories will provide the AI Office with an updated Model Report at least every six months. Signatories do not need to do so if: (1) the model’s capabilities, propensities, and/or affordances havenot changed since they have last provided the AI Office with the Model Report, or update thereof; (2)they will place a more capable model on the market in less than a month; and/or (3) the model isconsidered similarly safe or safer (pursuant to Appendix 2.2) for each identified systemic risk(pursuant to Measure 2.1). The updated Model Report will contain:(1) the updated information specified in Measures 7.1 to 7.5 based on the results of the fullsystemic risk assessment and mitigation process (pursuant to Measure 1.2, third paragraph); and(2) a changelog, describing how and why the Model Report has been updated, along with aversion number and the date of change. Measure 7.7 Model Report notifications Signatories will provide the AI Office with access to the Model Report (without redactions, unless theyare required by national security laws to which Signatories are subject) by the time they place a modelon the market, e. g. through a publicly accessible link or through a sufficiently secure channel specifiedby the AI Office. If a Model Report is updated, Signatories will provide the AI Office with access to theupdated Model Report (without redactions, unless they are required by national security laws to To facilitate the placing on the market of a model, Signatories may delay providing the AI Office witha Model Report, or an update thereof, by up to 15 business days. This may be done only if the AIOffice considers the Signatory to be acting in good faith and if the Signatory provides an interim Model Report, containing the information specified in Measures 7.2 and 7.5, to the AI Office without delay. Commitment 8 Systemic risk responsibility allocation LEGAL TEXT: Article 55(1) and recital 114 AI Act Signatories commit to: (1) defining clear responsibilities for managing the systemic risks stemmingfrom their models across all levels of the organisation (as specified in Measure 8.1); (2) allocatingappropriate resources to actors who have been assigned responsibilities for managing systemic risk(as specified in Measure 8.2); and (3) promoting a healthy risk culture (as specified in Measure 8.3). Measure 8.1 Definition of clear responsibilities Signatories will clearly define responsibilities for managing the systemic risks stemming from theirmodels across all levels of the organisation. This includes the following responsibilities:(1) Systemic risk oversight: Overseeing the Signatories’ systemic risk assessment and mitigationprocesses and measures.(2) Systemic risk ownership: Managing systemic risks stemming from Signatories’ models, including the systemic risk assessment and mitigation processes and measures, and managingthe response to serious incidents.(3) Systemic risk support and monitoring: Supporting and monitoring the Signatories’ systemicrisk assessment and mitigation processes and measures.(4) Systemic risk assurance: Providing internal and, as appropriate, external assurance about theadequacy of the Signatories’ systemic risk assessment and mitigation processes and measuresto the management body in its supervisory function or another suitable independent body(such as a council or board). Signatories will allocate these responsibilities, as suitable for the Signatories’ governance structureand organisational complexity, across the following levels of their organisation:(1) the management body in its supervisory function or another suitable independent body (suchas a council or board);(2) the management body in its executive function;(3) relevant operational teams;(4) if available, internal assurance providers (e. g. an internal audit function); and(5) if available, external assurance providers (e. g. third-party auditors). This Measure is presumed to be fulfilled, if Signatories, as appropriate for the systemic risks stemmingfrom their models, adhere to all of the following:(1) Systemic risk oversight: The responsibility for overseeing the Signatory’s systemic riskmanagement processes and measures has been assigned to a specific committee of themanagement body in its supervisory function (e. g. a risk committee or audit committee) orone or multiple suitable independent bodies (such as councils or boards). For Signatories thatare SMEs or SMCs, this responsibility may be primarily assigned to an individual member ofthe management body in its supervisory function.(2) Systemic risk ownership: The responsibility for managing systemic risks from models hasbeen assigned to suitable members of the management body in its executive function whoare also responsible for relevant Signatory core business activities that may give rise tosystemic risk, such as research and product development (e. g. Head of Research or Head of Product). The members of the management body in its executive function have assignedlower-level responsibilities to operational managers who oversee parts of the systemic-risk-producing business activities (e. g. specific research domains or specific products). Dependingon the organisational complexity, there may be a cascading responsibility structure.(3) Systemic risk support and monitoring: The responsibility for supporting and monitoring the Signatory’s systemic risk management processes and measures, including conducting riskassessments, has been assigned to at least one member of the management body in itsexecutive function (e. g. a Chief Risk Officer or a Vice President, Safety & Security Framework). This member(s) must not also be responsible for the Signatory’s core business activities thatmay produce systemic risk (e. g. research and product development). For Signatories that are SMEs or SMCs, there is at least one individual in the management body in its executivefunction tasked with supporting and monitoring the Signatory’s systemic risk assessment andmitigation processes and measures.(4) Systemic risk assurance: The responsibility for providing assurance about the adequacy ofthe Signatory’s systemic risk assessment and mitigation processes and measures to themanagement body in its supervisory function or another suitable independent body (such asa council or board) has been assigned to a relevant party (e. g. a Chief Audit Executive, a Headof Internal Audit, or a relevant sub-committee). This individual is supported by an internalaudit function, or equivalent, and external assurance as appropriate. The Signatories’ internalassurance activities are appropriate. For Signatories that are SMEs or SMCs, the managementbody in its supervisory function periodically assesses the Signatory’s systemic risk assessmentand mitigation processes and measures (e. g. by approving the Signatory’s Frameworkassessment). Measure 8.2 Allocation of appropriate resources Signatories will ensure that their management bodies oversee the allocation of resources to thosewho have been assigned responsibilities (pursuant to Measure 8.1) that are appropriate for thesystemic risks stemming from their models. The allocation of such resources will include:(1) human resources;(2) financial resources;(3) access to information and knowledge; and(4) computational resources. Measure 8.3 Promotion of a healthy risk culture Signatories will promote a healthy risk culture and take appropriate measures to ensure that actorswho have been assigned responsibilities for managing the systemic risks stemming from their models(pursuant to Measure 8.1) take a reasoned and balanced approach to systemic risk. Examples of indicators of a healthy risk culture for the purpose of this Measure are:(1) setting the tone for a healthy systemic risk culture from the top, e. g. by the leadership clearlycommunicating the Signatory’s Framework to staff;(2) allowing clear communication and challenge of decisions concerning systemic risk;(3) setting incentives and affording sufficient independence of staff involved in systemic riskassessment and mitigation to discourage excessive systemic-risk-taking and encourage anunbiased assessment of the systemic risks stemming from their models;(4) anonymous surveys find that staff are comfortable raising concerns about systemic risks, areaware of channels for doing so, and understand the Signatory’s Framework;(5) internal reporting channels are actively used and reports are acted upon appropriately;(6) annually informing workers of the Signatory’s whistleblower protection policy and makingsuch policy readily available to workers such as by publishing it on their website; and/or(7) not retaliating in any form, including any direct or indirect detrimental action such astermination, demotion, legal action, negative evaluations, or creation of hostile workenvironments, against any person publishing or providing information acquired in the contextof work-related activities performed for the Signatory to competent authorities aboutsystemic risks stemming from their models for which the person has reasonable grounds tobelieve its veracity. Commitment 9 Serious incident reporting LEGAL TEXT: Article 55(1), and recitals 114 and 115 AI Act Signatories commit to implementing appropriate processes and measures for keeping track of, documenting, and reporting to the AI Office and, as applicable, to national competent authorities, without undue delay relevant information about serious incidents along the entire model lifecycleand possible corrective measures to address them, as specified in the Measures of this Commitment. Further, Signatories commit to providing resourcing of such processes and measures appropriate forthe severity of the serious incident and the degree of involvement of their model. Measure 9.1 Methods for serious incident identification Signatories will consider the exemplary methods in Measure 3.5 to keep track of relevant informationabout serious incidents. Additionally, Signatories will:(1) review other sources of information, such as police and media reports, posts on social media, research papers, and incident databases; and(2) facilitate the reporting of relevant information about serious incidents by downstreammodifiers, downstream providers, users, and other third parties to:(a) the Signatory; or(b) the AI Office and, as applicable, national competent authorities; by informing such third parties of direct reporting channels, if available, without prejudice toany of their reporting obligations under Article 73 AI Act. Measure 9.2 Relevant information for serious incident tracking, documentation, andreporting Signatories will keep track of, document, and report to the AI Office and, as applicable, to nationalcompetent authorities, at least the following information to the best of their knowledge, redactedto the extent necessary to comply with other Union law applicable to such information:(1) the start and end dates of the serious incident, or best approximations thereof if the precisedates are unclear;(2) the resulting harm and the victim or affected group of the serious incident;(3) the chain of events that (directly or indirectly) led to the serious incident;(4) the model involved in the serious incident;(5) a description of material available setting out the model’s involvement in the serious incident;(6) what, if anything, the Signatory intends to do or has done in response to the serious incident;(7) what, if anything, the Signatory recommends the AI Office and, as applicable, nationalcompetent authorities to do in response to the serious incident;(8) a root cause analysis with a description of the model’s outputs that (directly or indirectly) ledto the serious incident and the factors that contributed to their generation, including theinputs used and any failures or circumventions of systemic risk mitigations; and(9) any patterns detected during post-market monitoring (pursuant to Measure 3.5) that canreasonably be assumed to be connected to the serious incident, such as individual oraggregate data on near misses. Signatories will investigate the causes and effects of serious incidents, including the informationwithin the preceding list, with a view to informing systemic risk assessment. If Signatories do not yethave certain relevant information from the preceding list, they will record that in their seriousincident reports. The level of detail in serious incident reports will be appropriate for the severity ofthe incident. Measure 9.3 Reporting timelines Signatories will provide the information in points (1) to (7) of Measure 9.2 in an initial report that issubmitted to the AI Office and, as applicable, to national competent authorities, at the followingpoints in time, save in exceptional circumstances, if the involvement of their model (directly orindirectly) led to:(1) a serious and irreversible disruption of the management or operation of criticalinfrastructure, or if the Signatories establish or suspect with reasonable likelihood such acausal relationship between their model and the disruption, not later than two days after the Signatories become aware of the involvement of their model in the incident;(2) a serious cybersecurity breach, including the (self-)exfiltration of model weights andcyberattacks, or if the Signatories establish or suspect with reasonable likelihood such acausal relationship between their model and the breach, not later than five days after the Signatories become aware of the involvement of their model in the incident;(3) a death of a person, or if the Signatories establish or suspect with reasonable likelihood sucha causal relationship between their model and the death, not later than 10 days after the Signatories become aware of the involvement of their model in the incident; and(4) serious harm to a person’s health (mental and/or physical), an infringement of obligationsunder Union law intended to protect fundamental rights, and/or serious harm to property orthe environment, or if the Signatories establish or suspect with reasonable likelihood such acausal relationship between their model and the harms or infringements, not later than 15days after the Signatories become aware of the involvement of their model in the incident. For unresolved serious incidents, Signatories will update the information in their initial report andadd further information required by Measure 9.2, as available, in an intermediate report that issubmitted to the AI Office and, as applicable, to national competent authorities, at least every fourweeks after the initial report. Signatories will submit a final report, covering all the information required by Measure 9.2, to the AIOffice and, as applicable, to national competent authorities, not later than 60 days after the seriousincident has been resolved. If multiple similar serious incidents occur within the reporting timelines, Signatories may include themin the report(s) of the first serious incident, while respecting the timelines for reporting for the firstserious incident. Measure 9.4 Retention period Signatories will keep documentation of all relevant information gathered in adhering to this Commitment for at least five years from the date of the documentation or the date of the seriousincident, whichever is later, without prejudice to Union law applicable to such information. Commitment 10 Additional documentation andtransparency LEGAL TEXT: Articles 53(1)(a) and 55(1) AI Act Signatories commit to documenting the implementation of this Chapter (as specified in Measure 10.1)and publish summarised versions of their Framework and Model Reports as necessary (as specifiedin Measure 10.2). Measure 10.1 Additional documentation Signatories will draw up and keep up-to-date the following information for the purpose of providingit to the AI Office upon request:(1) a detailed description of the model’s architecture;(2) a detailed description of how the model is integrated into AI systems, explaining howsoftware components build or feed into each other and integrate into the overall processing, insofar as the Signatory is aware of such information;(3) a detailed description of the model evaluations conducted pursuant to this Chapter, includingtheir results and strategies; and(4) a detailed description of the safety mitigations implemented (pursuant to Commitment 5). Documentation will be retained at least 10 years after the model has been placed on the market. Further, Signatories will keep track of the following information, to the extent it is not already coveredby the first paragraph, for the purpose of evidencing adherence to this Chapter to the AI Office uponrequest:(1) their processes, measures, and key decisions that form part of their systemic risk assessmentand mitigation; and(2) justifications for choices of a particular best practice, state-of-the-art, or other moreinnovative process or measure if a Signatory relies upon it for adherence to this Chapter. Signatories need not collect the information of the third paragraph in one medium or place but maycompile it upon the AI Office’s request. Measure 10.2 Public transparency If and insofar as necessary to assess and/or mitigate systemic risks, Signatories will publish (e. g. viatheir websites) a summarised version of their Framework and Model Report(s), and updates thereof(pursuant to Commitments 1 and 7), with removals to not undermine the effectiveness of safetyand/or security mitigations and to protect sensitive commercial information. For Model Reports, suchpublication will include high-level descriptions of the systemic risk assessment results and the safetyand security mitigations implemented. For Frameworks, such publication is not necessary if all of the Signatory’s models are similarly safe or safer models pursuant to Appendix 2.2. For Model Reports, such publication is not necessary if the model is a similarly safe or safer model pursuant to Appendix2.2.