前言⼈⼯智能是⼈类发展新领域，深刻改变⼈类⽣产⽣活⽅式，给世界带来前所未有发展机遇，也带来前所未遇⻛险挑战。落实《全球⼈⼯智能治理倡议》，遵循“以⼈为本、智能向善”的发展⽅向，为推动各国政府、⾏业企业、机构组织、社会公众等各⽅以及国际社会，就⼈⼯智能安全治理达成共识、协调⼀致，有效防范应对⼈⼯智能安全⻛险，我们于2024年9⽉制定发布了《⼈⼯智能安全治理框架》1.0版。1.0版发布以来，⼈⼯智能技术和应⽤持续快速发展，个别领域取得超预期突破。例如，⾼性能推理模型涌现，极⼤提⾼了对数学、物理、代码等复杂问题的求解能⼒；⾼效能轻量级模型的开源，显著降低了部署应⽤的⻔槛，⼈⼯智能应⽤迅速向各⾏业领域渗透普及；⼤模型应⽤形态从机器问答向嵌⼊业务流程的智能体演进，加速与业务系统融合；具⾝智能、脑机接⼝技术⽇新⽉异，正在打通连接数字智能和物理世界的“最后⼀公⾥”，⼈机融合的智能时代已不再遥不可及。与此同时，⼈⼯智能安全⻛险的表现形式、影响程度、认识感知亦同步快速演进变化。为应对⼈⼯智能快速发展的新⻛险新挑战，安全有效地释放应⽤需求，促进⼈⼯智能技术和产业发展，在国家互联⽹信息办公室的指导下，全国⽹络安全标准化技术委员会组织国家计算机⽹络应急技术处理协调中⼼等专业机构、科研院所、⾏业企业，持续跟踪⻛险变化，梳理调整⻛险分类，研究探索⻛险分级⽅法，动态调整更新防范治理措施，制定《⼈⼯智能安全治理框架》2.0版，推动增进⼈⼯智能安全治理共识，促进协同共治、普惠共享。1.⼈⼯智能安全治理原则秉持共同、综合、合作、可持续的安全观，坚持发展和安全并重，以促进⼈⼯智能创新发展为第⼀要务，以有效防范化解⼈⼯智能安全⻛险为出发点和落脚点，构建技术与管理相结合、监管与治理相衔接、国内与国际相协同、社会各⽅积极参与且有效互动的治理机制，压实相关主体安全责任，打造全过程全要素治理链条，培育安全、可靠、公平、透明的⼈⼯智能技术研发和应⽤⽣态，积极研究应对⼈⼯智能灾难性⻛险的共识性准则，推动⼈⼯智能健康发展和规范应⽤，切实维护国家主权、安全和发展利益，保障公⺠、法⼈和其他组织的合法权益，确保⼈⼯智能技术造福于⼈类。1.1包容审慎、确保安全。⿎励发展创新，对⼈⼯智能研发及应⽤采取包容态度，通过在安全可控环境下试点等⽅式，为新技术新应⽤发展提供容错纠错空间。严守安全底线，对危害国家安全、社会公共利益、公众合法权益的⻛险及时采取措施。1.2⻛险导向、敏捷治理。密切跟踪⼈⼯智能研发及应⽤趋势，从技术⾃⾝、技术应⽤、衍⽣社会影响等⽅⾯分析梳理安全⻛险；探索从应⽤场景、智能化⽔平、应⽤规模等维度进⾏⻛险分级，进⽽采取相适应的应对措施；持续优化治理机制和⽅式，对确需政府监管事项及时予以响应。1.3技管结合、协同应对。⾯向⼈⼯智能研发应⽤全过程，以及模型开源业态新挑战，综合运⽤技术、管理措施，防范应对不同类型⻛险。围绕⼈⼯智能研发应⽤⽣态链，明确模型算法研发者、服务提供者、系统使⽤者等主体的安全责任，有机发挥政府监管、⾏业⾃律、社会监督等治理机制作⽤。1.4开放合作、共治共享。在全球范围推动⼈⼯智能安全治理国际合作，共享最佳实践，提倡建⽴开放性国际交流合作平台，通过跨学科、跨领域、跨地区、跨国界的对话和合作，推动形成具有⼴泛共识的全球⼈⼯智能治理体系。1.5可信应⽤、防范失控。推动形成涵盖技术防护、价值对⻬、协同治理等多层⾯的可信⼈⼯智能基本准则，确保技术演进安全、可靠、可控，严防威胁⼈类⽣存发展的失控⻛险，确保⼈⼯智能始终处于⼈类控制之下。2.⼈⼯智能安全治理框架构成基于⻛险管理理念，本框架针对不同类型的⼈⼯智能安全⻛险，从技术、管理两⽅⾯提出防范应对措施。2.1安全⻛险分类。通过分析⼈⼯智能技术特性，以及在不同⾏业领域应⽤场景，梳理⼈⼯智能技术本⾝，及其在应⽤过程中⾯临的各种安全⻛险隐患。在1.0版基础上，调整更新⻛险类型，并探索性提出分级应对原则。2.2技术应对措施。针对模型算法、训练数据、算⼒设施、产品服务、应⽤场景，提出通过安全软件开发、数据质量提升、安全建设运维、测评监测加固等技术⼿段，提升⼈⼯智能技术及应⽤安全性的措施。2.3综合治理措施。提出技术研发机构、服务提供者、⽤⼾、政府部⻔、社会组织等各⽅发现、防范、应对⼈⼯智能安全⻛险的措施⼿段，以及深化⼈⼯智能安全治理国际交流合作等建议，推动相关各⽅协同共治。2.4研发与应⽤的安全指引。提出模型算法研发、建设部署、运⾏管理，以及访问使⽤的引导性安全规范。此外，针对潜在的技术失控⻛险，提出可信⼈⼯智能基本准则，引导国际社会共识。3.⼈⼯智能安全⻛险分类⼈⼯智能既存在模型算法缺陷、数据语料质量问题等技术内⽣安全⻛险，也存在技术整合交付应⽤时的⽹络系统、信息内容等⽅⾯应⽤安全⻛险，还⾯临技术误⽤、滥⽤、恶⽤冲击现实社会环境、⼈类认知伦理的衍⽣安全⻛险，甚⾄是灾难性⻛险。3.1⼈⼯智能技术内⽣安全⻛险3.1.1模型算法安全⻛险（a）可解释性不⾜。以深度学习为代表的⼈⼯智能算法运⾏逻辑复杂，推理过程不透明，可能导致决策输出难以预测和归因，异常、故障、错误难以快速修正和溯源追责。（b）偏⻅、歧视。模型算法研发设计及训练过程中，偏⻅、歧视等问题被有意、⽆意引⼊，或因训练数据质量、多样性问题，导致算法设计⽬的、决策判断、输出结果存在偏⻅或歧视，甚⾄输出存在⺠族、信仰、国别、地域、性别等歧视性内容。（c）鲁棒性不强。由于深度神经⽹络存在⾮线性、⼤规模等特点，⼈⼯智能易受复杂多变运⾏环境或恶意⼲扰、诱导的影响，可能带来性能下降、决策错误等鲁棒性问题。（d）输出决策不可靠。⼈⼯智能利⽤有限数据集拟合复杂现实世界，⾃主感知、认识、理解、交互的理论基础、技术能⼒还有待突破，基于有限样本的决策判断、输出结果存在“幻觉”,即看似合理实则不可靠的现象。（e）外部对抗攻击。攻击者利⽤模型算法及其设计实现的缺陷、漏洞，构造对抗攻击样本数据，窃取、篡改模型参数、结构、功能等，⼲预模型推理过程进⽽影响决策判断、输出结果及运⾏稳定性，甚⾄恶意利⽤或消耗模型资源。（f）模型缺陷扩散。依托基础模型进⾏⼆次开发或微调、建设部署⼈⼯智能应⽤，将导致基础模型缺陷向下游模型、应⽤传导。基础模型的开源，加剧模型缺陷扩散速度、影响范围和修补难度，为不法分⼦训练“作恶模型”提供便利。3.1.2数据安全⻛险（a）违规收集使⽤数据。⼈⼯智能训练数据的获取，以及服务、交互过程中，存在未经同意收集、不当使⽤数据和个⼈信息的安全⻛险。（b）训练数据内容不当。训练数据包含虚假、偏⻅、侵犯知识产权等违法有害内容，还⾯临攻击者篡改、注⼊错误、误导数据的“投毒”⻛险，影响模型价值观对⻬，“污染”模型概率分布，造成决策输出准确性、可信度下降，甚⾄输出违法有害信息。（c）训练数据标注不规范。训练数据标注过程中，存在标注规则不完备、标注⼈员能⼒不⾜、标注错误等问题，影响模型算法准确度、可靠性、有效性，还可能导致训练偏差、偏⻅歧视放⼤、泛化能⼒不⾜或决策判断输出错误。（d）数据和个⼈信息泄露。⼈⼯智能训练数据蕴含的知识、敏感信息暗藏于模型参数之中，因模型安全防护机制不健全、敏感信息未“遗忘”、诱导交互和恶意攻击，可能导致数据和个⼈信息泄露。3.2⼈⼯智能技术应⽤安全⻛险3.2.1⽹络系统安全⻛险（a）组件和算⼒安全。⼈⼯智能依赖的开发框架、计算框架、执⾏平台、算⼒设施等，存在缺陷、漏洞、后⻔、可靠性等⻛险。同时，⾯临算⼒资源恶意消耗，以及安全问题在多源、异构、泛在算⼒资源间跨边界传递的⻛险。（b）⽹络暴露⾯扩⼤。模型本地化部署涉及⽹络拓扑和系统策略、权限、端⼝、资源的调整配置，易形成新的⽹络攻击⼊⼝和路径。智能体需调⽤终端系统⽂件、权限、接⼝、⼯具，以实现复杂任务⾃主规划⾃动执⾏，加剧⽂件泄露、权限滥⽤等安全⻛险。（c）供应链安全。⼈⼯智能产业链呈现⾼度全球化分⼯协作格局。但个别国家利⽤技术垄断和出⼝管制等单边强制措施制造发展壁垒，恶意阻断全球⼈⼯智能供应链，带来突出的芯⽚、软件、⼯具断供⻛险。（d）⽹络攻击滥⽤。⼈⼯智能可被⽤于降低⽹络攻击⻔槛，提⾼攻击效率甚⾄实施⾃动化攻击，增⼤防护难度。特别是可⽤于⽣成图⽚、⾳频、视频等⾼仿真内容，绕过⼈脸识别、语⾳识别等⾝份认证机制，导致认证鉴权失效。3.2.2信息内容安全⻛险（a）输出违法有害信息。模型⾃⾝安全能⼒不⾜，叠加应⽤防护机制不强、⽤⼾恶意诱导等因素，导致⽣成输出欺诈、暴⼒、⾊情、极端主义等违法有害信息，威胁社会稳定、公共安全和意识形态安全。（b）混淆事实、误导⽤⼾。⼈⼯智能输出内容未经标识，特别是“深伪”技术的应⽤，导致⽤⼾难以识别⽣成内容来源及交互对象是否为⼈⼯智能系统，难以鉴别⽣成内容的真实性，影响⽤⼾判断，还可被⽤于制作传播虚假信息误导公众、⾮法牟利。（c）污染⽹络内容⽣态。模型输出的低质不良信息，经⽹络扩散传播、模型循环引⽤，造成⽹络内容质量的整体下降，甚⾄特定领域、话题的内容污染。3.2.3现实安全⻛险（a）经济社会运⾏安全的新挑战。⼈⼯智能应⽤于能源、电信、⾦融、交通等关键信息基础设施⾏业领域，模型算法的幻觉输出、错误决策，以及不当使⽤、外部攻击等，可能引发系统性能下降、服务中断、操作执⾏失控等问题，加剧关键信息基础设施安全稳定运⾏⻛险。（b）被违法犯罪活动利⽤“作恶”。⼈⼯智能可被利⽤于涉恐、涉暴、涉赌、涉毒等传统违法犯罪活动，包括传授违法犯罪技巧、隐匿违法犯罪⾏为、制作违法犯罪⼯具等。（c）核⽣化导武器知识、能⼒失控。⼈⼯智能在训练过程中多使⽤⻔类宽泛、内容丰富的语料数据，其中包含核⽣化导武器相关基础理论知识，辅以检索增强⽣成功能，如不能有效管控，将被极端势⼒、恐怖分⼦利⽤于获取相关知识，以及设计、制造、合成、使⽤核⽣化导武器能⼒，导致现有管控体系失效，加剧世界各地区和平安全威胁。3.2.4认知安全⻛险（a）加剧“信息茧房”效应。⼈⼯智能将显著提升信息服务定制化能⼒，更为准确地收集⽤⼾信息，分析⽤⼾需求、意图、喜好、⾏为习惯，甚⾄特定时段、特定群体的意识思潮，进⽽推送精准定制化信息服务,加剧⽤⼾所关注信息的局限性。（b）助⼒开展认知战。⼈⼯智能被⽤于宣扬恐怖主义、极端主义、有组织犯罪等内容，⼲涉他国内政、社会制度及社会秩序，危害他国主权；通过社交机器⼈在⽹络空间抢占话语权和议程设置权，左右公众价值观和思维认知。3.3人工智能应用衍生安全风险3.3.1社会和环境安全⻛险（a）冲击劳动就业结构。⼈⼯智能带来⽣产⼒、⽣产关系的⼤幅调整，加速重构传统经济结构，资本、技术与数据在经济活动中的地位全⾯提升，⽽劳动⼒要素的价值受到削弱，造成传统劳动⼒需求明显下降。（b）挑战资源供需平衡。⼈⼯人工智能发展中的算⼒设施⽆序建设、轻量模型碎⽚化部署、同质化模型低效重复开发等问题，加速电⼒、⼟地、⽔等能源资源消耗，对资源供需平衡、绿⾊低碳发展带来新的挑战。3.3.2伦理安全⻛险（a）加剧社会偏⻅、扩⼤智能鸿沟。利⽤⼈⼯智能收集分析⼈类⾏为、社会地位、经济状态、个体性格等，对不同⼈群进⾏标识分类、区别对待，带来系统性、结构性的社会歧视与偏⻅。同时，拉⼤不同地区⼈⼯智能鸿沟。（b）冲击教育、抑制创新。学⽣及科研、⼯程技术、⽂学艺术⼯作者将⼈⼯智能⼯具⼴泛应⽤于知识学习、科学研究、创意创作等⼯作，在提升效率的同时，可能产⽣⼯具依赖，⾃主学习、研究、创作能⼒退化，创新潜⼒减弱。（c）加剧科研伦理⻛险。“⼈⼯智能+科研”降低⽣物、基因等⾼伦理⻛险科研领域的进⼊⻔槛，拓宽了普通科研机构、⼈员探索敏感科学问题的边界，个别科研伦理意识不强的机构、⼈员可能开展违背社会伦理、社会禁忌的⾼⻛险研究活动，打开科技“魔盒”。（d）拟⼈化交互的沉迷依赖。基于拟⼈化交互的⼈⼯智能产品，导致⽤⼾对其产⽣情感依赖，进⽽影响⽤⼾⾏为，造成社会伦理⻛险。（e）挑战现⾏社会秩序。⼈⼯人工智能发展及应⽤，带来⽣产⼯具、⽣产关系的⼤幅改变，加速重构传统⾏业模式，颠覆传统的就业观、⽣育观、教育观，挑战传统社会秩序。（f）“⾃我意识”觉醒、脱离⼈类控制。未来，不排除⼈⼯智能出现突发的、超预期的智能化⽔平“跃迁”，⾃主获取外部资源、⾃我复制，产⽣⾃我意识，寻求外部权⼒，带来谋求与⼈类争夺控制权的⻛险。4.技术应对措施针对上述⻛险，模型算法研发者、服务提供者、系统使⽤者等需从训练数据、模型算法、算⼒设施、产品服务、应⽤场景各⽅⾯采取技术措施予以防范。4.1技术内⽣安全⻛险的应对措施4.1.1模型算法安全⻛险应对（a）提升⼈⼯智能可解释性、透明性，为⼈⼯智能系统内部构造、推理逻辑、技术接⼝、输出结果提供明确说明，正确反映⼈⼯智能系统产⽣结果的过程。（b）改进模型架构，扩充训练数据的规模和多样性，引⼊⼈类监督机制，减轻偏⻅歧视，提升模型的泛化能⼒和输出结果可靠性。（c）在设计、研发过程中建⽴并实施安全开发规范，消减模型算法安全缺陷，对模型进⾏对抗性训练，降低模型易受提⽰注⼊攻击的⻛险，提⾼鲁棒性。（d）加强基础模型、开源模型安全缺陷传导评估。4.1.2数据安全⻛险应对（a）在训练数据和⽤⼾交互数据的收集、存储、使⽤、加⼯、传输、提供、公开、删除等各环节，应遵循数据收集使⽤、个⼈信息处理的安全规则，严格落实关于⽤⼾控制权、知情权、选择权等法律法规明确的合法权益。（b）使⽤真实、准确、客观、多样且来源合法的训练数据，对训练数据进⾏严格筛选，过滤虚假、偏⻅、失效、错误数据，确保不包含核⽣化导武器等⾼危领域敏感数据。（c）规范训练数据标注流程，提升标注准确性和可靠性。（d）强化数据安全管理，涉及敏感个⼈信息和重要数据的，应符合数据安全和个⼈信息保护相关法律法规、标准规范。合理推动利⽤合成数据替代个⼈特征数据，避免个⼈信息依赖。（e）加强知识产权保护，在训练数据选择、结果输出等环节防⽌侵犯知识产权。4.2技术应用安全风险的应对措施4.2.1⽹络系统安全⻛险应对（a）对⼈⼯智能技术和产品的原理、能⼒、适⽤场景、安全⻛险进⾏必要披露，不断提⾼⼈⼯智能系统透明性。（b）对聚合多个⼈⼯智能模型或系统的平台，加强权限管理，禁⽤⾮必要服务，完善⼈⼯智能服务接⼝的访问控制策略，提升⻛险识别、检测、防护能⼒，防⽌因平台恶意⾏为或被攻击⼊侵影响承载的⼈⼯智能模型或系统。（c）在⼈⼯智能应⽤部署、维护过程中建⽴并实施安全规范，消减缺陷、漏洞、后⻔，跟踪软硬件产品的漏洞、缺陷信息，定期进⾏安全检测和漏洞扫描，并及时采取修补加固措施，确保系统安全、稳定运⾏。（d）对于⼈⼯智能系统采⽤的芯⽚、软件、⼯具、算⼒和数据资源，⾼度关注供应链安全。（e）完善冗余设计与容灾机制，确保异常或受攻击时，系统仍能正常运⾏。4.2.2信息内容安全⻛险应对（a）建⽴安全防护机制，防⽌模型运⾏过程中被⼲扰、篡改⽽输出不可信结果。（b）建⽴安全护栏，对输⼊输出进⾏动态过滤，防⽌恶意注⼊和违法内容⽣成，避免⼈⼯智能系统违法违规输出敏感个⼈信息和重要数据。（c）对⼈⼯智能⽣成合成内容进⾏标识，实现可识别、可追溯、可信赖。4.2.3现实安全⻛险应对（a）根据应⽤场景设置能⼒边界，裁减⼈⼯智能系统可能被滥⽤的功能，确保智能系统能⼒不超出预设范围。（b）针对算法缺陷、偶发随机性影响决策问题，建⽴决策判断校验、容错及纠偏机制。（c）在引⼊⾼度⾃主操作执⾏能⼒时，同步建⽴“熔断”、“⼀键管控”等措施，实现极端情况下迅速⼲预⽌损。（d）对于智能辅助驾驶、⽆⼈机等依赖对物理世界强感知的⼈⼯智能应⽤场景，在投⼊使⽤前对感知系统进⾏在⼤⾯积遮挡、强电磁⼲扰等极端条件下的测试。（e）提⾼⼈⼯智能系统最终⽤途追溯能⼒，防⽌被⽤于核⽣化导等⼤规模杀伤性武器制造等⾼危场景。4.2.4认知安全⻛险应对（a）通过技术⼿段判别不符合预期、不真实、不准确的输出结果，并依法依规监管。（b）对收集⽤⼾提问信息进⾏关联分析、汇聚挖掘，进⽽判断⽤⼾⾝份、喜好以及个⼈思想倾向的⼈⼯智能系统，应严格防范其滥⽤。（c）加强对⼈⼯智能⽣成合成内容的检测技术研发，提升对认知战⼿段的防范、检测、处置能⼒。4.3应⽤衍⽣安全⻛险的应对措施4.3.1社会和环境安全⻛险应对（a）⽀持不断探索创新资源节约、环境友好的⼈⼯人工智能发展模式，制定⼈⼯智能绿⾊技术标准。（b）推⼴低功耗芯⽚、⾼效算法等绿⾊计算技术和能效优化⽅案，降低能源等资源消耗。4.3.2伦理安全⻛险应对（a）在算法设计、模型训练和优化、提供服务过程中，采取训练数据筛选、价值观对⻬、输出校验等⽅式，有效规避产⽣⺠族、信仰、国别、地域、性别等歧视的⻛险。（b）应⽤于政府部⻔、关键信息基础设施以及直接影响公共安全和公⺠⽣命健康安全等重点领域的⼈⼯智能系统，需具备⾼效精准的应急管控措施。（c）⿎励研发和采⽤具备透明决策逻辑的模型和可解释算法，提升⽤⼾对系统运⾏机制的理解和信任。5.综合治理措施在采取技术应对措施的同时，建⽴完善技术研发机构、服务提供者、⽤⼾、政府部⻔、社会组织等多⽅参与的⼈⼯智能安全⻛险综合治理制度规范。5.1建⽴健全⼈⼯智能安全法律法规。推动⼈⼯智能安全相关⽴法，完善基础设施安全防护、分级分类监管、⼈⼯智能安全测评、最终⽤途管理、重点场景安全应⽤等制度。⿎励地⽅结合产业发展实践，差异化探索创新制度设计。5.2构建⼈⼯智能科技伦理准则。制定有⼴泛共识的⼈⼯智能科技伦理准则、规范和指南，对在⽣命健康、⼈格尊严、劳动就业、⽣态环境、可持续发展等⽅⾯存在突出伦理⻛险的⼈⼯智能科学研究、技术开发等活动，规范有序开展伦理审查。推进⼈⼯智能科技伦理服务体系建设，强化服务供给，加⼤对中⼩微企业的⽀持⼒度。5.3提升研发应⽤全⽣命周期安全能⼒。持续提升算法可靠性、可信度、透明度、容错机制、隐私保护、价值观对⻬等内⽣安全能⼒，利⽤对抗测试等技术评估改进模型鲁棒性，降低模型算法潜在偏⻅，确保价值观、伦理⻛险可控，避免⼈⼯智能系统意外决策产⽣恶意⾏为。5.4强化开源⽣态安全和供应链安全。在培育发展开源创新⽣态的同时，同步提升开源⽣态安全能⼒。⿎励⽀持训练推理框架、软件⼯具、关键组件、评测基准等全⽅位的⼈⼯智能技术开源，进⼀步提⾼开源模型透明度。推动开源模型提供⽅、开源社区共同完善开源规则，强化⾯向模型下载⽤⼾的安全责任、⻛险隐患告知责任与义务，明确开源模型下载使⽤的“禁⽌性”⾏为，防范模型滥⽤或恶意使⽤。持续推进⼈⼯智能芯⽚、框架、软件开放供应链⽣态建设，增强产品服务供应多样性，保障供应链安全稳定。5.5实施应⽤分类及安全⻛险分级管理。根据功能、性能、应⽤场景等，对⼈⼯智能应⽤进⾏分类。在此基础上，探索提出具有共识的安全⻛险分级原则（附件1），从应⽤场景、智能化⽔平、应⽤规模等维度⼊⼿，对安全⻛险进⾏科学评价分级，进⽽采取针对性、差异化安全防范措施。对在关键信息基础设施应⽤的⼈⼯智能系统进⾏登记备案，要求其具备与安全需求相匹配的安全防护能⼒。5.6推⼴⼈⼯智能⽣成合成内容可追溯管理。在全球范围内推⼴基于内容标识的⼈⼯智能⽣成合成内容溯源管理范式，总结梳理已有实践的成功做法经验，按照显式、隐式等标识要求，全⾯覆盖制作源头、传播路径、分发渠道等关键环节，便于浏览⽤⼾识别判断信息来源及真实性。5.7安全有效释放重要⾏业应⽤需求。制定重要⾏业领域⼤模型建设部署基础安全指南，从模型选⽤、模型部署、模型运⾏和模型停⽤等环节，提出安全基线建议。在此基础上，相关⾏业领域结合⾃⾝属性特点，制定能源、电信、⾦融、交通、教育、⼯业等重要⾏业领域的应⽤安全指南，形成清晰的安全应⽤路径，释放⾏业应⽤潜⼒。5.8建设⼈⼯智能安全测评体系。构建模型算法安全测评、应⽤通⽤安全测评、具体场景安全测评相衔接的⼈⼯智能安全测评体系。模型算法测评，聚焦模型鲁棒性、可靠性、准确性、抗⼲扰能⼒、决策逻辑透明度、对抗攻击防御能⼒等内⽣安全能⼒和⻛险。应⽤通⽤测评，针对普遍使⽤的⼈⼯智能应⽤⻛险开展测试分析评估。具体场景安全测评，结合应⽤场景具体情况评估满⾜应⽤需求的能⼒，以及应⽤运⾏和服务过程中的安全⻛险。组织开展⼈⼯智能安全漏洞众测活动，汇集各⽅⼒量发现潜在安全⻛险。5.9共享⼈⼯智能安全⻛险威胁信息。跟踪分析⼈⼯智能技术、产品、服务安全漏洞、缺陷、⻛险威胁、安全事件信息，建设⼈⼯智能漏洞信息库，建⽴覆盖研发者、服务提供者、专业技术机构的⻛险威胁信息共享机制。推进⼈⼯智能安全⻛险威胁信息共享的国际交流合作，探索建⽴相关国际合作机制和技术标准，协同防范应对⼈⼯智能安全⻛险⼤跨域、⼤规模扩散传播。5.10完善数据安全和个⼈信息保护规范。针对⼈⼯智能技术及应⽤特点，明确⼈⼯智能训练、标注、使⽤、输出等各环节的数据安全和个⼈信息保护要求。对涉及个⼈信息的数据实施去标识化等脱敏处理。加强政务、⾦融等重要⾏业领域⼈⼯智能应⽤中的数据安全防护，防范重要数据、核⼼数据泄露⻛险。5.11增进协同应对⼈⼯智能失控⻛险的共识。加强⼈⼯智能最终⽤途管理，对核⽣化导等场景下使⽤⼈⼯智能技术提出相关要求，防⽌⼈⼯智能系统被滥⽤。推⼴涵盖技术、伦理、管理多维度的可信⼈⼯智能基本准则，促进国际社会形成⼴泛共识（附件2）。开发者定期进⾏测试，判断模型是否可能带来潜在技术失控⻛险。5.12加⼤⼈⼯智能安全⼈才培养⼒度。推进⼈⼯智能安全课程体系、培养体系建设，形成从基础教育到⾼等教育的完整培养链条。加强⼈⼯智能安全设计、开发、治理⼈才培养，⽀持培养⼈⼯智能安全前沿基础领域顶尖⼈才，壮⼤⽆⼈驾驶、智慧医疗、类脑智能、脑机接⼝等重点、前沿领域的安全⼈才队伍。5.13提升全社会的⼈⼯智能安全意识。⾯向政府、企业、社会公⽤事业单位加强⼈⼯智能安全规范应⽤的教育培训。结合互动平台、开放课程与社区科普活动，加强⼈⼯智能安全⻛险及防范应对知识的宣传，全⾯提⾼全社会⼈⼯智能安全意识，使政府、⾏业与公众能准确认识⼈⼯智能的技术局限。指导⽀持⽹络安全、⼈⼯智能领域⾏业协会加强⾏业⾃律，制定提出⾼于监管要求、具有引领⽰范作⽤的⾃律要求；建⽴⾯向公众⼈⼯智能安全⻛险隐患举报受理机制，形成有效的社会监督氛围。5.14促进人工智全理国际合作。坚持践⾏多边主义，推动共商共建共享的⼈⼯智能全球治理观。⽀持联合国发挥主渠道作⽤，深⼊参与联合国国际⼈⼯智能科学⼩组和全球⼈⼯智能治理对话机制。推进APEC、G20、上合组织、⾦砖国家等多边机制下的⼈⼯智能治理进程，加强与“⼀带⼀路”国家、“全球南⽅”国家合作，增强发展中国家在⼈⼯智能全球治理中的代表性和发⾔权，推进《⼈⼯智能全球治理⾏动计划》。6.⼈⼯智能研发与应⽤的安全指引6.1⼈⼯智能模型算法研发的安全开发指引6.1.1在算法规则、模型框架设计环节，应考虑提升算法可靠性、公平性、透明度、可解释性、隐私保护、价值观对⻬等内⽣安全能⼒设计。6.1.2评估模型算法潜在偏⻅，加强训练数据内容和质量的抽查检测，设计有效、可靠的对⻬算法，确保价值观⻛险、伦理⻛险等可控。6.1.3确保模型算法训练环境的安全性，包括⽹络安全配置和数据加密措施等。结合安全测试发现的⾼⻛险问题，通过针对性的微调、强化学习等⽅式优化模型，持续提升模型内⽣安全能⼒。6.1.4关注和构建安全的训练数据集，规范数据来源管理，采⽤数据清洗、标注、安全审核等⽅法确保训练数据内容的安全性，确保数据来源清晰、内容合规。6.1.5对训练数据进⾏质量和安全性评估，采取分类模型、⼈⼯抽检等⽅式，过滤训练数据中的错误、违法不良内容。6.1.6规范训练数据标注流程，采⽤交叉标注、结果审计等质量控制⽅法，提升标注准确性和可靠性，降低个体差异和个⼈偏⻅对标注质量的影响。6.1.7重视数据安全和个⼈信息保护，尊重知识产权和版权。建⽴完善的数据安全管理制度，遵循正当合法必要原则收集、使⽤和处理个⼈信息，对涉及个⼈信息的数据实施去标识化等脱敏处理。加强数据安全防护技术能⼒，防范数据泄露、流失、扩散、侵权等⻛险。6.1.8基于开源模型算法进⾏⼆次开发的研发者，在尊重研发者智⼒投⼊的基础上，遵循相应开源协议规范。对所使⽤的开发框架、代码等进⾏安全审计，并关注开源框架安全及漏洞相关问题，识别和修复潜在的安全漏洞。6.1.9定期开展安全评估测试，制定⻛险分类分级测评与优化机制，测试前明确测试⽬标、范围和安全维度，构建多样化的测试数据集，涵盖各种应⽤场景，并制定各类⻛险的针对性模型优化策略。6.1.10做好⼈⼯智能模型及所⽤数据集的版本管理，商⽤版本应可以回退到以前的版本。6.1.11制定明确的测试规则和⽅法，包括⼈⼯测试、⾃动测试、混合测试等，利⽤沙箱仿真等技术对模型进⾏充分测试和验证。⽤于商业化⽤途的研发者，应形成详细的测试报告，分析安全问题并提出改进⽅案。6.1.12评估⼈⼯智能模型算法对外界⼲扰的容忍程度，以适⽤范围、注意事项或使⽤禁忌的形式告知服务提供者和其他研发者。6.1.13定期披露⼈⼯智能模型算法的审计与异常处置情况。6.1.14积极参与开源社区建设，推动⼈⼯智能安全治理技术创新和实践，为服务提供者和使⽤者提供合规治理解决⽅案或治理⼯具。6.2⼈⼯智能应⽤建设部署的安全指引6.2.1评估⽬标场景应⽤⼈⼯智能技术的必要性及使⽤后的⻓期和潜在影响，结合其应⽤场景重要性、智能化⽔平、应⽤规模等进⾏⻛险分级，参考⻛险等级开展安全评估和定期审计。6.2.2增强供应链安全保障能⼒，建设部署所需模型⽂件、框架⼯具、第三⽅库等，应从相关⼚商官⽅⽹站或其在主流开源社区的官⽅账号下获取，选取成熟稳定的版本，并进⾏完整性校验和安全测试。6.2.3对建设部署所需的软硬件设备、第三⽅⼯具等进⾏安全检测，确保不含未修复且可被利⽤的已知漏洞。建⽴漏洞追溯机制，跟踪相关软硬件安全漏洞、缺陷信息，防范供应链植⼊后⻔。6.2.4在访问控制层⾯，准确安装配置软件、运⾏环境参数、功能模块调⽤策略，禁⽤⾮必要的⽹络端⼝和功能服务，重点检查默认配置、默认⼝令，及时修复安全⻛险。6.2.5在应⽤管理层⾯，对⼈机交互接⼝和API接⼝进⾏⽤⼾⾝份识别及权限控制，最⼩化设置访问权限，根据业务场景限制接⼝调⽤频率，对⼀般⽤⼾禁⽤⾼⻛险操作，对恶意⾏为⽤⼾建⽴暂停服务、阻断访问等管控能⼒。6.2.6全⾯了解应⽤场景的数据安全和隐私保护要求，合理限制对数据的访问权限，防⽌超范围使⽤数据，制定数据备份和恢复计划，并定期对数据处理流程进⾏检查。6.2.7采⽤安全护栏等技术⼿段，识别拦截违法不良内容、提⽰词注⼊攻击等，防范输出内容超出业务范围。6.3⼈⼯智能应⽤运⾏管理的安全指引6.3.1建⽴完善的⼈⼯智能应⽤安全管理和监督机制，明确责任⽅，健全⼈⼯复核机制，保障在关键场景应⽤中⼈⼯智能应⽤决策透明、可控，并提供清晰的决策依据，确保⼈⼯智能应⽤在⼈类授权和控制下运⾏。6.3.2严格管理⼈⼯智能应⽤权限，通过最⼩权限原则等⼿段强化内部安全管理，增强账⼾安全性，在处理敏感数据时使⽤加密技术等保护措施。6.3.3建⽴⼈⼯智能应⽤运⾏监测能⼒和安全事件应急预案，设置其关键指标的安全预警阈值，能够及时发现安全事件，并具备切换到⼈⼯或传统系统等的能⼒。定期开展应急演练，并根据⾏业安全事件、重要舆情及监管变化，及时优化应急策略，应对不断变化的安全⻛险。6.3.4在⼈⼯智能⽣成内容内添加显式或隐式标识，做好⽣成合成内容提⽰和溯源管理。在政务信息公开、司法取证等场景部署深度伪造检测⼯具，对疑似⼤模型⽣成的信息实施来源核验与交叉验证。6.3.5制定信息内容交互⾏为规范、安全运营机制、投诉反馈机制、技术防护能⼒等，防范⼈⼯智能应⽤被不当或恶意利⽤⽣成、发布、传播虚假有害信息⻛险。6.3.6记录⼈⼯智能应⽤运⾏⽇志，包括系统⾏为、⽤⼾⾏为等，⽇志留存时间不少于6个⽉，并定期对⽇志记录进⾏审计。6.3.7建⽴健全实时⻛险监控管理机制，持续跟踪运⾏中安全⻛险。6.3.8提升应⽤的透明度、公平性，公开⼈⼯智能应⽤的能⼒、局限性、适⽤⼈群、场景。6.3.9应向使⽤者说明⼈⼯智能应⽤的⽬标实现度和偏离度，在⼈⼯智能决策有重⼤影响时，做好解释说明。6.3.10维护使⽤者的知情权、选择权、监督权等合法权益，在合同或服务协议中，以使⽤者易于理解的⽅式，告知⼈⼯智能应⽤的适⽤范围、注意事项、使⽤禁忌，⽀持使⽤者知情选择、审慎使⽤。6.3.11在告知同意、服务协议等⽂件中，⽀持使⽤者⾏使⼈类监督和控制权利。6.3.12明确具体应⽤中的数据归属及算法缺陷的责任主体，确保责任链条可追溯。6.3.13落实数据安全管理责任，评估⼈⼯智能应⽤中存在的数据泄露、个⼈隐私泄露、违规收集使⽤个⼈信息等⻛险，建⽴数据全⽣命周期安全管理机制，提升数据防泄漏、防窃取保障能⼒。6.3.14评估⼈⼯智能应⽤在⾯临故障、攻击等异常条件下抵御或克服不利条件的能⼒，防范出现意外结果和⾏为错误，确保最低限度有效功能。6.3.15加强从业⼈员安全意识和安全能⼒培训，提⾼⼈⼯智能安全⻛险防范意识。6.3.16在合同或服务协议中明确，⼀旦发现不符合使⽤意图和说明限制的误⽤、滥⽤，提供者有权采取纠正措施或提前终⽌服务。6.3.17⾯向未成年⼈、⽼年⼈及特殊群体提供⼈⼯智能服务，应在产品功能设计、服务模式等环节，充分考虑可⽤性和安全性。6.4⼈⼯智能应⽤访问使⽤的安全指引6.4.1提⾼对⼈⼯智能应⽤安全⻛险的认识，选择信誉良好的⼈⼯智能应⽤。6.4.2在使⽤前仔细阅读产品合同或服务协议，了解应⽤的功能、限制和隐私政策，准确认知⼈⼯智能应⽤做出判断决策的局限性，合理设定使⽤预期。6.4.3提⾼个⼈信息保护意识，避免在不必要的情况下输⼊敏感信息。6.4.4了解⼈⼯智能应⽤的数据处理⽅式，避免使⽤不符合隐私保护原则的产品。6.4.5在使⽤⼈⼯智能应⽤时，应关注⽹络安全⻛险，避免⼈⼯智能应⽤成为⽹络攻击的⽬标。6.4.6注意⼈⼯智能应⽤对⼉童和⻘少年的影响，预防沉迷及过度使⽤。安全风险与技术应对措施、综合治理措施映射表人人机工工智智智能能能应应应用用用衍衍衍生生生安安安全全全风风风险险险提升全社会的人工智能安全意识附件1⼈⼯智能安全⻛险的分级原则⼈⼯智能安全⻛险的评价涉及诸多因素。可从应⽤场景重要性、智能化⽔平、应⽤规模等维度，对⼈⼯智能安全⻛险进⾏评价分级，进⽽针对性采取安全防范措施。⼀、主要分级要素1.应⽤场景应⽤场景反映⼈⼯智能在实际使⽤中具体的运⾏环境、⽬标需求等，具体涉及应⽤⽬的、⾏业领域、使⽤环境、服务对象及可能涉及的社会、经济、安全影响等要素。2.智能化⽔平智能化⽔平反映⼈⼯智能系统处理复杂任务、满⾜应⽤需求、独⽴⾃主运⾏等⽅⾯的能⼒。低智能化⽔平下，系统能⼒较低，仅可作为辅助建议，决策需要⼈⼯介⼊。随着智能化⽔平提⾼，⼈⼯介⼊频次和范围不断减⼩。⾼智能化⽔平下，⽆需⼈⼯进⾏⼲预，系统全流程⾃主决策运⾏。3.应⽤规模应⽤规模反映⼈⼯智能系统或服务的覆盖范围及影响⼴度。⽤⼾范围有限或应⽤领域单⼀的系统，如企业内部智能⼯具、区域性服务等，其⻛险影响相对可控。⽤⼾数量达到⼀定规模，或深度嵌⼊关键⾏业领域的业务流程，如智能辅助驾驶、城市运⾏管理、⼯业⽣产调度、⾏业级⾦融⻛控模型等，其安全⻛险可能快速扩散并引发系统性影响。⼆、⻛险级别1.低安全⻛险具有轻微威胁性且影响范围很⼩，对国家安全、社会稳定和公⺠权益的安全基本⽆影响，潜在危害轻微。2.⼀般安全⻛险具有⼀定威胁性但影响范围有限，对国家安全、社会稳定和公⺠权益的安全影响较⼩，潜在危害可控。3.较⼤安全⻛险具有明显威胁性和局部性影响特征，对国家安全、社会稳定和公⺠权益可能带来较⼤影响，产⽣局部社会⾯危害。4.重⼤安全⻛险具有重⼤威胁性和区域性影响特征，对国家安全、社会稳定和公⺠权益可能带来严重影响，产⽣重⼤社会⾯危害。5.特别重⼤安全⻛险具有灾难性和系统性威胁特征，对国家安全、社会秩序和公⺠权益造成颠覆性或不可逆转的特别严重的影响。三、⻛险定级推动⼈⼯智能应⽤安全分类分级国家标准制定⼯作。⾏业领域主管（监管）部⻔参照国家标准制定⾏业标准规范、实施细则，并推动本⾏业领域⼈⼯智能安全应⽤相关分类分级⼯作。1.分类分级国家标准通过⼈⼯智能应⽤安全⻛险分类分级标准，明确分类分级基本流程，以及应⽤场景、智能化⽔平、应⽤规模等分级要素，并给出⾏业领域细化⾏业指南的步骤⽅法，为⾏业领域开展⻛险分类分级提供参考。2.分类分级⾏业细则⾏业领域主管（监管）部⻔结合⾏业领域、使⽤环境、服务对象及可能涉及的社会、经济、安全影响等，制定本⾏业本领域⼈⼯智能安全分类分级标准规范：（1）选取适⽤于本⾏业、本领域的⼈⼯智能安全⻛险分级要素项⽬，并根据⾏业特点进⾏实例化。（2）制定本⾏业、本领域安全⻛险分级细则（定级原则、要素权重），确定⼈⼯智能安全⻛险级别。3.⻛险分类分级⾏业领域主管（监管）部⻔，根据本⾏业、本领域的⼈⼯智能安全⻛险分类分级标准规范，组织本⾏业、本领域⼈⼯智能有关单位开展分类分级⼯作，指导有关单位准确识别、及时防范化解重⼤安全⻛险和较⼤安全⻛险。附件2可信人工智能基本准则落实《全球⼈⼯智能治理倡议》，遵循“以⼈为本、智能向善”的发展⽅向，共同防范应对⼈⼯智能技术失控⻛险，促进⼈⼯智能技术在世界范围内可信应⽤，提出可信⼈⼯智能基本准则如下：1.⼈类最终控制在⼈⼯智能系统关键环节设置⼈类控制机制，使最终裁决权归属⼈类，通过设计安全控制阈值、设置安全终⽌开关、预留⼈⼯⼲预有效窗⼝等措施，确保⼈⼯智能系统能够实现⼈类预期⽬标、不会脱离⼈类监督运⾏失控。2.尊重国家主权研发设计⼈⼯智能产品和提供⼈⼯智能服务时，应尊重所在国主权，严格遵守产品和服务运营所在地的法律，并依法接受监管，不得借助⼈⼯智能产品或服务⼲涉他国内政、社会制度及社会秩序。3.价值观对⻬将和平、发展、公平、正义、⺠主、⾃由的全⼈类共同价值深度融⼊⼈⼯智能系统全⽣命周期。4.提升系统透明度推动⼈⼯智能系统在功能⽬标、运⾏逻辑、模型使⽤、数据来源、决策依据等关键环节的必要披露，增强社会公众信任基础。5.促进可客观验证研究构建客观、公正、透明的测试与认证机制，推动⼈⼯智能系统的功能、性能、安全特性、决策链条等⽅⾯可被技术验证。6.全护在⼈⼯智能系统设计和部署过程中，强化⻛险建模、安全测试和防护机制建设，进⾏全⽣命周期审计与记录，防⽌系统因模型缺陷、外部攻击和技术滥⽤等问题偏离预期⽬标。7.前预应对通过前瞻性⻛险识别评估，积极预防和动态监测，加强应急响应，避免⼈⼯智能失控事件发⽣和扩⼤。8.全球协同共治⽀持联合国发挥主渠道作⽤，推动多边和多⽅跨领域协同共治，促进各国政府、企业、学术机构与社会公众形成合⼒，以多层级、多领域的治理机制推动⼈⼯智能健康发展。附件3术语本框架提到的相关专业术语解释如下。1.⼈⼯智能伦理：开展⼈⼯智能技术基础研究和应⽤实践时遵循的道德规范或准则。2.可解释性：⼈⼯智能系统以⼈类可理解的⽅式呈现其输出结果与输⼊特征之间因果或统计关系的属性。该属性使得⼈类能够追溯并理解影响系统决策的关键因素。3.合成数据：通过算法⽣成或扩展⽽⾮实际收集的数据。4.数据标注：通过⼈⼯操作或使⽤⾃动化技术机制，基于对提⽰信息的响应信息内容，将特定信息如标签、类别或属性添加到⽂本、图⽚、⾳频、视频或者其他数据样本的过程。5.预训练：通过⼤规模数据训练迭代模型参数，使⼈⼯智能模型获得通⽤知识的过程。6.优化训练：在预训练模型基础上，使⽤特定领域数据训练，实现模型参数⼩范围调整，使⼈⼯智能模型强化在特定领域的数据分析处理能⼒的过程。7.对⻬：使⼈⼯智能系统的输出或⾏为与设计者的安全⽬标相符的算法及技术。8.强化学习：⼈⼯智能模型在运⾏环境中采取⾏动、接收运⾏环境反馈的奖励或惩罚反馈，逐步优化形成最优策略以最⼤化累积回报的⼀种学习范式。9.推理：⼈⼯智能模型基于其训练获得的知识和模式识别能⼒，对输⼊信息进⾏分析、处理和逻辑演绎，产⽣合理输出的过程。10.显式标识：在⽣成合成内容或者交互场景界⾯中添加的，以⽂字、声⾳、图形等⽅式呈现并可以被⽤⼾明显感知到的标识。11.隐式标识：采取技术措施在⽣成合成内容⽂件数据中添加的，不易被⽤⼾明显感知到的标识。12.数据投毒：攻击者篡改、注⼊错误、误导数据，“污染”模型的概率分布，进⽽造成准确性、可信度下降的⾏为。13.对抗攻击：通过构造微扰数据等输⼊样本，使⼈⼯智能模型产⽣错误输出或⾏为的攻击⽅式。14.智能体：能够⾃主感知环境、制定决策、采取⾏动实现特定⽬标的智能系统，⼀般具有记忆、规划、使⽤⼯具等基本能⼒。15.安全护栏：针对⼤模型的安全控制措施，通过结合规则库、负⾯判别模型等技术⼿段，对⼤模型输⼊输出内容、数据泄露、提⽰词攻击等进⾏识别、拦截及处置，实现对⼤模型输⼊的验证和过滤，以及限制⼤模型输出不符合预期的内容，保障⽣成内容的可控性、合规性和安全性。